## Preface

æœ¬ä»“åº“è®°å½•å…³äºOOD Generalization (Domain Generlization)/Graph OOD Generalization/In-context Learning/LLM/LVMç­‰topicçš„æ–‡ç« ã€‚çœ‹è¿‡çš„æ–‡ç« ä¼šè‡³å°‘ç”¨ä¸€å¥è¯æ¦‚æ‹¬å†…å®¹ï¼Œæœ‰äº›è¿˜ä¼šæœ‰notesã€‚åªæœ‰æ ‡é¢˜çš„å°±æ˜¯è¿˜æ²¡çœ‹è¿‡çš„ï¼Œåªæ˜¯å…ˆå­˜æ¡£åˆ°è¿™é‡Œã€‚

###  ğŸ”¥ Updates

- 2024-05 æ¥ä¸‹æ¥ä¸»è¦å…³æ³¨æ¢ç©¶ICLæœºåˆ¶çš„ç›¸å…³å·¥ä½œ
- 2024-02-21 æ¥ä¸‹æ¥å°†ä¼šä¸»è¦å…³æ³¨LLM/Multi-modal LLMsçš„generalizationã€‚

## Directory

* [OOD Generalization of (Multimodal) LLMs](#ood-generalization-of-multimodal-LLMs)
* [OOD Generalization using Large Vision-language models](#ood-generalization-using-large-vision-language-models)
* [New tasks of OOD Generalization](#new-tasks-of-ood-generalization)
* â­[OOD Generalization (classic)](#ood-generalization-classic)
* [Test-time Adaptation for Generalization](#test-time-adaptation-for-generalization)
* [Domain Generalization/Adaptation in Down-stream CV/NLP Tasks](#domain-generalizationadaptation-in-down-stream-cvnlp-tasks)
* â­[Graph OOD Generalization (graph-level & node-level)](#graph-ood-generalization-graph-level--node-level)
* [Domain Adaptation](#domain-adaptation)
* [LLMs/Large Multi-modal models](#llmslarge-multi-modal-models) 
* [Prompt Learning](#prompt-learning)
* â­[In-Context Learning](#in-context-learning)
* [ICL Theories](#icl-theories)
* [Others](#others)
    * [Optimization](#optimization) 
    * [Adversarial Robustness](#adversarial-robustness)
    * [Neural Collapse](#neural-collapse)
    * [Feature Collpase](#feature-collapse)
    * [Causality](#causality)
    * [Graph Homophily/Heterophily](graph-homophilyheterophily)
* [Slides](#slides)



## OOD Generalization of (Multimodal) LLMs 

### 2024

1. **On the Out-Of-Distribution Generalization of Multimodal Large Language Models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.06599) å¯¹MLLMè¿›è¡Œäº†ä¸€ç³»åˆ—OODæ€§èƒ½çš„éªŒè¯ï¼Œå‘ç°ï¼šâ‘ åœ¨ç»å…¸OODæ•°æ®é›†ä¸ŠMLLMèƒ½è¶…è¿‡ä»¥å‰çš„sota â‘¡åœ¨åŒ»å­¦ã€åˆ†å­æ•°æ®é›†ä¸Šæ•ˆæœä¸å¥½ â‘¢ICLèƒ½æå‡MLLM OODæ€§èƒ½ï¼Œå³ä½¿ICL exampleçš„åˆ†å¸ƒå’Œæµ‹è¯•åŸŸä¸åŒã€‚

### 2023

1. **In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax** (Arxiv Nov 2023) [[paper] ](https://arxiv.org/abs/2311.07811) æœ¬æ–‡é€šè¿‡æ„å»ºä¸€äº›è¯­æ³•ä»»åŠ¡æ¥æµ‹è¯•æ¨¡å‹å¯¹äºå¥å­ç»“æ„çš„ç†è§£èƒ½åŠ›ï¼Œä»¥åŠOODæ³›åŒ–æ€§èƒ½ã€‚æ€»çš„è¯´æ¥ï¼ŒLLMè¿˜æ˜¯ä¼šç”¨åˆ°ä¸€äº›spurious correlationã€‚
2. **On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective** (ICLR 2024 workshop highlighted) [[paper]](http://arxiv.org/abs/2302.12095)
3. **Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes** (Arxiv Nov 2023, ongoing work) [[paper]](Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes) å‘ç°æ¨¡å‹å¯¹äºdemonstrationçš„permutation invarianceæˆ–è®¸æ˜¯ICL OODçš„å…³é”®ã€‚æå‡ºä½¿ç”¨ç›¸åŒçš„positional encodingæ¥æå‡ICL OODæ€§èƒ½ã€‚
4. **A Closer Look at In-Context Learning under Distribution Shifts** (Arxiv May 2023) [[paper]](http://arxiv.org/abs/2305.16704) åœ¨ä¸€å®šçš„covariate shiftä¸‹ï¼Œtransformeræ¯”set-based MLPçš„æ€§èƒ½å¥½ï¼›åœ¨ä¸¥é‡çš„åˆ†å¸ƒåç§»ä¸‹ï¼Œä¸¤ç§æ¨¡å‹çš„ICLèƒ½åŠ›éƒ½ä¸§å¤±äº†ã€‚3. 4. è¿™ä¸¤ç¯‡éƒ½æ˜¯æ•´ä¸ªæµ‹è¯•promptå’Œè®­ç»ƒpromptåˆ†å¸ƒä¸åŒçš„æƒ…å†µã€‚
5. **Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation** (Arxiv May 2023) [[paper]](https://www.lsv.uni-saarland.de/wp-content/uploads/2023/07/Few-shot-Fine-tuning-vs.-In-context-Learning.pdf) åœ¨å‚æ•°é‡ç›¸å½“çš„æƒ…å†µä¸‹ï¼ŒICLçš„OODä¸å¦‚FTã€‚30Bçš„ICLè·Ÿ6.7Bçš„FTæ€§èƒ½ç›¸å½“ã€‚å¤§éƒ¨åˆ†æƒ…å†µä¸‹ICLä¸å¦‚FTã€‚

### 2022

1. **What Can Transformers Learn In-Context? A Case Study of Simple Function Classes** (NeurIPS 2022) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/c529dba08a146ea8d6cf715ae8930cbe-Abstract-Conference.html) åœ¨ä¸€ä¸ªtoy linear regressionä»»åŠ¡ä¸Šï¼Œå°è¯•äº†ä¸¤ç§OOD settingï¼šâ‘ testå’Œtrain promptåˆ†å¸ƒä¸åŒ â‘¡testæ—¶çš„ICEå’Œqueryåˆ†å¸ƒä¸åŒã€‚å‘ç°ICLåœ¨è¿™ä¸¤ç§åœºæ™¯ä¸‹å‡æœ‰ä¸€å®šçš„æ³›åŒ–èƒ½åŠ›ã€‚



## OOD Generalization using Large Vision-language models

### 2023

1. **PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization** (ICCV 2023) å…ˆè®­ç»ƒKä¸ªç”¨äºç”Ÿæˆstyle word embeddingï¼ˆå¤šæ ·æ€§+ä¿æŒè¯­ä¹‰ï¼‰ï¼Œç„¶åæŠŠå®ƒä»¬å’ŒNä¸ªç±»åˆ«è¯embeddingç»“åˆï¼Œå–‚ç»™CLIPtextç”ŸæˆK*Nä¸ªstyle-contentçš„text featureã€‚ä¹‹åæ‹¿è¿™äº›featureè®­ç»ƒä¸€ä¸ªlinear classifierã€‚æ¨æ–­æ—¶æŠŠè¿™ä¸ªclassifieræ¥åˆ°CLIP image encoderä¸Šï¼ˆ**å› ä¸ºåœ¨joint image-textç©ºé—´ä¸­ï¼Œå­˜åœ¨cross-modal transferability phenomenon**ï¼‰ã€‚ [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Cho_PromptStyler_Prompt-driven_Style_Generation_for_Source-free_Domain_Generalization_ICCV_2023_paper.html)
3. **A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance** (ICCV 2023) å°†CLIPçš„language encoderè¾“å‡ºçš„embeddingä½œä¸º"generic text representation"ï¼Œç„¶åè®©studentï¼ˆvisual modelï¼‰çš„è¡¨ç¤ºå»å¯¹é½teacherï¼ˆCLIPï¼‰çš„text representationã€‚åŒæ—¶å¯¹é½studentå’Œteacherçš„é¢„æµ‹logitã€‚ã€insight 1ã€‘recent workå‘ç°åŸºäºç¯å¢ƒåˆ’åˆ†çš„æ–¹æ³•ä¸é‚£ä¹ˆworkï¼Œå› ä¸ºçœŸå®ä¸–ç•Œçš„ç¯å¢ƒåˆ’åˆ†ä¸æ˜ç¡® ã€insight 2ã€‘ä»ä¼˜åŒ–éš¾åº¦è§’åº¦è¯´æ˜anchor sampleåœ¨å¯¹é½æ—¶çš„ä½œç”¨ [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Huang_A_Sentence_Speaks_a_Thousand_Images_Domain_Generalization_through_Distilling_ICCV_2023_paper.html) [[slides]](/all_notes/2023.12.29-OOD-LM.pptx)
4. **Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models** (NIPS 2023) åœ¨ç”¨In domainæ•°æ®ç”Ÿæˆçš„Discrete adversarial eaxmpleä¸Šæ‹¿ä¸€ä¸ªCLIPåšè’¸é¦æå‡å°±èƒ½è¶…è¿‡æ™®é€šçš„Knowledge Distillationå’ŒDAT [[paper]](https://arxiv.org/pdf/2311.01441.pdf) [[slides]](/all_notes/2023.12.29-OOD-LM.pptx)
5. **Distilling from Vision-Language Models for Improved OOD Generalization in Vision Tasks** (Arxiv Oct 2023) å¯¹é½student modelç»è¿‡ä¸€ä¸ªprojectoråçš„è¡¨ç¤ºå’ŒCLIPçš„text/image encoderçš„è¾“å‡º [[paper]](https://arxiv.org/abs/2310.08255) [[slides]](/all_notes/2023.12.29-OOD-LM.pptx)
6. **Context-Aware Robust Fine-Tuning** (IJCV Dec 2023) å‘ç°å¾®è°ƒä¼šç ´åCLIPåŸæœ‰çš„å¯¹äºcontextï¼ˆè™šå‡ç‰¹å¾ï¼‰zero-shotçš„åˆ†ç±»èƒ½åŠ›ã€‚æå‡ºåœ¨å¾®è°ƒçš„æ—¶å€™å¯¹é½é¢„è®­ç»ƒçš„CLIPé¢„æµ‹çš„contextçš„æ¦‚ç‡åˆ†å¸ƒå’Œæ­£åœ¨è¢«å¾®è°ƒçš„æ¨¡å‹é¢„æµ‹çš„contextçš„æ¦‚ç‡åˆ†å¸ƒ [[paper]](https://link.springer.com/article/10.1007/s11263-023-01951-2) 
7. **ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models**(Arxiv Nov 2023) [[paper]](http://arxiv.org/abs/2311.16494) æœ¬æ–‡æå‡ºäº†neg promptï¼Œå°±æ˜¯åœ¨prompté‡ŒåŠ å…¥neg attributeçš„embeddingã€‚åœ¨prompt tuningæ—¶ï¼Œå¸Œæœ›åœ¨ä½¿ç”¨neg promptæ—¶ï¼Œé¢„æµ‹æˆä»»ä½•ç±»åˆ«çš„æ¦‚ç‡ç›¸åŒï¼Œä»¥æ­¤æ¥å¼ºè¡Œæ¶ˆé™¤è™šå‡å…³è”ï¼ˆæ–‡ä¸­losså¼(9)ï¼‰ã€‚ä½¿ç”¨neg promptæ—¶é¢„æµ‹ä¸ºæŸä¸€ç±»çš„æ¦‚ç‡å°±æ˜¯ç”¨neg embedding+ä»»ä½•classçš„embeddingå’Œå›¾åƒè¡¨ç¤ºç®—ç›¸ä¼¼åº¦ã€‚



## New tasks of OOD Generalization

1. **SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization** (NeurIPS 2023) å¤šæ¨¡æ€DGï¼Œå°†å„æ¨¡æ€çš„ç‰¹å¾åˆ†ä¸ºä¸åŒæ¨¡æ€shareçš„éƒ¨åˆ†ä»¥åŠå„ä¸ªæ¨¡æ€specificçš„éƒ¨åˆ†ï¼Œæ‹‰è¿‘åŒä¸€classçš„ä¸åŒæ¨¡æ€sharedéƒ¨åˆ†çš„ç‰¹å¾ï¼Œæ¨è¿œshareéƒ¨åˆ†çš„ç‰¹å¾å’Œspecificéƒ¨åˆ†çš„ç‰¹å¾ã€‚[[paper]](https://openreview.net/forum?id=RiSMijlsLT)



## OOD Generalization (classic)
### 2024
1. **Spurious Feature Diversification Improves Out-of-distribution Generalization** (ICLR 2024) é€šè¿‡ensembleå­¦æ›´å¤šçš„spurious featureèƒ½â€œå†²æ·¡â€å®ƒä»¬å„è‡ªçš„å½±å“ [[paper]](https://openreview.net/forum?id=d6H4RBi7RH)
1. **Out-Of-Domain Unlabeled Data Improves Generalization** (ICLR 2024 spotlight) [[paper]](https://openreview.net/forum?id=Bo6GpQ3B9a)
1. **Robust agents learn causal world models** (ICLR 2024 Oral) [[paper]](http://arxiv.org/abs/2402.10877) é€šè¿‡æ„å»ºä¸€ä¸ªcausal influence diagram (CIDï¼Œä¸€ç§åŸºäºcausal baysian networkæ‰©å±•çš„æ¨¡å‹)ï¼Œè¯æ˜äº†å¯¹äºä¸€ä¸ªå†³ç­–ä»»åŠ¡ï¼ˆåˆ†ç±»ã€å›å½’ç­‰ä¼ ç»Ÿä»»åŠ¡éƒ½å¯ä»¥ç”¨å†³ç­–ä»»åŠ¡æ¥å»ºæ¨¡ï¼‰ï¼Œå­¦ä¹ è¿‘ä¼¼çš„causal mnodelæ˜¯å­¦åˆ°è¯¯å·®æœ‰ç•Œçš„ç­–ç•¥çš„å……è¦æ¡ä»¶ã€‚
1. **Context is Environment** (ICLR 2024) [[paper]](https://openreview.net/forum?id=8VPWfqtQMX) æå‡ºä»¥ä¸€ç§ICLçš„èŒƒå¼æ¥å¸®åŠ©domain generalizationï¼Œ æœ¬è´¨ä¸Šæ˜¯ä¸€ç§test-time adaptation
1. **Ask Your Distribution Shift if Pre-Training is Right for You** (ICLR 2024 rejected) ã€ç»“è®ºå­˜ç–‘ã€‘å®éªŒä¸Šå‘ç°pretrainå¯¹out-of-supportæ•°æ®ï¼ˆç±»ä¼¼diversity shiftï¼‰æ›´æœ‰ç”¨ï¼Œå¯¹in supportï¼ˆç±»ä¼¼correlation shiftï¼‰ä½œç”¨ä¸å¤§ã€‚
1. **Improving Domain Generalization with Domain Relations** (ICLR 2024 spotlight) [[paper]](https://openreview.net/forum?id=Dc4rXq3HIA) åˆ©ç”¨æ•°æ®é›†çš„meta-dataæ„å»ºdomain relationï¼Œåœ¨è®­ç»ƒæ—¶ä¼˜åŒ–è¿™ä¸ªrelationå¹¶ä¸ºæ¯ä¸ªdomainè®­ç»ƒä¸€ä¸ªdomain specific headï¼Œåœ¨æµ‹è¯•æ—¶æ ¹æ®testå’Œtraining domainä¹‹é—´çš„relationåŠ æƒæ¯ä¸ªtraining domain expertçš„é¢„æµ‹ä½œä¸ºæœ€ç»ˆé¢„æµ‹ã€‚

### 2023

1. **FREE LUNCH FOR DOMAIN ADVERSARIAL TRAINING: ENVIRONMENT LABEL SMOOTHING** (ICLR2023) ä½¿ç”¨ç®€å•çš„domain label smoothingæ–¹æ³•æ¥è§£å†³DANNè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œç»™å‡ºäº†ä¸°å¯Œçš„ç†è®ºæ”¯æŒ [[paper]](https://arxiv.org/abs/2302.00194)[[è®ºæ–‡åˆ†äº«ä¼šslides]](./all_notes/2023.2.17domain_adversarial_training_with_ELS.pdf)

2. **Generalizability of Adversarial Robustness Under Distribution Shifts** (AAAI2023) ATæœ‰åŠ©äºDomain Adaptationï¼Œä½†æ˜¯åœ¨Domain Generalization settingä¸‹ä¸å¦‚æ­£å¸¸æ¨¡å‹ [[paper]](https://arxiv.org/abs/2209.15042)

3. **WHAT IS MISSING IN IRM TRAINING AND EVALUATION? CHALLENGES AND SOLUTIONS** (ICLR2023) æŒ‡å‡ºlarge-batchä¼šä½¿å¾—å„IRMæ–¹æ³•çš„ç»“æœä¸å‡†ï¼Œsmall-batchçš„æ•ˆæœè¶³ä»¥åŒ¹æ•ŒåŸºäºlarge-batchçš„ä¼˜åŒ–ç®—æ³•ã€‚æŒ‡å‡ºäº†CMNISTåªæµ‹-90%æ˜¯ä¸å®Œå–„çš„ã€‚åŸºäºIRM-GAMEæå‡ºäº†BLOC-IRMï¼Œå‘ç°æ˜¾ç¤ºåœ°å¼•å…¥per-environment stationaryæ­£åˆ™é¡¹èƒ½ç¼“è§£å„domainåˆ†ç±»å™¨ä¸€è‡´çš„è¿™ä¸ªçº¦æŸå¸¦æ¥çš„å„ç¯å¢ƒåˆ†ç±»å™¨æ¬¡ä¼˜çš„é—®é¢˜ã€‚[[paper]](https://openreview.net/forum?id=MjsDeTcDEy)

4. **Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization** (Arxiv 2023) ä½¿ç”¨contrastive learningæ¥æ‹‰è¿‘ç»Ÿä¸€domainæ•°æ®çš„è¡¨ç¤ºã€æ¨è¿œä¸åŒdomainæ•°æ®çš„è¡¨ç¤ºï¼ˆæ–‡ç« claimè‡ªå·±æ˜¯é¦–ä¸ªåœ¨contrastive learning for DGä¸­è€ƒè™‘ä¸åŒdomainçš„ï¼‰ï¼ŒåŒæ—¶æå‡ºæ‰€è°“çš„domain-specific featureä¹Ÿæœ‰åŠ©äºæ³›åŒ–ï¼ˆä½†æ˜¯æ–‡ç« çš„å®ç°ç®—æ³•è²Œä¼¼æ²¡æœ‰æ˜¾å¼çš„è§£è€¦invariant/variant featureçš„è¿‡ç¨‹ï¼Œä¹Ÿæ²¡æœ‰æ˜¾ç¤ºåœ°æ„å»ºdomain-specific featureï¼‰[[paper]](https://arxiv.org/abs/2302.02350)

5. **Domain Generalization Emerges from Dreaming** (arxiv2023) å…ˆé€šè¿‡ç±»ä¼¼AdaINçš„æ–¹æ³•ç”Ÿæˆé£æ ¼è¿ç§»çš„å›¾åƒ $x'$ï¼Œå†minimizeåŸå§‹å›¾åƒ $x$ å’Œ $x'$çš„predictionçš„å·®è·ã€‚æ€§èƒ½ç”šè‡³å¯ä»¥è¶…è¿‡SWADã€‚[[paper]](https://arxiv.org/pdf/2302.00980.pdf)

6. **BREAKING CORRELATION SHIFT VIA CONDITIONAL INVARIANT REGULARIZER** (ICLR2023) è¯æ˜äº†å¯¹äºcorrelation shiftï¼Œåœ¨ç»™å®šyæ—¶ï¼Œè‹¥ç½‘ç»œè¾“å‡º $f(X)$ ç‹¬ç«‹äºspurious feature $Z$ ï¼Œåˆ™å¯ä¿è¯OOD [[paper]](https://arxiv.org/pdf/2207.06687.pdf)

7. **ASSESSING MODEL OUT-OF-DISTRIBUTION GENERALIZATION WITH SOFTMAX PREDICTION PROBABILITY BASELINES AND A CORRELATION METHOD** (ICLR 2023 rejected) æå‡ºäº†æ–°çš„OOD metricï¼Œé¼“åŠ±ï¼šâ‘ é¢„æµ‹çš„confidenté«˜ â‘¡é¢„æµ‹å…·æœ‰å¤šæ ·æ€§ï¼ˆé¿å…æ‰€æœ‰æ ·æœ¬è¢«é¢„æµ‹ä¸ºåŒä¸€ä¸ªç±»ï¼‰ã€‚å› ä¸ºè¢«å®¡ç¨¿äººå–·ç¼ºä¹ç†è®ºä¾æ®è€Œè¢«æ‹’ã€‚[[openreview] ](https://openreview.net/forum?id=1maXoEyeqx)

8. **MODELING THE DATA-GENERATING PROCESS IS NECESSARY FOR OUT-OF-DISTRIBUTION GENERALIZATION** (ICLR 2023 top 25%) ç”¨ç»Ÿä¸€çš„causal graphæ¥æè¿°correlation shiftå’Œdiversity shiftï¼Œæå‡ºäº†multi-shiftçš„é—®é¢˜ï¼Œå¹¶è¯æ˜äº†å­¦åˆ°çš„è¡¨ç¤ºå¿…é¡»æ»¡è¶³ç”±causal graphæ‰€å¯¼å‡ºçš„æ¡ä»¶ç‹¬ç«‹æ€§æ¡ä»¶æ˜¯ä¿è¯OODæ³›åŒ–çš„å¿…è¦æ¡ä»¶ã€‚ [[paper]](https://openreview.net/forum?id=uyqks-LILZX) [[è®ºæ–‡åˆ†äº«ä¼šslides]](./all_notes/2023.3.15[ICLR2023]CACM.pdf)

9. **Domain Generalization via Nuclear Norm Regularization** (Arxiv 2023) è®¤ä¸ºå¼•å…¥å¯¹ $\phi(x)$ çš„ä½ç§©constraintæœ‰åŠ©äºè§£è€¦ä¸å˜ç‰¹å¾å’Œç¯å¢ƒç‰¹å¾ [[paper]](https://arxiv.org/pdf/2303.07527.pdf)

10. **REVISITING ADAPTERS WITH ADVERSARIAL TRAINING** å‘ç°AdvPropä¸­ä¸ºcleanå’Œadvæ ·æœ¬åˆ†åˆ«å‡†å¤‡BNå±‚ä¸æ˜¯å¿…é¡»çš„ï¼Œå…³é”®åœ¨äºå…±äº«å‚æ•°+domain adaptator (ICLR 2023) [[paper]](https://openreview.net/forum?id=HPdxC1THU8T)

11. **UNDERSTANDING OUT-OF-DISTRIBUTION ACCURACIES THROUGH QUANTIFYING DIFFICULTY OF TEST SAMPLES**  (Arxiv 2023) ä½¿ç”¨entropy based confusion scoreæ¥è¡¡é‡æ ·æœ¬çš„difficultyï¼Œå‘ç°OODæ•°æ®confusion scoreæ›´é«˜ï¼ˆæ¨¡å‹é¢„æµ‹çš„ç†µæ›´é«˜ï¼‰ [[paper]](https://arxiv.org/pdf/2203.15100.pdf)

12. **Effective Robustness against Natural Distribution Shifts for Models with Different Training Data** (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2302.01381.pdf)

13. **Sharpness-Aware Gradient Matching for Domain Generalization** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Sharpness-Aware_Gradient_Matching_for_Domain_Generalization_CVPR_2023_paper.html)

14. **Improved Test-Time Adaptation for Domain Generalization** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Improved_Test-Time_Adaptation_for_Domain_Generalization_CVPR_2023_paper.html)

15. **Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Michalkiewicz_Domain_Generalization_Guided_by_Gradient_Signal_to_Noise_Ratio_of_ICCV_2023_paper.html)

16. **Cross Contrasting Feature Perturbation for Domain Generalization** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Li_Cross_Contrasting_Feature_Perturbation_for_Domain_Generalization_ICCV_2023_paper.html)

17. **A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance** (ICCV 2023) å°†CLIPçš„language encoderè¾“å‡ºçš„embeddingä½œä¸º"generic text representation"ï¼Œç„¶åè®©studentï¼ˆvisual modelï¼‰çš„è¡¨ç¤ºå»å¯¹é½teacherï¼ˆCLIPï¼‰çš„text representationã€‚åŒæ—¶å¯¹é½studentå’Œteacherçš„é¢„æµ‹logitã€‚ã€insight 1ã€‘recent workå‘ç°åŸºäºç¯å¢ƒåˆ’åˆ†çš„æ–¹æ³•ä¸é‚£ä¹ˆworkï¼Œå› ä¸ºçœŸå®ä¸–ç•Œçš„ç¯å¢ƒåˆ’åˆ†ä¸æ˜ç¡® ã€insight 2ã€‘ä»ä¼˜åŒ–éš¾åº¦è§’åº¦è¯´æ˜anchor sampleåœ¨å¯¹é½æ—¶çš„ä½œç”¨ [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Huang_A_Sentence_Speaks_a_Thousand_Images_Domain_Generalization_through_Distilling_ICCV_2023_paper.html)

18. **Understanding Hessian Alignment for Domain Generalization** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Hemati_Understanding_Hessian_Alignment_for_Domain_Generalization_ICCV_2023_paper.pdf)

19. **Flatness-Aware Minimization for Domain Generalization** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Flatness-Aware_Minimization_for_Domain_Generalization_ICCV_2023_paper.html)

20. **MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_MAP_Towards_Balanced_Generalization_of_IID_and_OOD_through_Model-Agnostic_ICCV_2023_paper.html)

21. **Learning Diverse Features in Vision Transformers for Improved Generalization** (ICML 2023 Spurious Correlations Workshop) æœ€å°åŒ–ViTä¸åŒheadå¯¹äºåŒä¸€tokenæ¢¯åº¦çš„ç›¸ä¼¼æ€§æ¥é¼“åŠ±æ¨¡å‹çš„diversityã€‚å®éªŒå‘ç°ViTçš„ä¸åŒtokenä¹‹é—´æœ‰äº›æ˜¯spuriousï¼Œæœ‰äº›æ˜¯shift-robustçš„ã€‚æå‡ºçš„regularizationæœ‰åŠ©äºæ‰©å¤§ä¸åŒheadçš„gapã€‚ [[paper]](https://arxiv.org/pdf/2210.04206.pdf)

22. **Towards Understanding Feature Learning in Out-of-Distribution Generalization** (NeulPS 2023) ERMå·²ç»èƒ½å¤Ÿå­¦åˆ°invå’Œspç‰¹å¾ã€‚ä¸å¥½çš„ERM pretrainä¼šå¯¼è‡´IRMv1å­¦ä¸åˆ°invç‰¹å¾ã€‚æå‡ºäº†ä¸€ç§è¿­ä»£å­¦ä¹ å‰ä¸€è½®æ²¡å­¦åˆ°çš„ç‰¹å¾çš„ç®—æ³•FATã€‚ [[paper]](https://arxiv.org/pdf/2304.11327.pdf)

23. **Diversify and Disambiguate: Learning From Underspecified Data** (ICLR 2023) å¾ˆåƒCVPR2022 Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions With Superior OOD Generalizationçš„åšæ³•ï¼šåŒä¸€ä¸ªbackboneï¼ˆfeature extractorï¼‰è®­ç»ƒå¾ˆå¤šheadï¼ŒåŠ æ­£åˆ™é¼“åŠ±è¿™äº›headçš„ä¸åŒï¼Œç„¶åå€ŸåŠ©oracleä¿¡æ¯æ¥é€‰å‡ºä¸€ä¸ªå¤´ï¼ˆæœ¬æ–‡æ˜¯ç”¨ä¸€ä¸ªæ‰€è°“çš„æœ€informativeçš„æµ‹è¯•å­é›†æ¥é€‰ï¼ŒCVPRé‚£ç¯‡æ˜¯ç”¨ä¸€ä¸ªood validation setæ¥é€‰ï¼‰ã€‚[[paper]](http://arxiv.org/abs/2202.03418)

24. **SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization** (NeurIPS 2023) å¤šæ¨¡æ€DGï¼Œå°†å„æ¨¡æ€çš„ç‰¹å¾åˆ†ä¸ºä¸åŒæ¨¡æ€shareçš„éƒ¨åˆ†ä»¥åŠå„ä¸ªæ¨¡æ€specificçš„éƒ¨åˆ†ï¼Œæ‹‰è¿‘åŒä¸€classçš„ä¸åŒæ¨¡æ€sharedéƒ¨åˆ†çš„ç‰¹å¾ï¼Œæ¨è¿œshareéƒ¨åˆ†çš„ç‰¹å¾å’Œspecificéƒ¨åˆ†çš„ç‰¹å¾ã€‚[[paper]](https://openreview.net/forum?id=RiSMijlsLT)

25. **PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization** (ICCV 2023) å…ˆè®­ç»ƒKä¸ªç”¨äºç”Ÿæˆstyle word embeddingï¼ˆå¤šæ ·æ€§+ä¿æŒè¯­ä¹‰ï¼‰ï¼Œç„¶åæŠŠå®ƒä»¬å’ŒNä¸ªç±»åˆ«è¯embeddingç»“åˆï¼Œå–‚ç»™CLIPtextç”ŸæˆK*Nä¸ªstyle-contentçš„text featureã€‚ä¹‹åæ‹¿è¿™äº›featureè®­ç»ƒä¸€ä¸ªlinear classifierã€‚æ¨æ–­æ—¶æŠŠè¿™ä¸ªclassifieræ¥åˆ°CLIP image encoderä¸Šï¼ˆ**å› ä¸ºåœ¨joint image-textç©ºé—´ä¸­ï¼Œå­˜åœ¨cross-modal transferability phenomenon**ï¼‰ã€‚ [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Cho_PromptStyler_Prompt-driven_Style_Generation_for_Source-free_Domain_Generalization_ICCV_2023_paper.html)

26. **Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization** (ICML 2023) åˆ©ç”¨å¾ˆå¤šåœ¨ä¸åŒä»»åŠ¡ä¸Šfine-tuneçš„åŒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ¥æå‡OODæ€§èƒ½ [[paper]](https://proceedings.mlr.press/v202/rame23a/rame23a.pdf) 

27. **Explore and Exploit the Diverse Knowledge in Model Zoo for Domain Generalization** (ICML 2023) å……åˆ†åˆ©ç”¨å¤šæ ·çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¸ä»…ä»…æ˜¯åˆ©ç”¨æœ€å¼ºçš„æ¨¡å‹ï¼‰æ¥æå‡OODæ€§èƒ½ [[paper]](https://proceedings.mlr.press/v202/chen23m/chen23m.pdf)

28. **Understanding and Improving Feature Learning for Out-of-Distribution Generalization** (NeurIPS 2023) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/d73d5645ddbb9ada6c862116435574f6-Paper-Conference.pdf)

    


### 2022

1. **Out-of-distribution Generalization with Causal Invariant Transformations** (CVPR2022) ç»™å‡ºäº†ä¸€ç§ä¸éœ€è¦discover causal featureçš„æ–¹æ³•ï¼Œè€Œæ˜¯ç›´æ¥æ‰¾Causal Invariant Transformationï¼Œè¯æ˜äº†å¦‚æœç½‘ç»œè¾“å‡ºå¯¹äºCITä¸å˜ï¼Œåˆ™å¯å®ç°OODæ³›åŒ– [[paper]](https://arxiv.org/abs/2203.11528)[[note]](./all_notes/Out-of-distribution_Generalization_with_Causal_Invariant_Transformations.md)
2. **Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization** (CVPR2022) å¯¹AdaINçš„æ”¹è¿›ï¼Œé€šè¿‡å¯¹é½å†…å®¹å›¾ç‰‡å’Œé£æ ¼å›¾ç‰‡çš„é«˜é˜¶ç»Ÿè®¡é‡ï¼Œæ¥å®ç°æ›´å¥½çš„é£æ ¼è¿ç§»ï¼Œå¹¶ç”¨è¿ç§»çš„å›¾ç‰‡æ¥å¢å¼ºæ³›åŒ–èƒ½åŠ› [[paper]](https://arxiv.org/abs/2203.07740) [[è®ºæ–‡åˆ†äº«ä¼šslides]](./all_notes/2022.11.17-EHM(final).pdf)
3. **Enhance the Visual Representation via Discrete Adversarial Training** (NeurIPS 2022) åˆ©ç”¨VQGANï¼Œåœ¨å…¶äº§ç”Ÿçš„ç¦»æ•£è¡¨ç¤ºä¸ŠåšATï¼Œç”Ÿæˆå…·æœ‰æ›´å°‘é«˜é¢‘å™ªå£°ã€æ›´å¤§èŒƒæ•°ã€æ›´ç¬¦åˆç°å®ä¸–ç•ŒOODåç§»çš„augmented dataï¼Œæ¥å¢å¼ºOODæ³›åŒ–ã€‚åˆ·çˆ†è®¸å¤šImageNet OOD datasetçš„SOTAã€‚ [[paper]](https://arxiv.org/abs/2209.07735) [[è®ºæ–‡åˆ†äº«ä¼šslides]](./all_notes/2022.12.15DAT.pdf)
4. **Pyramid Adversarial Training Improves ViT Performance** (CVPR2022) é€šè¿‡å…·æœ‰ä¸åŒå°ºåº¦ç»“æ„æ‰°åŠ¨çš„ATæ¥å¢å¼ºViTæ€§èƒ½ï¼Œå­¦åˆ°çš„æ‰°åŠ¨èƒ½ä¿å­˜shapeä¿¡æ¯ [[paper]](https://arxiv.org/abs/2111.15121)
5. **CROSSMATCH: CROSS-CLASSIFIER CONSISTENCY REGULARIZATION FOR OPEN-SET SINGLE DOMAIN GENERALIZATION** (ICLR2022) å¾ˆéš¾çš„æ–°settingï¼Œopen-set single domain generalization [[paper]](https://openreview.net/forum?id=48RBsJwGkJf) [[notes]](./all_notes/cross-matching-ICLR22.pdf)
6. **UNCERTAINTY MODELING FOR OUT-OF-DISTRIBUTION GENERALIZATION** (ICLR2022) å‡è®¾æ•°æ®çš„å‡å€¼å’Œæ–¹å·®æœä»æ½œåœ¨çš„é«˜æ–¯åˆ†å¸ƒ $\mathcal{N}(\mu,\Sigma_\mu)$ å’Œ $\mathcal{N}(\sigma,\Sigma_\sigma)$ ï¼Œæå‡ºäº†ä¼°è®¡ $\Sigma_\mu$ å’Œ $\Sigma_\sigma$ çš„æ–¹æ³•ï¼Œå¹¶ç”±æ­¤æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼º [[paper]](https://arxiv.org/abs/2202.03958) [[notes]](./all_notes/uncertain.pdf)
7. **Gradient Matching for Domain Generalization** ï¼ˆICLR2022ï¼‰é€šè¿‡æœ€å¤§åŒ–ä¸åŒenvironmentçš„lossæ¢¯åº¦çš„å†…ç§¯ï¼Œæ¥å¯»æ‰¾domain invariant featuresã€‚[[paper]](https://openreview.net/forum?id=vDwBW49HmO) [[notes]](./all_notes/grad_fishr.pdf)
8. **Fishr: Invariant Gradient Variances for Out-of-distribution Generalization** (ICML2022) matchä¸åŒdomainçš„lossæ¢¯åº¦çš„åæ–¹å·®çŸ©é˜µ [[paper]](https://arxiv.org/abs/2109.02934) [[notes]](./all_notes/grad_fishr.pdf)
9. **Towards Principled Disentanglement for Domain Generalization** (CVPR2022 Oral) åˆ†åˆ«ä½¿ç”¨ä¸¤ä¸ªç½‘ç»œå»ºæ¨¡semanticç‰¹å¾å’Œvariationç‰¹å¾ï¼ˆnon-causalï¼‰ï¼Œå¹¶é€šè¿‡è¿™ä¸¤ä¸ªç‰¹å¾é‡å»ºxï¼Œè¦æ±‚é‡å»ºè¿‡ç¨‹å…³äºvariationç‰¹å¾ä¸å˜ï¼Œæå‡ºäº†æ‰€è°“çš„*Invariance based on disentanglement*ã€‚ [[paper]](https://arxiv.org/abs/2111.13839)
10. **On the Strong Correlation Between Model Invariance and Generalization** (NeurIPS2022) æå‡ºäº†æ–°çš„ä¸å˜æ€§è¡¡é‡æ ‡å‡†EIï¼ˆè¡¡é‡çš„æ˜¯ç½‘ç»œå¯¹äºxå’Œç»OODå˜æ¢åçš„x'çš„é¢„æµ‹çš„å·®è·ï¼‰ï¼Œå‘ç°å…¶å¯¹äºä¸å˜æ€§çš„åˆ»ç”»è¿œä¼˜äºJS divergenceã€‚åŒæ—¶éªŒè¯äº†EIä¸OODæ³›åŒ–æ€§èƒ½çš„æ­£æ¯”å…³ç³»ã€‚[[paper]](https://arxiv.org/pdf/2207.07065.pdf)
11. **OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization** (CVPR2022 Oral) diversity shift/Correlation shift [[paper]](https://arxiv.org/abs/2106.03721) [[notes(åŒ…å«å…³äºä¸¤ç§OODæ•°æ®é›†æ‰€å¯¼å‡ºçš„ P(X) ä»¥åŠ P(Y|X)) æ€§è´¨çš„æ€»ç»“]](./all_noets/oodbench.pdf)
12. **Assaying Out-Of-Distribution Generalization in Transfer Learning** (Arxiv 2022) å„ç§metricå¯¹OOD accå½±å“çš„å®éªŒæ€§æ€»ç»“ [[paper]](https://arxiv.org/abs/2207.09239)
13. **On Certifying and Improving Generalization to Unseen Domains** (Arxiv 2022) å‘ç°äº†æ¨¡å‹å¯¹äºåœ¨åŒä¸€åˆ†å¸ƒè·ç¦»ä¸‹ä¸åŒç±»å‹OOD shiftçš„æ€§èƒ½æœ‰è¾ƒå¤§å·®å¼‚ï¼Œæå‡ºäº†minimizeæŸä¸€è·ç¦»ä¸‹worst-case lossã€‚ [[paper]](https://arxiv.org/abs/2206.12364) [[notes]](./all_notes/certify.md)
14. **ZooD: Exploiting Model Zoo for Out-of-Distribution Generalization** æå‡ºäº†ä¸€ç§OOD accuracyçš„metricï¼šä»å¤šä¸ªé¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨ä¸­ä¸­æ ¹æ®MLEé€‰å‡ºä½¿ä¼¼ç„¶$p(y'|\phi',y,\phi)$å’Œ$p(\phi'|\phi)$æœ€å¤§çš„æ¨¡å‹ã€‚åœ¨domainbedæœ‰å¤§å¹…æå‡ã€‚[[paper]](https://arxiv.org/pdf/2210.09236.pdf)
15. **ID AND OOD PERFORMANCE ARE SOMETIMES INVERSELY CORRELATED ON REAL-WORLD DATASETS** (Arxiv 2022) å‘ç°äº†ä¹‹å‰å·¥ä½œä¸­å¿½ç•¥çš„OODå’ŒID accurayå‘ˆåç›¸å…³çš„æƒ…å†µï¼ˆæ–‡ç« claimè¿™æ˜¯ç”±äºdiversity-inducing methodä¸å¤Ÿå¤šã€ä¸åŒæ¨¡å‹çš„è¡¨ç°åœ¨åŒä¸€training epochæ˜¯ä¸åŒçš„è€Œä¸åº”è¯¥æ··åœ¨ä¸€èµ·ã€ä¸æ˜¯æ‰€æœ‰çš„pretrain seedéƒ½ä¼šå‡ºç°è€Œå¼•èµ·ï¼‰ [[https://arxiv.org/pdf/2209.00613.pdf]]
16. **Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift** (NeurIPS 2022) åœ¨å¯ä»¥è·å–OOD unlabelled dataçš„æƒ…å†µä¸‹ï¼Œå‘ç°IDå’ŒOOD accuracyæœ‰å¼ºæ­£ç›¸å…³ $\leftrightarrow$ IDå’ŒOODçš„agreementå¼ºæ­£ç›¸å…³ [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/7a8d388b7a17df480856dff1cc079b08-Paper-Conference.pdf)
17. **When Does Group Invariant Learning Survive Spurious Correlations?** (NeurIPS 2022) æŒ‡å‡ºäº†group invariant learningå¿…é¡»æ»¡è¶³çš„ä¸¤ä¸ªå‡†åˆ™
18. **Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions with Superior OOD Generalization** (CVPR 2022) ä¸»å¼ ä½¿ç”¨å¤šä¸ªåˆ†ç±»å™¨ï¼Œå¹¶é¼“åŠ±è¿™äº›åˆ†ç±»å™¨è¾“å‡ºå¯¹è¾“å…¥ç‰¹å¾çš„æ¢¯åº¦å·®å¼‚å°½é‡å¤§ï¼Œä»¥å¾—åˆ°diverseçš„æ¨¡å‹ã€‚ä¹‹åä¾èµ–äºOOD validation é€‰å‡ºä¸€ä¸ªæœ€å¥½çš„æ¨¡å‹ä½œä¸ºæœ€ç»ˆæµ‹è¯•çš„æ¨¡å‹ã€‚ [[paper]](https://ieeexplore.ieee.org/document/9878716/) 
19. **Rich Feature Construction for the Optimization-Generalization Dilemma** (ICML 2022) ERM pre-trainå¯¹äºæå‡OOD objectivveçš„æ€§èƒ½å¾ˆå…³é”® [[paper]](https://proceedings.mlr.press/v162/zhang22u/zhang22u.pdf)
20. **Diverse Weight Averaging for Out-of-Distribution Generalization** (DiWA) (NeurIPS 2022) ensemblingï¼Œå¹³å‡ä¸åŒè¶…å‚çš„æ¨¡å‹ [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/46108d807b50ad4144eb353b5d0e8851-Paper-Conference.pdf)
21. **Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization** (EoA) (NeurIPS 2022) å…ˆå¯¹æ¯ä¸ªæ¨¡å‹åšæ²¿è®­ç»ƒè½¨è¿¹çš„moving averageï¼ˆä¸åŒepochï¼‰ï¼Œå†å¯¹è¿™äº›moving averageå¾—åˆ°çš„æ¨¡å‹åšensemblingï¼ˆä¸åŒè¶…å‚ï¼‰ã€‚å…·ä½“åšæ³•ï¼šsimply combines all checkpoints uniformly starting from batch 100 until the end of training [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/372cb7805eaccb2b7eed641271a30eec-Paper-Conference.pdf)
22. **Model Agnostic Sample Reweighting for Out-of-Distribution Learning** (MAPLE) (ICML 2022) æå‡ºäº†ä¸€ä¸ªè¿­ä»£è¿›è¡Œçš„bilevel optimizationè¿‡ç¨‹ï¼š(1)ä¼˜åŒ–ä»»æ„çš„ä¸€ä¸ªOOD lossæ›´æ–°æ¨¡å‹å‚æ•°$\theta^*$ (2)åœ¨æ›´æ–°å¾—åˆ°çš„$\theta^*$ä¸‹ä¼˜åŒ–ERM lossï¼Œæ›´æ–°weight $w$
23. 

### 2021

1. **The Many Faces of Robustness: A Critical Analysis of OOD Generalization** (ICCV 2021) OOD robustnessä¸åº”è¯¥æ˜¯ä¸€ä¸ªç®€å•çš„æŒ‡æ ‡ï¼Œå®ƒåœ¨ä¸åŒdistribution shiftä¸‹åº”è¯¥æ˜¯ä¸åŒçš„æŒ‡æ ‡ã€‚éš¾ä»¥æ¨æ–­ç°æœ‰æ–¹æ³•ä¸­å“ªä¸ªèƒ½æ›´å¹¿æ³›åœ°work[[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Hendrycks_The_Many_Faces_of_Robustness_A_Critical_Analysis_of_Out-of-Distribution_ICCV_2021_paper.pdf)[[note]](./all_notes/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-OOD-Generalization.pdf)
2. **Improved OOD Generalization via Adversarial Training and Pre-training** (ICML 2021) ä»ç†è®ºä¸Šè¯æ˜äº†åœ¨ç”¨Wassersteinè·ç¦»åˆ»ç”»åˆ†å¸ƒåç§»çš„æƒ…å†µä¸‹ï¼ŒATå¯¹ä¸€å®šè·ç¦»å†…çš„OODæ•°æ®æœ‰æ³›åŒ–èƒ½åŠ› [[paper]](https://arxiv.org/abs/2105.11144) 
3. **Learning Representations that Support Robust Transfer of Predictors** (Arxiv 2021) æå‡ºäº†TRM(Transfer Risk Minimization)ï¼ŒæŠŠæ¨¡å‹åœ¨ä¸åŒdomainçš„æ³›åŒ–èƒ½åŠ›ç›´æ¥ä½œä¸ºobjectiveä¼˜åŒ– [[paper]](https://arxiv.org/abs/2110.09940) [[notes]](./all_notes/TRM.pdf)
4. **Learning Causal Semantic Representation for OoD prediction** (NeurIPS 2021) å¾ˆç¡¬æ ¸ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„Causal Semantic Modelï¼Œæ–‡ç« å…³é”®å‡è®¾æ˜¯ $p(x|s,v)$ å’Œ $p(y|s)$ åœ¨domainé—´ä¿æŒä¸å˜ï¼Œ $p(s,v)$ æ˜¯domain changeçš„å”¯ä¸€æ¥æºã€‚è¯æ˜äº†causalæœºåˆ¶çš„å¯è¾¨è¯†æ€§  [[paper]](https://proceedings.neurips.cc/paper/2021/file/310614fca8fb8e5491295336298c340f-Paper.pdf) [[notes]](./all_notes/causal1.pdf)
5. **Exploiting Domain-Specifific Features to Enhance Domain Generalization** (NeurIPS 2021) ä½¿ç”¨äº†å¾ˆå¤štrickï¼šä»information bottleneckçš„è§’åº¦è¯æ˜äº†domain-specific featureå¯¹æ³›åŒ–ä¹Ÿæ˜¯æœ‰å¸®åŠ©çš„ï¼›æå‡ºäº†ä¸€ç§åŸºäºæœ€å°åŒ–domain-invariant featureå’Œdomain-specific featureä¹‹é—´åæ–¹å·®çŸ©é˜µçš„è§£è€¦ï¼›æå‡ºé€šè¿‡meta learningæ¥åˆ©ç”¨domain-specific featureçš„æ³›åŒ–èƒ½åŠ›ã€‚[[paper]](https://arxiv.org/abs/2110.09410) [[notes]](./all_notes/disen1.md)
6. **Towards a Theoretical Framework of Out-of-Distribution Generalization** (NeurIPS 2021) é€šè¿‡å®šä¹‰äº†conditioned on classçš„è¡¨ç¤ºä¸å˜æ€§ï¼Œä»¥åŠè¡¨ç¤ºçš„informativenessï¼Œå¹¶æ®æ­¤å®šä¹‰äº†ä»€ä¹ˆæ˜¯å¯å­¦ä¹ çš„OODé—®é¢˜ï¼Œæ¨å¯¼å‡ºäº†OODæ³›åŒ–è¯¯å·®ä¸Šç•Œå¯ä»¥è¢«è¿™ä¸¤ä¸ªé‡è¡¨ç¤º [[paper]](https://proceedings.neurips.cc/paper/2021/hash/c5c1cb0bebd56ae38817b251ad72bedb-Abstract.html) 
7. **Quantifying and Improving Transferability in Domain Generalization** (NeurIPS 2021)  æå‡ºäº†åŸºäºç»™å®šå‡è®¾æ—çš„åŸŸé—´å¯è¿ç§»æ€§çš„æŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå®¹æ˜“è¿ç§»/æ³›åŒ–ï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªmin-maxä¼˜åŒ–è¿‡ç¨‹æ¥æå‡è¿ç§»æ€§ï¼ˆæœ‰ç†è®ºä¿è¯ï¼‰ [[paper]](https://arxiv.org/abs/2106.03632)
8. **SWAD: Domain Generalization by Seeking Flat Minima** (NeurIPS 2021) æå‡ºäº†SWADï¼ˆå¹³å‡åŒä¸€æ¨¡å‹çš„ä¸åŒepochçš„å‚æ•°ï¼‰ï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†flatçš„loss landscapeå¯¹OODæ³›åŒ–æœ‰å¸®åŠ© [[paper]](https://arxiv.org/pdf/2102.08604.pdf)
9. **Domain Generalization Using Causal Matching** (ICML 2021) å¯¹é½ä¸åŒç¯å¢ƒçš„åŒä¸€ç±»åˆ«çš„è¡¨ç¤ºæ¥å­¦ä¹ ä¸å˜ç‰¹å¾ã€‚[[paper]](http://arxiv.org/abs/2006.07500)
10. **Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization** (Arxiv 2021) æå‡ºäº†IB-IRMï¼Œå³åŠ äº†ä¸€ä¸ªæœ€å°åŒ–æ¨¡å‹è¡¨ç¤ºçš„ç†µçš„ç›®æ ‡ã€‚æœ¬æ–‡çš„ä¸€ä¸ªæ ¸å¿ƒè§‚å¯Ÿæ˜¯ï¼šå½“Yå’Œspurious featureæ²¡æœ‰å…³è”ï¼ˆæœ¬æ–‡ç§°ä½œFIIFï¼Œfully informative invariant featuresï¼Œå¯¹åº”äºcovariate shiftçš„æƒ…å†µï¼‰æ—¶ï¼Œæ™®é€šIRMä¼šå¤±æ•ˆï¼›åœ¨æ–‡ä¸­æ„å»ºçš„FIIF exampleä¸‹ï¼Œåªä½¿ç”¨inv featureæˆ–åªä½¿ç”¨sp featureéƒ½ä¼šå¯¼è‡´è¡¨ç¤ºçš„ç†µæ¯”åŒæ—¶ä½¿ç”¨invå’Œsp featureçš„è¡¨ç¤ºçš„ç†µæ›´ä½ï¼Œå› æ­¤IBçš„ç›®æ ‡ä¼šå¯¼è‡´åªä½¿ç”¨invæˆ–åªä½¿ç”¨spçš„è§£ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šæœ€å°åŒ–åˆ†ç±»lossï¼Œå°±èƒ½å­¦åˆ°ä»…ä¾èµ–äºinvçš„è§£ã€‚ [[paper]](http://arxiv.org/abs/2106.06607) [[notes]](./all_notes/IB-IRM.md)
11. 

### 2020

1. **Out-of-Distribution Generalization via Risk Extrapolation (V-REx)** (2020) ç›®æ ‡ï¼š

   $\min _{\omega, \Phi} \sum^e \mathcal{R}^e(\omega, \Phi)+\lambda \text{Var}\left(\mathcal{R}^e(\omega, \Phi)\right)$ï¼ŒåŠ¨æœºï¼šå¦‚æœ$\Phi$æ˜¯ä¸å˜è¡¨ç¤ºï¼Œé‚£ä¹ˆä¸åŒç¯å¢ƒeåº”è¯¥æ»¡è¶³ $P^e(Y \mid \Phi(X))=P(Y \mid \Phi(X))$ ï¼Œæ­¤æ—¶lossä¹Ÿåº”è¯¥ä¸€æ ·ï¼Œæ‰€ä»¥å¯¹lossçš„æ–¹å·®åšæ­£åˆ™ã€‚ï¼ˆä¸ªäººè§‰å¾—å­˜åœ¨é—®é¢˜ï¼Œå› ä¸ºå½“ä¸åŒç¯å¢ƒlossçš„å€¼ä¸€æ ·æ—¶ï¼Œæœ€ä¼˜åˆ†ç±»å™¨ä¹Ÿä¸ä¸€å®šä¸€æ ·ï¼‰[[paper]](https://arxiv.org/pdf/2003.00688.pdf)

2. **DISTRIBUTIONALLY ROBUST NEURAL NETWORKS FOR GROUP SHIFTS: ON THE IMPORTANCE OF REGULARIZATION FOR WORST-CASE GENERALIZATION (Group DRO)** (ICLR 2020) å°†æ•°æ®é›†åˆ†æˆ(spurious feature, label)çš„groupï¼Œæœ€å°åŒ–æ¯ä¸ªgroupä¸Šçš„lossçš„æœ€å¤§çš„å‡¸ç»„åˆï¼ˆæƒé‡å¯ä»¥ä¼˜åŒ–ï¼Œlossæ›´å¤§çš„groupçš„losså€¾å‘äºæœ‰æ›´å¤§çš„æƒé‡ï¼‰ [[paper]](https://arxiv.org/abs/1911.08731)

3. **AUGMIX: A SIMPLE DATA PROCESSING METHOD TO IMPROVE ROBUSTNESS AND UNCERTAINTY** (ICLR 2020) é¼“åŠ±æ¨¡å‹å¯¹äºç»è¿‡ä¸åŒdata augmentationå¢å¼ºåçš„æ ·æœ¬çš„é¢„æµ‹ä¸€è‡´æ€§ [[paper]](https://arxiv.org/pdf/1912.02781.pdf)

4. **Adversarial Examples Improve Image Recognition** (NeurIPS 2020)  Advprop [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Xie_Adversarial_Examples_Improve_Image_Recognition_CVPR_2020_paper.html)

5. **Improving out-of-distribution generalization via multi-task self-supervised pretraining** (Arxiv 2020) SSL pretraining tasks for OOD [[paper]](http://arxiv.org/abs/2003.13525)

### ç»å…¸æ–‡ç« 

1. **Domain-Adversarial Training of Neural Networks** (2016) æå‡ºDANNï¼Œè®­ç»ƒdomain labelåˆ¤åˆ«å™¨æ¥ä¿ƒä½¿ç‰¹å¾æå–å™¨å­¦ä¹ åˆ°domain-invariantçš„ç‰¹å¾ [[paper]](https://www.jmlr.org/papers/volume17/15-239/15-239.pdf)
2. **In Search of Lost Domain Generalization** (ICLR 2021) Domainbed benchmark [[paper]](https://arxiv.org/abs/2007.01434)
3. **Invariant Risk Minimization** (2019) IRM æ³¨æ„ï¼šIRMçš„æ³›åŒ–è¯¯å·®boundä¸­ï¼ŒXæ˜¯é€šè¿‡è™šå‡ç‰¹å¾å’Œä¸å˜ç‰¹å¾concatenateå†ç»è¿‡çº¿æ€§å˜æ¢å¾—æ¥çš„ï¼Œå¾ˆstrict [[paper]](https://arxiv.org/abs/1907.02893)
4. **Self-Challenging Improves Cross-Domain Generalization** (Arxiv 2020) RSC 
5. **UNDERSTANDING THE FAILURE MODES OF OUT-OFDISTRIBUTION GENERALIZATION** (ICLR 2021) åˆ»ç”»äº†ERM OODæ³›åŒ–å¤±è´¥çš„ä¸¤ç§æƒ…å†µï¼šgeometric skewå’Œstatistical skewã€‚ç›´è§‚ç†è§£ï¼šgeometric skewæ˜¯ç”±äºmax-margin classifierç”±äºmajor partçš„å­˜åœ¨å¯¼è‡´ä¸ºäº†ç»´æŒæœ€å¤§marginï¼Œä¸å¾—ä¸ä½¿ç”¨spurious featureï¼›statistical skewçº¯ç²¹æ˜¯ç”±äºåˆ†æ$\frac{w_{\text{sp}(t)}}{w_{\text{inv}(t)}}$çš„æ”¶æ•›ï¼Œå‘ç°è¯¥æ¯”å€¼çš„lower boundä¼šéšç€spurious correlation pçš„å¢åŠ è€Œå•è°ƒä¸Šå‡ã€‚ [[paper]](http://arxiv.org/abs/2010.15775)







## Test-time adaptation for Generalization

### 2024

1. **Context is Environment** (ICLR 2024) [[paper]](https://openreview.net/forum?id=8VPWfqtQMX) æå‡ºä»¥ä¸€ç§ICLçš„èŒƒå¼æ¥å¸®åŠ©domain generalizationï¼Œ æœ¬è´¨ä¸Šæ˜¯ä¸€ç§test-time adaptation

### 2023

1. **Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization** [[paper]](https://arxiv.org/pdf/2311.01459.pdf)
2. 




## Domain Generalization/Adaptation in Down-stream CV/NLP Tasks

### 2023

1. **CLIP the Gap: A Domain Generalization Approach for Object Detection** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Vidit_CLIP_the_Gap_A_Single_Domain_Generalization_Approach_for_Object_CVPR_2023_paper.html)
2. **Improving Zero-Shot Generalization and Robustness of Multi-Modal Models** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.html)
3. **Towards Domain Generalization for Multi-View 3D Object Detection in Bird-Eye-View** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Towards_Domain_Generalization_for_Multi-View_3D_Object_Detection_in_Bird-Eye-View_CVPR_2023_paper.html)
4. **Single Domain Generalization for LiDAR Semantic Segmentation** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Single_Domain_Generalization_for_LiDAR_Semantic_Segmentation_CVPR_2023_paper.html)
5. **Modality-Agnostic Debiasing for Single Domain Generalization** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Qu_Modality-Agnostic_Debiasing_for_Single_Domain_Generalization_CVPR_2023_paper.html)
6. **Improving Zero-Shot Generalization for CLIP with Synthesized Prompts** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Improving_Zero-Shot_Generalization_for_CLIP_with_Synthesized_Prompts_ICCV_2023_paper.html)
7. **Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation** (ICCV 2023) [[paper]](https://arxiv.org/abs/2303.16456)
8. **Bayesian Prompt Learning for Image-Language Model Generalization** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.html) 



## Graph OOD Generalization (graph-level & node-level)

### 2024

1. **Graph Out-of-Distribution Generalization via Causal Intervention** (WWW 2024) [[paper]](http://arxiv.org/abs/2402.11494) node-levelã€‚æäº†ä¸€ç§data drivençš„æ–¹æ³•æ¥æ¨æµ‹ç¯å¢ƒæ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶å¯¹æ¯ä¸ªç¯å¢ƒçš„aggregationè¿‡ç¨‹æ·»åŠ ä¸€ä¸ªç¯å¢ƒç›¸å…³çš„å¯è®­ç»ƒçš„projector

### 2023

1.  **Towards Understanding Generalization of Graph Neural Networks** (ICLR 2023) åˆ†æäº†ä¸åŒGNNæ¶æ„å¯¹æ³›åŒ–è¯¯å·®çš„å½±å“ã€‚ç‰¹åˆ«åœ°ï¼Œå¯¹äºGNNæœ€å¤§åº¦æ•°è¶Šå°ï¼Œæ³›åŒ–è¯¯å·®è¶Šå°ã€‚ [[paper]](https://openreview.net/forum?id=BhMyLk0YNy)
2.  **Multi-task Self-supervised Graph Neural Networks Enable Stronger Task Generalization** (ICLR 2023) åˆ©ç”¨Multiple Gradient Descentæ¥åŒæ—¶ä¼˜åŒ–å¤šä¸ªSSLä»»åŠ¡çš„æŸå¤±ï¼Œä»¥å®ç°task generalization [[paper]](https://openreview.net/forum?id=1tHAZRqftM)
3.  **Mind the Label Shift of Augmentation-based Graph OOD Generalization (LiSA)** (CVPR 2023) æœ€å°åŒ–å¢å¼ºå›¾çš„åˆ†ç±»è¯¯å·®ä»¥åŠå¢å¼ºå›¾ä¸åŸå›¾ï¼ˆå­å›¾å’Œå…¨å›¾ï¼‰çš„äº’ä¿¡æ¯ï¼ŒåŒæ—¶æœ€å¤§åŒ–ä¸åŒå­å›¾ä¹‹é—´åŸºäºenergy functionè¡¡é‡çš„åˆ†å¸ƒè·ç¦»ï¼Œæ¥è·å¾—ä¸€ç³»åˆ—label-invariantçš„å­å›¾ï¼ˆä¸€ä¸ªå­å›¾ç”Ÿæˆå™¨æ¨¡æ‹Ÿäº§ç”Ÿä¸€ä¸ªç¯å¢ƒçš„æ•°æ®ï¼‰ã€‚å¾—åˆ°ä¸åŒç¯å¢ƒæ•°æ®åå†min loss+varï¼ˆå¸¸è§„åšæ³•ï¼‰ [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Mind_the_Label_Shift_of_Augmentation-Based_Graph_OOD_Generalization_CVPR_2023_paper.html)
4.  **Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization (LECI)** (Arxiv 2023) **[FIIF&PIIF]** éœ€è¦ç¯å¢ƒæ ‡ç­¾ã€‚ä¼˜åŒ–ä¸¤ä¸ªç›®æ ‡ï¼š(1)è®©å­å›¾ç”Ÿæˆå™¨äº§ç”Ÿåˆ†ç±»è¯¯å·®å°½é‡å¤§çš„spurious subgraphï¼Œå¹¶å¯¹æŠ—åœ°è®­ç»ƒä¸€ä¸ªç”¨spurious graphåšé¢„æµ‹çš„åˆ†ç±»å™¨ (2)è®©å­å›¾ç”Ÿæˆå™¨äº§ç”Ÿæ— æ³•åˆ¤åˆ«ç¯å¢ƒçš„causal graphï¼Œå¹¶å¯¹æŠ—åœ°è®­ç»ƒä¸€ä¸ªç¯å¢ƒåˆ¤åˆ«å™¨ [[paper]](https://arxiv.org/abs/2306.01103)
5.  **RETHINKING INVARIANT GRAPH REPRESENTATION LEARNING WITHOUT ENVIRONMENT PARTITIONS (GALA)** (ICLR 2023 Domain Generalization Workshop) æ”¹è¿›ç‰ˆCIGAã€‚è®²CIGAçš„causal contrastive lossæ”¹ä¸ºå¯¹é½spurious correlationä¸»å¯¼å’Œinvariant correlationä¸»å¯¼çš„ä¸¤éƒ¨åˆ†å­é›†ï¼Œä¸åŒå…³è”ä¸»å¯¼çš„æ•°æ®é€šè¿‡ä¸€ä¸ªé¢å¤–çš„ERMæ¨¡å‹è·å¾—ï¼ˆERMé¢„æµ‹çš„labelä¸åŒï¼Œå°±è®¤ä¸ºæ˜¯å®ƒä»¬å±äºç”±ä¸åŒçš„å…³è”ä¸»å¯¼çš„partï¼‰ [[paper]](https://openreview.net/forum?id=bjw5jqGtDy)
6.  **FLOOD: A Flexible Invariant Learning Framework for Out-of-Distribution Generalization on Graphs** (KDD 2023) node-level OODä»»åŠ¡ã€‚é€šè¿‡å·²æœ‰çš„å›¾augmentationæ–¹æ³•äº§ç”Ÿä¸€ç³»åˆ—å¢å¼ºç¯å¢ƒï¼Œå†ä½¿ç”¨å¦‚VRExçš„ä¸å˜å­¦ä¹ ç›®æ ‡ï¼›åŒæ—¶åŠ ä¸€ä¸ªBootstrapped Representation Learningç›®æ ‡ã€‚ [[paper]](https://dl.acm.org/doi/10.1145/3580305.3599355)
7.  **Causality and Independence Enhancement for Biased Node Classification** (CIKM 2023) æŠŠcausal featureçœ‹æˆdo(c)ï¼Œç„¶åå‡è®¾sæ˜¯cçš„backdoorï¼ŒåŸºäºæ­¤æ¨¡å‹æ¥ç”¨ä¸€ä¸ªç»éªŒæ€§æ–¹æ³•å»ºæ¨¡p(Y|C,S)ä»è€Œå­¦å‡ºcausal feature/spurious featureï¼ˆé€šè¿‡åœ¨node featureä¸ŠåŠ 2ä¸ªMLPå®ç°ï¼‰ã€‚æ„Ÿè§‰causal graphçš„å‡è®¾å¤ªå¼ºã€‚å¹¶ä¸”åªèƒ½è§£å†³concept shiftã€‚ [[paper]](https://dl.acm.org/doi/10.1145/3583780.3614804)
8.  **Learning Invariant Representations of Graph Neural Networks via Cluster Generalization** (NeurIPS 2023) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/8ed2293e714b7692b63117e330e551e8-Paper-Conference.pdf) è§£å†³ç»“æ„shiftï¼Œsemi-supervised settingã€‚é€šè¿‡èšç±»node featureè·å¾—ç¯å¢ƒï¼Œç„¶ååˆ©ç”¨èšç±»ä¿¡æ¯å¤–æ’node featureåšæ•°æ®å¢å¼ºã€‚
9.  **Stable Prediction on Graphs with Agnostic Distribution Shift** (KDD 2023) node-levelï¼Œå¯¹é½ä¸åŒç¯å¢ƒçš„aggregation weightï¼ˆè¦æ±‚æ•°æ®é›†æ˜¯ä¸åŒç¯å¢ƒçš„å›¾é•¿å¾—ä¸€æ ·ï¼‰å’Œä¸åŒç¯å¢ƒçš„lossï¼ˆVRExï¼‰ã€‚[[paper]](https://arxiv.org/abs/2110.03865) 
10.  **Individual and Structural Graph Information Bottlenecks for Out-of-Distribution Generalization** (TKDE 2023) æœ€å°åŒ–input graphå’Œä¸­é—´å±‚representationä¹‹é—´çš„äº’ä¿¡æ¯ä»¥æ¶ˆé™¤è™šå‡ç‰¹å¾ï¼Œæœ€å¤§åŒ–representationå’Œlabelä¹‹é—´çš„äº’ä¿¡æ¯æ¥å­¦ä¹ ä¸å˜ç‰¹å¾ã€‚[[paper]](http://arxiv.org/abs/2306.15902)
11.  

###  2022

1. **Learning Substructure Invariance for Out-of-Distribution Molecular Representations** (NeurIPS 2022 Spotlight) æ— ç¯å¢ƒæ ‡ç­¾çš„å›¾åˆ†ç±»ï¼Œèƒ½æ‹¿åˆ°å¤šä¸ªå›¾ï¼ˆåˆ†å­ï¼‰ã€‚å…ˆæ¨å¯¼å‡ºäº†ä¸€ä¸ªELBOä»¥è®­ç»ƒå‡ºä¸€ä¸ªç¯å¢ƒåˆ¤åˆ«å™¨ï¼Œä¼˜åŒ–ç›®æ ‡ä¸ºæœ€å¤§åŒ–å›¾è¡¨ç¤º $z$ ä¸æ ‡ç­¾ $y$ä¹‹é—´çš„äº’ä¿¡æ¯ã€æœ€å°åŒ–$I(y;e|z)$ã€‚[[paper]](https://openreview.net/forum?id=2nWUNTnFijm) [[notes]](./all_notes/molecular.pdf)
2. **Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs (CIGA)**  (NeurIPS 2022) **[FIIF&PIIF]** é¦–å…ˆé€šè¿‡gæ¥é¢„æµ‹causalå­å›¾ï¼Œå…¶è¡¥å›¾å°±æ˜¯spurious subgraphã€‚ç„¶åé€šè¿‡æ¨å¯¼è¯´æ˜äº†causal featureä¸ç¯å¢ƒæ— å…³çš„æ¡ä»¶å¯ä»¥è½¬åŒ–ä¸ºæœ€å¤§åŒ–åŒä¸€ç±»åˆ«çš„featureçš„äº’ä¿¡æ¯ï¼ˆé€šè¿‡ä¸€ä¸ªsupervised contrastive losså®ç°ï¼‰ã€‚åœ¨è¿™ä¸ªçº¦æŸä¸‹ï¼Œå†æœ€å¤§åŒ– $G_s$  $G_c$  åˆ†åˆ«ä¸yçš„äº’ä¿¡æ¯ï¼ˆ$G_s$ æ˜¯ $G_c$  çš„è¡¥å›¾ï¼Œæœ€å¤§åŒ–Gså’Œyçš„äº’ä¿¡æ¯æ˜¯ä¸ºäº†â€œå°†Gsä»Gcä¸­åˆ†ç¦»å‡ºå»â€ï¼‰ï¼ŒåŒæ—¶è¦ä¿è¯Gsä¸yçš„äº’ä¿¡æ¯æ¯”Gcå’Œyçš„äº’ä¿¡æ¯è¦å°ï¼ˆäº’ä¿¡æ¯çš„ä¼˜åŒ–é€šè¿‡ä¼˜åŒ–losså®ç°ï¼Œlossè¶Šå°äº’ä¿¡æ¯è¶Šå¤§ï¼‰[[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/8b21a7ea42cbcd1c29a7a88c444cce45-Supplemental-Conference.pdf) [[notes]](./all_notes/nips22_causal.md)
3. **Discovering Invariant Rationales for Graph Neural Networks (DIR)**  (ICLR 2022)  **[FIIF]** å°†ç‚¹ä¹‹é—´ç‰¹å¾å†…ç§¯æœ€å¤§çš„å­å›¾ä½œä¸ºcausalå­å›¾ï¼ŒåŒæ—¶æ‰‹åŠ¨åœ°æ”¹å˜causalå­å›¾å’Œspuriouså­å›¾çš„ç»„åˆï¼Œæ¨¡æ‹Ÿå¤šä¸ªsçš„ç¯å¢ƒã€‚ä¼˜åŒ–ç›®æ ‡ä¸ºä¼˜åŒ–æ‰€æœ‰sä¸‹çš„lossä»¥åŠå…¶æ–¹å·®ã€‚å…¶ä»–è¡¥å……ï¼šâ‘ åŠ äº†ä¸€ä¸ªpredictionå†…ç§¯é¡¹ä»¥å¯¹spurious correlationè¿›è¡Œæ­£åˆ™ â‘¡ä¸“é—¨å­¦ä¸€ä¸ªç”¨sæ¥é¢„æµ‹yçš„åˆ†æ”¯ï¼Œæœ‰ç‚¹ç±»ä¼¼Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs (NIPS 22)çš„åšæ³•ï¼Œæ˜¯ä¸ºäº†æ›´å¥½åœ°è§£è€¦cå’Œsã€‚æ€è€ƒï¼šcausalå­å›¾çš„å¯»æ‰¾æ–¹æ³•ç¼ºä¹ä¾æ® [[paper]](https://arxiv.org/abs/2201.12872) 
4. **Handling Distribution Shifts on Graphs: An Invariance Perspective (EERM)** (ICLR 2022) node-levelåˆ†ç±»ä»»åŠ¡ã€‚å…ˆé€šè¿‡ä¸€ä¸ªAT method ï¼Œè¿›è¡Œåˆ /å¢è¾¹å’Œæœ€å¤§åŒ–æ¯ä¸ªå­å›¾çš„æ–¹å·®æ¥è·å¾—kä¸ªå­å›¾ï¼ˆæ¥æ¨¡æ‹Ÿkä¸ªç¯å¢ƒï¼‰ï¼Œç„¶åå†minæ¯ä¸ªå­å›¾ï¼ˆç¯å¢ƒï¼‰ä¸Šlossçš„varianceå’Œåˆ†ç±»lossã€‚[[paper]](https://arxiv.org/abs/2202.02466) [[notes]](./all_notes/EERM.md)
5. **Shift-Robust Node Classification via Graph Adversarial Clustering** (Arxiv 2022) node-levelåˆ†ç±»ä»»åŠ¡ï¼Œopen-set DAåœºæ™¯ï¼Œä¸€ä¸ªå›¾é‡Œæœ‰labeled source dataå’Œunlabeled target dataã€‚è¯¥æ¡†æ¶å¯è§£å†³new classå’Œnew distributionçš„é—®é¢˜ã€‚å…ˆåœ¨unlabel target domainèšç±»ï¼Œç„¶åæŠŠèšç±»æ¨¡å‹æ‹¿åˆ°label sourceä¸Šå¯¹é½labelã€‚åŒæ—¶ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–èšç±»èƒ½åŠ›ï¼Œè¿˜æœ€å°åŒ–äº†sourceèšç±»ç»“æœå’ŒçœŸå®labelä¹‹é—´çš„KLæ•£åº¦ã€‚[[paper]](https://arxiv.org/pdf/2203.15802.pdf)
6. **GOOD: A Graph Out-of-Distribution Benchmark** (NeurIPS 2022) graph OOD benchmark [[paper]](http://arxiv.org/abs/2206.08452)
7. **Learning Invariant Graph Representations for Out-of-Distribution Generalization** (GIL) graph-level OODä»»åŠ¡ã€‚å…ˆåˆ’åˆ†sprious/causalå­å›¾ï¼Œç„¶åæ‹¿ç€spuriouså­å›¾èšç±»è·å¾—ç¯å¢ƒã€‚ä¹‹åç”¨ä¸€ä¸ªVRExç›®æ ‡ã€‚ [[paper]](https://openreview.net/forum?id=acKK8MQe2xc)

### 2021

1. **DISTRIBUTIONALLY ROBUST SEMI-SUPERVISED LEARNING OVER GRAPHS** (NeurIPS 2021) å¯¹æŠ—åœ°ä¼˜åŒ–DROçš„surrogate lossï¼Œæ¥è·å¾—å¯¹wassersteinè·ç¦»æ„ä¹‰ä¸‹distributionally robustçš„æ¨¡å‹ã€‚[[paper]](http://arxiv.org/abs/2110.10582)
1. **Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training Data** (NeurIPS 2021) ä¸€ç§è§£å†³node-level OOD åˆ†ç±»çš„ç½‘ç»œã€‚[[paper]](https://proceedings.neurips.cc/paper/2021/file/eb55e369affa90f77dd7dc9e2cd33b16-Paper.pdf)
1. **Mixup for Node and Graph Classification** (WWW 2021) å°†è¡¨ç¤ºmixupï¼Œå¹¶å°†å¯¹åº”çš„label mixupèµ·æ¥ï¼Œå°±èƒ½æå‡æ³›åŒ–ã€‚æ–‡ç« æå‡ºäº†node-levelå’Œgraph-levelåˆ†åˆ«çš„mixupæ–¹æ¡ˆï¼Œåç»­æ–‡ç« éªŒè¯äº†node-levelçš„æ–¹æ¡ˆè¦æ¯”graphçš„è¡¨ç°å¥½ [[paper]](https://dl.acm.org/doi/10.1145/3442381.3449796)

### 2020

1. **DROPEDGE: TOWARDS DEEP GRAPH CONVOLUTIONAL NETWORKS ON NODE CLASSIFICATION** (ICLR)



## Domain Adaptation

### 2023

1. **Diffusion-based Target Sampler for Unsupervised Domain Adaptation** ç”¨diffusion modelç”Ÿæˆå’Œç›®æ ‡åŸŸç‰¹å¾ä¸€è‡´çš„æ ·æœ¬æ¥å®ç°DA (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2303.12724.pdf)

### 2019

1. **On Learning Invariant Representations for Domain Adaptation** (ICML2019) ç»å…¸ä¹‹ä½œï¼ŒæŒ‡å‡ºäº†å­¦ä¹ ä¸å˜è¡¨ç¤ºçš„åšæ³•çš„é—®é¢˜ï¼šåœ¨æºåŸŸå’Œç›®æ ‡åŸŸçš„labelçš„è¾¹ç¼˜åˆ†å¸ƒ $p(Y)$ çš„å·®è·è¾ƒå¤§æ—¶ï¼Œå¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸä¸å˜è¡¨ç¤º+æœ€å°åŒ–æºåŸŸè¯¯å·®çš„åšæ³•ä¼šå¯¼è‡´ç›®æ ‡åŸŸè¯¯å·®å˜å¤§ [[paper]](http://proceedings.mlr.press/v97/zhao19a/zhao19a.pdf) [[notes]](./all_notes/On-Learning-Invariant-Representations-forDA.md)
2. **Support and Invertibility in Domain-Invariant Representations** (PMLR2019) ä¹Ÿclaimåªå¯¹é½è¡¨ç¤ºçš„è¾¹ç¼˜åˆ†å¸ƒ $p_s(Z) \approx p_t(Z)$ å¯¹äºDAæ˜¯ä¸å¤Ÿçš„ [[paper]](http://proceedings.mlr.press/v89/johansson19a.html)







## LLMs

### 2024

1. **Model Editing with Canonical Examples** [[paper]](http://arxiv.org/abs/2402.06155) æå‡ºäº†ä¸€ä¸ªæ–°ä»»åŠ¡ï¼šè®©æ¨¡å‹å­¦ä¹ å‡ ä¸ªç‰¹å®šçš„æ–‡æœ¬ä¾‹å­ï¼Œä»¥å®ç°æŸäº›çº æ­£ï¼ŒåŒæ—¶è¿˜ä¸èƒ½è®©æ¨¡å‹æ”¹å˜å¾ˆå¤šã€‚
1. **Evaluating Large Language Models at Evaluating Instruction Following** [[paper]](https://openreview.net/forum?id=tr0KidwPLc) (ICLR 2024) 
1. **Not all Layers of LLMs are Necessary during Inference** (Arxiv April 2024) è®­ç»ƒä¸€ä¸ªå¯¹LLMä¸­é—´å±‚featureçš„åˆ†ç±»å™¨åˆ¤æ–­æ˜¯å¦åº”è¯¥æ—©åœæ¥è·å–æ—©åœå±‚æ•°ï¼Œæ¥åŠ é€ŸLLMæ¨ç†ã€‚è¿˜å‘ç°ä¸­é—´å±‚é¢„æµ‹çš„top probå’Œtop prob-second top probåœ¨å„ä¸ªä»»åŠ¡ä¸Šéƒ½å‘ˆç°å‡ºéšç€å±‚æ•°åŠ æ·±è€Œå¢åŠ å¹¶é€æ¸ç¨³å®šçš„è¶‹åŠ¿ï¼ˆä½†åœ¨ä¸åŒä»»åŠ¡ä¸Šå±‚æ•°ä¸ä¸€æ ·ï¼‰ã€‚[[paper]](http://arxiv.org/abs/2403.02181)
1. **Demonstrating Mutual Reinforcement Effect through Information Flow** (Arxiv March 2024) [[paper]](https://arxiv.org/pdf/2403.02902) ç ”ç©¶äº†åŒæ—¶è¿›è¡Œwordåˆ†ç±»å’Œtextåˆ†ç±»çš„MREï¼ˆMutual Reinforcement Effectï¼‰ä»»åŠ¡ï¼Œä¹Ÿè§‚å¯Ÿåˆ°äº†anchoré‚£ç¯‡ä¸­çš„ä¸‰ç§attention activationéšlayerçš„åˆ†å¸ƒè¶‹åŠ¿ã€‚
1. **A Theoretical Understanding of Self-Correction through In-context Alignment** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2405.18634) ç†è®ºåˆ†ætransformerä¸­çš„å„ä¸ªæ¨¡å—åœ¨self-correctionä¸­å‘æŒ¥çš„ä½œç”¨
1. **Mechanics of Next Token Prediction with Self-Attention** (AISTATS 2024) [[paper]](https://proceedings.mlr.press/v238/li24f.html) æ„é€ äº†ä¸€ä¸ªgraphæ¥æè¿°next token predictionä»»åŠ¡ï¼Œåœ¨ç®€åŒ–settingä¸‹ç†è®ºåˆ†æå‡ºlast tokenæ›´å€¾å‘äºç»™æ›´ç»å¸¸ä½œä¸ºlabelçš„tokenåˆ†é…æ›´é«˜çš„attentionã€‚
1. **The pitfalls of next-token prediction** (Arxiv April 2024) [[paper]](http://arxiv.org/abs/2403.06963) æŒ‡å‡ºäº†è‡ªå›å½’æ¨¡å‹çš„ç¼ºé™·ï¼šé”™è¯¯æ»šé›ªçƒæ•ˆåº”å’Œåœ¨ä¸€ä¸ªå•ä¸€tokenè·¯å¾„ä¸Šåªèƒ½å­¦å‡ºä¸€ä¸ªç±»ä¼¼induction headçš„shortcutæ¨¡å‹
1. 

### 2023

1. **Instruction-following Evaluation through Verbalizer Manipulation** (Arxiv July 2023) [[paper]](http://arxiv.org/abs/2307.10558) å‘ç°LLMéµå¾ªflipped-label instructionsçš„èƒ½åŠ›å¾ˆå·®ï¼Œè¯´æ˜ICLå¯èƒ½åªæ˜¯ç›´æ¥åˆ©ç”¨äº†é¢„è®­ç»ƒè¯­æ–™çš„çŸ¥è¯†ï¼Œè€Œä¸æ˜¯å­¦ä¹ äº†contextã€‚å³ä½¿æ˜¯å¼ºå¦‚GPT-4çš„æ¨¡å‹ä¹Ÿä¸èƒ½å¾ˆå¥½åœ°éµå¾ªflipped-label instructionsã€‚
2. **Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks** (Arxiv Aug 2023) [[paper]](http://arxiv.org/abs/2307.02477) ä¸€äº›ä¸»è¦å‘ç°ï¼šâ‘ æ¨¡å‹åœ¨counterfactualçš„settingä¸­æ€§èƒ½ä¼šå˜å·®ï¼Œä¸”settingå’Œå¸¸è§çš„ã€ç¬¦åˆäº‹å®çš„settingç›¸å·®è¶Šè¿œï¼Œæ€§èƒ½è¶Šå·®ï¼Œè¯´æ˜äº†æ¨¡å‹å¯èƒ½çš„è®°å¿†ç°è±¡ã€‚â‘¡åœ¨ç®—æœ¯ä»»åŠ¡ä¸Šï¼ŒICLèƒ½æå‡counterfactualï¼ˆä¸åŒè¿›åˆ¶çš„è®¡ç®—ï¼‰æ€§èƒ½ï¼Œä½†å’Œdefault settingçš„å·®è·éš¾ä»¥æŠ¹å¹³ã€‚
3. **Can the Inference Logic of Large Language Models be Disentangled into Symbolic Concepts?** (Arxiv Apr 2023) [[paper]](https://arxiv.org/abs/2304.01083) æå‡ºäº†ä¸€ç§empiricalçš„æŒ‡æ ‡æ¥è¡¡é‡è¾“å…¥å¥å­é‡Œçš„æŸäº›è¯å’Œè¯ç»„å¯¹æŸä¸€ç‰¹å®šè¾“å‡ºçš„å†³å®šç¨‹åº¦ã€‚
4. **Contrastive Chain-of-Thought Prompting** (Arxiv Nov 2023) [[paper]](http://arxiv.org/abs/2311.09277) ä½¿ç”¨å¯¹æ¯”CoTï¼Œå³ä¸€ä¸ªæ­£ç¡®CoTæ­é…ä¸€ä¸ªé”™è¯¯CoTèƒ½ç›¸æ¯”å¸¸è§„çš„CoTå¸¦æ¥æå‡.

### 2022

1. **Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models** [[paper]](https://arxiv.org/pdf/2210.14199.pdf)

2. 

### 2021

**LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS** å°†å¯¹æ¨¡å‹æƒé‡çŸ©é˜µçš„æ›´æ–°é™åˆ¶ä¸ºä½ç§©çŸ©é˜µä¹˜ç§¯$BA$çš„å½¢å¼ï¼Œæå¤§å‡å°‘äº†pre-trained modelè¿ç§»åˆ°æ–°ä»»åŠ¡çš„ä»£ä»·ï¼ˆä¸ç”¨fine-tuneæ‰€æœ‰å‚æ•°ï¼‰ [[paper]](https://arxiv.org/abs/2106.09685)

### 2019

1. **Are Sixteen Heads Really Better than One?** (NeurIPS 2019) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html) åœ¨æŸäº›å±‚ä¸Šï¼Œåªç”¨ä¸€ä¸ªheadæ€§èƒ½ä¹Ÿèƒ½ä¿æŒä¸å˜ã€‚åŒæ—¶æå‡ºäº†ä½¿ç”¨attentionæ¢¯åº¦æ¥è¡¡é‡headçš„é‡è¦æ€§ï¼Œæå‡ºäº†å‰ªæç­–ç•¥ã€‚



## LVMs

### 2024

1. **VisionLLaMA: A Unified LLaMA Interface for Vision Tasks** (Arxiv Mar 2024) [[paper]](https://arxiv.org/pdf/2403.00522) Vision LLaMa

### 2022

1. **Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)** (Arxiv 2022) [[paper]](https://arxiv.org/abs/2205.01397)
2. 





## Prompt Learning

### Prompt learningï¼š
1. **Conditional Prompt Learning for Vision-Language Models** (CoCoOp, CVPR2022) å°†å›¾ç‰‡ç‰¹å¾ç›´æ¥åŠ åˆ°context tokenä¸Šï¼Œè·å¾—sample-wiseçš„promptï¼Œä»¥å®ç°instanceçš„generalizationã€‚å…¶å®å°±æ˜¯å¸Œæœ›é€šè¿‡å¼•å…¥å›¾åƒä¿¡æ¯æ¥ä½¿å¾—promptæè¿°å¾—æ›´è´´åˆ‡ã€‚ä¸è¿‡æ„Ÿè§‰è¿˜æ˜¯æœ‰ç‚¹æ€ªï¼Œå› ä¸ºæ‰€æœ‰classéƒ½åŠ ä¸Šäº†åŒæ ·çš„å¯å­¦ä¹ prefixï¼Œä¸ºä»€ä¹ˆèƒ½æé«˜é¢„æµ‹ä¸ºæ­£ç¡®ç±»çš„æ¦‚ç‡ï¼Ÿ
2. MaPLe: Multi-modal Prompt Learning, CVPR2023 
3. Prompt-aligned Gradient for Prompt Tuning, ICCV2023
4. Compound Text-Guided Prompt Tuning via Image-Adaptive Cues, AAAI2024
5. MmAP : Multi-modal Alignment Prompt for Cross-domain Multi-task Learning, AAAI2024
6. **Improving Zero-Shot Generalization for CLIP with Synthesized Prompts** (ICCV 2023)

### For DA:

1. Domain Adaptation via Prompt Learning, arxiv 2022
2. AD-CLIP: Adapting Domains in Prompt Space Using CLIP, ICCV2023
3. Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation, NIPS2023
4. Prompt-based Distribution Alignment for Unsupervised Domain Adaptation, AAAI2024

### For DG:

1. StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization, arxiv2023





## In-Context Learning

### 2024

1. **Explore Spurious Correlations at the Concept Level in Language Models for Text Classification** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2311.08648) å‘ç°äº†LLMåœ¨æ–‡æœ¬åˆ†ç±»ä¸­ä¼šä¾èµ–çš„concept-label spurious correlationï¼Œæå‡ºä½¿ç”¨ChatGPTæ¥æ‰©å……æ•°æ®æ¥æ¶ˆé™¤è™šå‡å…³è”ã€‚

2. **Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes** (ongoing work) [[paper]](Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes) å‘ç°æ¨¡å‹å¯¹äºdemonstrationçš„permutation invarianceæˆ–è®¸æ˜¯ICL OODçš„å…³é”®ã€‚æå‡ºä½¿ç”¨ç›¸åŒçš„positional encodingæ¥æå‡ICL OODæ€§èƒ½ã€‚

3. **Simple synthetic data reduces sycophancy in large language models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2308.03958) LLMsä¼šè¿åˆæé—®è€…çš„è§‚ç‚¹è€Œç½”é¡¾äº‹å®ã€‚æå‡ºåˆæˆä¸€äº›ç”¨æˆ·çš„è§‚ç‚¹å’Œæ­£ç¡®æ€§æ— å…³çš„æ–°promptï¼Œç„¶ååœ¨è¿™äº›æ•°æ®ä¸Šfine-tuneæ¥è§£å†³sycophancyé—®é¢˜ã€‚

4. **Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions** (ICLR 2024 Oral) [[paper]](https://arxiv.org/abs/2310.03016) æ¢ç©¶transformeråœ¨ä¸€ç³»åˆ—ç¦»æ•£ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚ç‰¹åˆ«åœ°ï¼Œå‘ç°ç»è¿‡é¢„è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”éšæœºåˆå§‹åŒ–çš„æ¨¡å‹è·å¾—äº†æ›´å¼ºçš„æœ€è¿‘é‚»ã€disjunctionå’Œconjunctionçš„èƒ½åŠ›ã€‚

5. **Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning**  (Arxiv Jan 2024) å‘ç°ä½¿ç”¨batch ICLï¼Œå°†Nä¸ªexampleè®¾ç½®ä¸ºNä¸ªone-shot inferenceï¼Œå†æŠŠæ¯ä¸ªinferenceå¾—åˆ°çš„tokenåšå¹³å‡ï¼Œæ›¿æ¢åˆ°query sampleåšaggregationæœ€ç»ˆå†é¢„æµ‹èƒ½å¸¦æ¥æå‡ã€‚ä¸€ä¸ªå¥‡ç‰¹çš„å‘ç°æ˜¯åšaggregationæ—¶ä»æŸä¸€å±‚å¾€ååšæ€§èƒ½ä¼šçªå¢ï¼Œåœ¨é‚£ä¹‹å‰æ€§èƒ½æ¥è¿‘é›¶ã€‚å¯¹æ­¤è§£é‡Šæ˜¯transformerçš„ä½å±‚æ˜¯åœ¨å­¦è¯­ä¹‰ä¿¡æ¯ã€‚

6. **RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.13463) è¯„ä¼°æ¨¡å‹çš„æ”¹å˜å®ƒä»¬çš„åŸå§‹è¾“å‡ºå¹¶éµå¾ªå’Œä¸€å¼€å§‹ç›¸è¿èƒŒçš„æŒ‡ä»¤çš„èƒ½åŠ›ã€‚ä¸»è¦è§‚å¯Ÿï¼š1)å¤§éƒ¨åˆ†æ¨¡å‹éƒ½ä¼šå€¾å‘äºéµå®ˆå®ƒä»¬çš„é¢„è®­ç»ƒçŸ¥è¯† 2)æ¨¡å‹å¾ˆéš¾æ ¹æ®äººç±»åç»­çš„åé¦ˆæ³›åŒ–åˆ°æ–°çš„é—®é¢˜ 3)æ‰€æœ‰æ¨¡å‹éƒ½ä¼šé€æ­¥å¿˜è®°äººç±»åé¦ˆå¹¶è½å›åˆ°å®ƒä»¬çš„å†…éƒ¨çŸ¥è¯†é‡Œ 4)æ¨¡å‹æ˜¯ä¸æ˜¯ç¬¬ä¸€æ—¶é—´éµå®ˆäº†äººç±»çš„åé¦ˆï¼Œå¯¹äºåç»­çš„è¡Œä¸ºèµ·åˆ°å…³é”®ä½œç”¨

7. **Function Vectors in Large Language Models** (ICLR 2024) [[paper]](http://arxiv.org/abs/2310.15213) å‘ç°context promptçš„æœ€åä¸€ä¸ªtokençš„éšå±‚è¡¨ç¤ºencodeäº†è¿™ä¸ªä»»åŠ¡çš„ä¿¡æ¯ï¼Œç§°ä¸ºfunction vectorï¼ˆFVï¼‰ã€‚å°†å…¶åŠ åˆ°zero-shotçš„promptä¸Šï¼Œå‘ç°æœ‰æ˜¾è‘—æå‡ã€‚

8. **A Data Generation Perspective to the Mechanism of In-Context Learning** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.02212) æœ‰å…³task recognitionå’Œtask learningçš„ç»¼è¿°

9. **Identifying and Analyzing Task-Encoding Tokens in Large Language Models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2401.11323) æ¢ç©¶äº†contextä¸­çš„templateè¯ï¼ˆ"data:","answer:"ï¼‰/stopwordï¼ˆæ ‡ç‚¹ã€è¿è¯ç­‰æ— æ„ä¹‰è¯ï¼‰/contentå¯¹performanceçš„æ„ä¹‰ã€‚ç»“æœå‘ç°templateè¯å¯¹ICLæ€§èƒ½æå‡æœ€æœ‰ç”¨ï¼Œcontentåè€Œæ²¡ä»€ä¹ˆç”¨ï¼›è¿˜æ¢ç©¶äº†templateè¯çš„ä»€ä¹ˆç‰¹å¾ä½¿å¾—å®ƒæœ‰åˆ«äºcontextä¸­çš„å…¶ä»–æˆåˆ†ï¼Œç»“æœå‘ç°templateè¯æœ¬èº«çš„è¯­ä¹‰ã€å…¶é‡å¤æ€§ã€å…¶åˆ†éš”xå’Œyçš„æ ¼å¼ä½œç”¨è¿™ä¸‰è€…éƒ½å¯¹ICLæ€§èƒ½æœ‰æ˜¾è‘—çš„ä½œç”¨ã€‚

10. **Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.19103) å‘ç°ï¼Œé—®é¢˜ä¸­çš„é”™è¯¯å‰æè€Œå¯¼è‡´çš„å›ç­”ä¸­çš„å¹»è§‰æ˜¯ç”±äºæ¨¡å‹ä¸­ç‰¹å®šçš„headçš„æ¿€æ´»æ‰€å¼•èµ·çš„ã€‚æå‡ºäº†ä¸€ç§å¼ºè¡Œæ¶ˆé™¤è¿™äº›headå¯¹äºé—®é¢˜ä¸­çš„é”™è¯¯å‰æå¯¹åº”çš„tokençš„attentionçš„æ–¹æ³•ã€‚

11. **In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering** (Arxiv Feb 2024) [[paper]](https://arxiv.org/abs/2311.06668) æå‡ºç”¨contextçš„ç¬¬Lå±‚è¡¨ç¤ºæ„é€ ä¸€ä¸ªè¡¨å¾ä»»åŠ¡ä¿¡æ¯çš„vectorï¼ˆICVï¼‰ï¼Œç„¶åå†åŠ åˆ°queryæ—¶çš„ç¬¬Lå±‚æ‰€æœ‰tokençš„è¡¨ç¤ºä¸Šã€‚

12. **The mechanistic basis of data dependence and abrupt learning in an in-context classification task** (ICLR 2024 Oral) [[paper]](https://arxiv.org/abs/2312.03002) æœ‰å…³transformer çš„IWLï¼ˆin-weights learningï¼‰å’ŒICLå­¦ä¹ è¿‡ç¨‹çš„å®éªŒæ€§åˆ†æã€‚åœ¨ä¸€ä¸ªä¸¤å±‚toy transformerä¸­æ­ç¤ºäº†induction headå­¦ä¹ æœºåˆ¶ã€‚

13. **Understanding In-context Learning From Repetitions** (ICLR 2024) [[paper]](https://openreview.net/forum?id=bGGYcvw8mp) æ­ç¤ºäº†contextä¸­é‡å¤å‡ºç°çš„patternä¼šå¯¼è‡´æ¨¡å‹æ›´å€¾å‘äºè¾“å‡ºè¿™ä¸ªpatternçš„ç°è±¡ã€‚

14. **In-context Learning Learns Label Relationships but is not Conventional Learning** (ICLR 2024) [[paper]](https://openreview.net/pdf?id=YPIA7bgd5y) ä»¥æ›´å¤§çš„æ¨¡å‹å’Œæ›´é•¿çš„contexté‡æ–°å®¡è§†ä»¥å¾€çš„ICLè®¨è®ºï¼Œå¹¶å¾—å‡ºäº†ä»¥ä¸‹ä¸‰ä¸ªç»“è®ºï¼š1)ICLä¼šå­¦x-yæ˜ å°„ï¼Œæ­£ç¡®çš„labelæ˜¯æœ‰ç”¨çš„ï¼Œä¸”æ¨¡å‹è¶Šå¤§è¿™ä¸€æ•ˆåº”è¶Šæ˜æ˜¾ 2)ICLèƒ½å­¦é¢„è®­ç»ƒæ—¶æ²¡è§è¿‡çš„æ–°ä»»åŠ¡ 3)å³ä½¿contextå¾ˆé•¿ï¼ŒICLä¹Ÿä¸èƒ½å½»åº•è¦†ç›–é¢„è®­ç»ƒè·å¾—çš„preference 4)LLMæ›´å…³æ³¨æ›´é è¿‘queryçš„example

15. **How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.02872) åœ¨ç®€å•çš„word classificationä»»åŠ¡ä¸Šï¼Œé¦–å…ˆæŒ‰ç…§ç±»ä¼¼Function Vectorçš„åšæ³•ï¼Œæå–å‡ºå¯¹è¾“å‡ºæ­£ç¡®é¢„æµ‹è´¡çŒ®æœ€å¤§çš„headã€‚ç„¶ååˆ†æè¿™äº›headå¹¶å‘ç°äº†å¦‚ä¸‹æœºåˆ¶ï¼šlabelçš„V encodeäº†labelçš„ç‰¹å¾ï¼Œlabelçš„K encodeäº†demonstrationçš„ç‰¹å¾ï¼›last tokençš„Q encodeäº†queryçš„ç‰¹å¾ï¼›last token queryå’Œæ­£ç¡®labelçš„Kçš„attention scoreæ¯”å…¶ä»–headçš„æ˜¾è‘—å¤§ï¼›last token Qä¸åœ¨contextä¸­å‡ºç°æ›´å¤šçš„label/æ›´é è¿‘queryçš„labelçš„Kçš„attention scoreæ›´å¤§ã€‚

16. **Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2312.12141) æå‡ºäº†ä¸€ç§å®šä½transformerä¸­å¯¹è¾“å‡ºæŸä¸€labelè´¡çŒ®æœ€å¤§çš„attentionæˆ–FFN layerï¼ˆæˆ–å…¶subvalueï¼‰çš„æ–¹æ³•ã€‚

17. **In-Context Learning State Vector with Inner and Momentum Optimization** (Arxiv April 2024) [[paper]](http://arxiv.org/abs/2404.11225) æäº†ä¸€ç§æ–°çš„ç”¨vectorå‹ç¼©ä¿¡æ¯çš„æŠ€æœ¯ï¼ˆState Vector SVï¼‰ï¼šæ˜¯å°†å‰Lå±‚çš„æ¯å±‚çš„attentionè¾“å‡ºconcatèµ·æ¥ã€‚ç„¶åæäº†ä¸‰ç§æŠ€æœ¯ï¼ˆaggregateæ¯ä¸€ä¸ªexampleçš„SVã€ç”¨momentumã€åˆ†ç»„æå–SVå†èšåˆï¼‰æ¥è¿›ä¸€æ­¥ä¼˜åŒ–SVï¼Œå–å¾—äº†ä¸€äº›æ€§èƒ½æå‡ã€‚

18. **GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network** (Arxiv Feb 2024)  [[paper]](http://arxiv.org/abs/2402.11709) æå‡ºå°†GNNæ’åœ¨LLMçš„æŸä¸€å±‚åé¢ï¼Œå¼ºè¡Œä½¿å¾—information flowï¼ˆtoken representationå°±æ˜¯node representationï¼‰æ˜¯ä»x->yå’Œy->:è¿è¾¹ï¼Œç„¶åå¾—åˆ°çš„node representationè¾“ç»™LLMçš„ä¸‹ä¸€å±‚ï¼ˆæ¯ä¸ªtokençš„éƒ½ä¿ç•™ç€ï¼Œå› ä¸ºGNNçš„è¾“å‡ºä¹Ÿæ˜¯æ‰€æœ‰nodeçš„è¾“å‡ºï¼‰ã€‚æœ€ååªåœ¨ICLæ•°æ®é›†ä¸Šå¾®è°ƒGNNï¼Œèƒ½å¤Ÿå®ç°å’Œloraåª²ç¾çš„é€Ÿåº¦å’Œæ›´å¥½çš„accã€‚

19. **Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning** (Arxiv April 2024) [[paper]](http://arxiv.org/abs/2404.07546) å°†ICLèƒ½åŠ›åˆ†æˆ1)æ­£åˆ™åŒ–è¾“å‡ºçš„label spaceã€2)æ­£åˆ™åŒ–è¾“å‡ºçš„label formatï¼Œå’Œ3)æå‡label space/formatåˆ†å¸ƒå†…çš„åˆ¤åˆ«èƒ½åŠ›ä¸‰ä¸ªæ–¹é¢ã€‚ç»“è®ºï¼šICLçš„èƒ½åŠ›ä¸»è¦æ¥è‡ªå‰ä¸¤è€…ã€‚åŒæ—¶ä¹Ÿåœ¨å®éªŒä¸Šé—´æ¥è¯æ˜äº†ICLä¼šå€¾å‘äºé¢„æµ‹å‡ºcontextå’Œtestæ›´åƒçš„æ ·æœ¬çš„labelã€‚

20. **The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.11004) åœ¨é¢„æµ‹Markovåºåˆ—ä»»åŠ¡ä¸Šï¼Œæ­ç¤ºäº†å­˜åœ¨ä¸€ä¸ªå­¦ä¹ å‡ºä»ç®€å•åˆ°å¤æ‚functionçš„è¿‡ç¨‹ï¼ˆuniform -> unigram -> bigrams (optimal)ï¼‰ã€‚æ­¤å¤–ï¼Œä¹ŸéªŒè¯äº†ç±»ä¼¼retrievalï¼ˆn-gramï¼‰ï¼Œå³æ‰¾æœ€ç›¸ä¼¼çš„context tokenç„¶åå–å®ƒåé¢çš„tokenä½œä¸ºé¢„æµ‹çš„æœºåˆ¶ 

21. **In-Context Language Learning: Architectures and Algorithms** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2401.12973) æ„é€ äº†ä¸€ä¸ªæ¨¡æ‹Ÿçš„language token ICLä»»åŠ¡ï¼Œç»™äº†ä¸€ç³»åˆ—å®éªŒè¯æ®è¯´æ˜transformerå®ç°äº†å’Œn-gramç±»ä¼¼çš„retrievalè¿‡ç¨‹

22. **Trusting Your Evidence: Hallucinate Less with Context-aware Decoding** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2305.14739) ä¸ºäº†å¢å¼ºå¯¹contextçš„å…³æ³¨èƒ½åŠ›ï¼Œæå‡ºåœ¨æ¨ç†æ—¶åŠ æƒä»¥contextä¸ºæ¡ä»¶çš„é¢„æµ‹å’Œä¸å«contextçš„é¢„æµ‹ï¼š$y=\text{softmax}((1+\alpha) p_\theta(y|c,x)-\alpha p_\theta(y|x))$â€‹ ã€‚èƒŒåçš„ç†è®ºåŸºç¡€æ˜¯æœ´ç´ è´å¶æ–¯ [[blog]](https://spaces.ac.cn/archives/9617)

23. **How In-Context Learning Emerges from Training on Unstructured Data: On the Role of Co-Occurrence, Positional Information, and Noise Structures** (Arxiv Jun 2024) [[paper]](http://arxiv.org/abs/2406.00131) åœ¨éICLæ ¼å¼çš„æ•°æ®ä¸Šè®­ç»ƒï¼Œæ¢ç©¶äº†â€œå›½å®¶-é¦–éƒ½â€ç±»ä»»åŠ¡ï¼ˆé¢„è®­ç»ƒå¸¸è§ï¼‰å’Œè¾“å‡ºé¦–å­—æ¯ä»»åŠ¡ï¼ˆä¸å¸¸è§ï¼‰ï¼Œå‘ç°patternåœ¨è®­ç»ƒæ•°æ®é‡Œçš„é‡å¤æ€§å’Œä½ç½®ä¿¡æ¯åˆ†åˆ«æ˜¯è¿™ä¸¤ç§ä»»åŠ¡çš„å…³é”®ã€‚

24. **Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.00743) åˆ†æå¤šå±‚ã€PEã€multi headç­‰æ¨¡å—å¯¹äºæå‡ICLåœ¨çº¿æ€§å›å½’ä»»åŠ¡ä¸Šæ€§èƒ½çš„ä½œç”¨ã€‚

25. **Do pretrained Transformers Learn In-Context by Gradient Descent?** (ICML 2024) [[paper]](http://arxiv.org/abs/2310.08540) è®¨è®ºäº†ä¸€ä¸‹ç›®å‰ICLå·¥ä½œçš„ä¸åˆ‡å®é™…çš„settingï¼Œä»ä¸€äº›å®éªŒæŒ‡æ ‡ä¸Šè¯´æ˜äº†ICLå’ŒGDæœ‰æ˜¾è‘—ä¸åŒã€‚

26. **Rectifying Demonstration Shortcut in In-Context Learning** (NAACL 2024) [[paper]](http://arxiv.org/abs/2403.09488) å‘ç°contextå•è¯çš„å­—é¢æ„æ€ä¼šå½±å“ICLåˆ†ç±»çš„ç»“æœï¼ˆä¸€ç§shortcutï¼‰ã€‚æå‡ºäº†ä¸€ç§calibrationçš„ç­–ç•¥ã€‚

27. **Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning** (Arxiv June 2024) [[paper]](http://arxiv.org/abs/2406.14022) è®­ç»ƒè¿‡ç¨‹ä¸­task learningå’Œtask recognitionå­˜åœ¨ç«äº‰ç°è±¡

28. **Transformers Can Perform Distributionally-robust Optimisation through In-context Learning** (ICML 2024 workshop on ICL) [[paper]](https://openreview.net/pdf?id=MOgg2cEms5) ICLæœ‰ä¸€å®šçš„DROçš„èƒ½åŠ› 

29. **How Do In-Context Examples Affect Compositional Generalization?** (ACL 2024) [[paper]](http://arxiv.org/abs/2305.04835) å‘ç°context exampleå¯¹äºç»„åˆæ³›åŒ–èƒ½åŠ›å½±å“æ˜¾è‘—ã€‚å…·ä½“æ¥è¯´ï¼Œcontext exampleå’Œqueryè¶Šåƒã€exampleè¶Šå¤šæ ·ã€æ¯ä¸ªæ ·æœ¬è¶Šç®€å•ï¼Œæ³›åŒ–èƒ½åŠ›è¶Šå¥½ã€‚

30. **What Do Language Models Learn in Context? The Structured Task Hypothesis** (ACL 2024) [[paper]](http://arxiv.org/abs/2406.04216) é€šè¿‡å®éªŒéªŒè¯äº†ICLèƒ½å¤Ÿå¯¹é¢„è®­ç»ƒè§è¿‡çš„ä»»åŠ¡è¿›è¡Œå¤åˆçš„å‡è®¾ï¼Œå¦å®šäº†ICLä»…ä»…èƒ½å¤Ÿè¿›è¡Œåˆ†å¸ƒå†…ä»»åŠ¡çš„è¯•åˆ«ä»¥åŠICLèƒ½å¤Ÿæ³›åŒ–åˆ°æŸäº›è®­ç»ƒæ—¶æ²¡è§è¿‡çš„ä»»åŠ¡çš„å‡è®¾ã€‚

31. **What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation** (ICML 2024) [[paper]](http://arxiv.org/abs/2404.07129) è¯†åˆ«äº†transformeråœ¨è§£å†³ICLçš„copy-and-pasteä»»åŠ¡ä¸­å­˜åœ¨çš„ä¸‰ç§circuit

32. **In-Context Learning of Energy Functions** (ICML 2024 ICL workshop) [[paper]](http://arxiv.org/abs/2406.12785) æå‡ºäº†å°†next-tokençš„æ¡ä»¶åˆ†å¸ƒå»ºæ¨¡ä¸ºèƒ½é‡å‡½æ•°çš„å½¢å¼ï¼Œå‘ç°transformerä¹Ÿèƒ½åœ¨è¿™ç§å½¢å¼ä¸‹å±•ç°å‡ºICLèƒ½åŠ›

    



### 2023

1. **Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?** [[paper]](https://arxiv.org/abs/2202.12837) åšäº†ä¸€ç³»åˆ—æ¶ˆèå®éªŒæ¥å¯¹ICLè¿›è¡Œè§£é‡Šã€‚ä¸»è¦ç»“è®ºï¼šå³ä½¿inputå’Œlabelä¸æ˜¯ä¸€ä¸€å¯¹åº”ï¼Œåªè¦labelçš„åˆ†å¸ƒåˆç†ï¼Œé‚£ä¹ˆICLåŒæ ·èƒ½ç»™å‡ºè¾ƒä¸ºæ­£ç¡®çš„ç­”æ¡ˆ.
2. **Symbol tuning improves in-context learning in language models** (EMNLP 2023) [[paper]](http://arxiv.org/abs/2305.08298) å°†demonstrationçš„labelæ¢ä¸ºæ— æ„ä¹‰çš„symbolï¼Œç„¶åå¾®è°ƒï¼Œä»¥æ­¤å¼ºè¿«æ¨¡å‹å­¦ä¹ input-label mappingã€‚
3. **In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax** (Arxiv Nov 2023) [[paper] ](In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax) æœ¬æ–‡é€šè¿‡æ„å»ºä¸€äº›è¯­æ³•ä»»åŠ¡æ¥æµ‹è¯•æ¨¡å‹å¯¹äºå¥å­ç»“æ„çš„ç†è§£èƒ½åŠ›ï¼Œä»¥åŠOODæ³›åŒ–æ€§èƒ½ã€‚æ€»çš„è¯´æ¥ï¼ŒLLMè¿˜æ˜¯ä¼šç”¨åˆ°ä¸€äº›spurious correlationã€‚
4. **A Closer Look at In-Context Learning under Distribution Shifts** (Arxiv May 2023) [[paper]](http://arxiv.org/abs/2305.16704) åœ¨ä¸€å®šçš„åˆ†å¸ƒåç§»ä¸‹ï¼Œtransformeræ¯”set-based MLPçš„æ€§èƒ½å¥½ï¼›åœ¨ä¸¥é‡çš„åˆ†å¸ƒåç§»ä¸‹ï¼Œä¸¤ç§æ¨¡å‹çš„ICLèƒ½åŠ›éƒ½ä¸§å¤±äº†ã€‚
5. **Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation** (Arxiv May 2023) [[paper]](https://www.lsv.uni-saarland.de/wp-content/uploads/2023/07/Few-shot-Fine-tuning-vs.-In-context-Learning.pdf) åœ¨å‚æ•°é‡ç›¸å½“çš„æƒ…å†µä¸‹ï¼ŒICLçš„OODä¸å¦‚FTã€‚30Bçš„ICLè·Ÿ6.7Bçš„FTæ€§èƒ½ç›¸å½“ã€‚å¤§éƒ¨åˆ†æƒ…å†µä¸‹ICLä¸å¦‚FTã€‚
6. **Instruction-following Evaluation through Verbalizer Manipulation** (Arxiv July 2023) [[paper]](http://arxiv.org/abs/2307.10558) å‘ç°LLMéµå¾ªflipped-label instructionsçš„èƒ½åŠ›å¾ˆå·®ï¼Œè¯´æ˜ICLå¯èƒ½åªæ˜¯ç›´æ¥åˆ©ç”¨äº†é¢„è®­ç»ƒè¯­æ–™çš„çŸ¥è¯†ï¼Œè€Œä¸æ˜¯å­¦ä¹ äº†contextã€‚å³ä½¿æ˜¯å¼ºå¦‚GPT-4çš„æ¨¡å‹ä¹Ÿä¸èƒ½å¾ˆå¥½åœ°éµå¾ªflipped-label instructionsã€‚
7. **Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks** (Arxiv Aug 2023) [[paper]](http://arxiv.org/abs/2307.02477) ä¸€äº›ä¸»è¦å‘ç°ï¼šâ‘ æ¨¡å‹åœ¨counterfactualçš„settingä¸­æ€§èƒ½ä¼šå˜å·®ï¼Œä¸”settingå’Œå¸¸è§çš„ã€ç¬¦åˆäº‹å®çš„settingç›¸å·®è¶Šè¿œï¼Œæ€§èƒ½è¶Šå·®ï¼Œè¯´æ˜äº†æ¨¡å‹å¯èƒ½çš„è®°å¿†ç°è±¡ã€‚â‘¡åœ¨ç®—æœ¯ä»»åŠ¡ä¸Šï¼ŒICLèƒ½æå‡counterfactualï¼ˆä¸åŒè¿›åˆ¶çš„è®¡ç®—ï¼‰æ€§èƒ½ï¼Œä½†å’Œdefault settingçš„å·®è·éš¾ä»¥æŠ¹å¹³ã€‚
8. **The Learnability of In-Context Learning** (NeurIPS 2023) [[paper]](https://openreview.net/forum?id=f3JNQd7CHM) è¯æ˜äº†å½“é¢„è®­ç»ƒåˆ†å¸ƒåŒ…å«ä¸‹æ¸¸ä»»åŠ¡çš„åˆ†å¸ƒçš„mixutureï¼ŒICLèƒ½é€¼è¿‘ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è´å¶æ–¯æœ€ä¼˜åˆ†ç±»å™¨ã€‚
9. **What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning** (Findings of ACL 2023) [[paper]](http://arxiv.org/abs/2305.09731) åˆ†åˆ«ç”¨éšæœºlabelï¼ˆx-yæ˜ å°„å…³ç³»è¢«ç ´åï¼‰å’Œéè‡ªç„¶è¯­è¨€labelï¼ˆx-yæ˜ å°„å…³ç³»ä¿ç•™ï¼‰æ¥æ£€éªŒæ¨¡å‹çš„ä»é¢„è®­ç»ƒçŸ¥è¯†ä¸­è¯†åˆ«ä»»åŠ¡å’Œä»contextä¸­å­¦ä¹ input-labelæ˜ å°„å…³ç³»çš„èƒ½åŠ›ï¼Œå‘ç°ï¼šè¿™ä¸¤ç§èƒ½åŠ›åŒæ—¶å­˜åœ¨ï¼›ä»»åŠ¡è¯†åˆ«èƒ½åŠ›åŸºæœ¬ä¸éšæ¨¡å‹è§„æ¨¡å˜åŒ–ï¼›in-contextå­¦ä¹ èƒ½åŠ›ä¼šéšæ¨¡å‹å˜å¤§è€Œä¸Šå‡ã€‚
10. **Larger language models do in-context learning differently** (Arxiv Mar 2023) [[paper]](http://arxiv.org/abs/2303.03846) å’Œdisentanglement TR and TL é‚£ç¯‡å·®ä¸å¤šï¼Œå‘ç°äº†ï¼šå°æ¨¡å‹ä¼šå€¾å‘äºç”¨priorï¼Œéšç€æ¨¡å‹å¢å¤§ï¼Œè¦†ç›–priorè€Œä»contextå­¦ä¹ æ˜ å°„å…³ç³»çš„èƒ½åŠ›ä¼šè¶Šæ¥è¶Šå¼ºã€‚
11. **In-Context Learning Creates Task Vectors** (Arxiv Oct 2023) [[paper]](http://arxiv.org/abs/2310.15916) åŒæ ·å‘ç°contextçš„æœ€åä¸€ä¸ªtokençš„è¡¨ç¤ºencodeäº†è¯¥ä»»åŠ¡çš„ä¿¡æ¯ã€‚é€šè¿‡å®éªŒå‘ç°ICLè¿‘ä¼¼æ˜¯åœ¨å®ç°å¦‚ä¸‹è¿‡ç¨‹ï¼š1)ä»contextå­¦å‡ºä¸€ä¸ªæ˜ å°„å‡½æ•° 2)å°†è¿™ä¸ªæ˜ å°„å‡½æ•°ç”¨åˆ°queryä¸Šæ¥é¢„æµ‹ã€‚ä¸€ä¸ªé‡è¦è§‚å¯Ÿæ˜¯ï¼šè¯´æ˜æ¨¡å‹æ›´å€¾å‘äºä½¿ç”¨vectoré‡Œçš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åŸå§‹context
12. **Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning** (EMNLP 2023) [[paper]](http://arxiv.org/abs/2305.14160) æµ…å±‚ç½‘ç»œä»textåˆ°labelèšåˆä¿¡æ¯ï¼Œæ·±å±‚ç½‘ç»œä»labelåˆ°last tokenèšåˆä¿¡æ¯ã€‚
13. **Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models** (Arxiv Nov 2023) [[paper]](http://arxiv.org/abs/2311.00871) å‘ç°ICLåœ¨æµ‹è¯•å’Œé¢„è®­ç»ƒä»»åŠ¡ä¸åŒæ—¶æ€§èƒ½ä¸å¥½ã€‚
14. **Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression** (NeurIPS 2023) [[paper]](https://arxiv.org/abs/2306.15063) å‘ç°é¢„è®­ç»ƒå­¦ä¹ çš„ä»»åŠ¡è¶Šå¤šï¼ŒICLåœ¨æ–°ä»»åŠ¡ä¸Šçš„æ³›åŒ–è¶Šå¼ºï¼ˆä¸åŒä»»åŠ¡ï¼šä¸åŒçº¿æ€§å›å½’çš„Wï¼‰



### 2022

1. **What Can Transformers Learn In-Context? A Case Study of Simple Function Classes** (NeurIPS 2022) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/c529dba08a146ea8d6cf715ae8930cbe-Abstract-Conference.html) å®éªŒå‘ç°ï¼š1)linear functionæ˜¯èƒ½é€šè¿‡transformerå­¦åˆ°çš„ï¼ˆæ€§èƒ½èƒ½é€¼è¿‘æœ€å°äºŒä¹˜ä¼°è®¡ï¼‰2)ICLæœ‰ä¸€å®šçš„OODæ³›åŒ–èƒ½åŠ›ï¼ˆtrain -> test, context -> testï¼‰3)ICLä¹Ÿèƒ½å­¦åˆ°æ›´å¤æ‚çš„å‡½æ•°ï¼Œæ¯”å¦‚sparse linear functionsã€ReLU NNsã€decision treesã€‚
2. **Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?** (EMNLP 2022) [[paper]](http://arxiv.org/abs/2202.12837) æ¢ç©¶ICL workçš„å› ç´ ã€‚
3. **On the Compositional Generalization Gap of In-Context Learning** (Arxiv 2022) [[paper]](http://arxiv.org/abs/2211.08473) åœ¨CFQç­‰ç»„åˆæ³›åŒ–ä»»åŠ¡ä¸Šæµ‹ï¼Œå‘ç°å¤§æ¨¡å‹çš„OODï¼ˆqueryå’Œcontextä¸ä¸€è‡´ï¼‰å’ŒIDä¹‹é—´çš„ç»„åˆæ³›åŒ–èƒ½åŠ›çš„gapç›¸æ¯”å°æ¨¡å‹æ›´å°ã€‚



## ICL Theories

### 2024

1. **How do Transformers perform In-Context Autoregressive Learning?** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.05787) åœ¨é™å®šlinear attentionã€diagonal weight matrixç­‰æ¡ä»¶ä¸‹ï¼Œå¯¹äºåºåˆ—é¢„æµ‹ä»»åŠ¡$s_{T+1}=Ws_T$ï¼ˆæ–‡ç« è€ƒè™‘çš„$W$æ˜¯é…‰çŸ©é˜µå’Œæ­£äº¤çŸ©é˜µä¸¤ç§æƒ…å†µï¼‰ï¼Œä»ç†è®ºä¸Šç»™å‡ºäº†å–åˆ°å…¨å±€æœ€ä¼˜è§£æ—¶ï¼Œtransformer å‚æ•°æ‰€åº”æ»¡è¶³çš„æ€§è´¨ã€‚

2. **On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2405.16845) ç†è®ºè¯æ˜äº†ï¼Œä¸åŒäºç›´æ¥åœ¨ICLç›®æ ‡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç»è¿‡è‡ªå›å½’é¢„è®­ç»ƒçš„one-layer linear attentionä¸èƒ½åœ¨ç®€å•å¦‚æœä»é«˜æ–¯åˆ†å¸ƒçš„åºåˆ—ä¸Šå®ç°ICLã€‚

3. **How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?** (ICML 2024) [[paper]](http://arxiv.org/abs/2402.15607) åœ¨è¿›è¡ŒICLé¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œç»™å‡ºäº†éçº¿æ€§attentionçš„IDå’ŒOODçš„æ³›åŒ–ä¿è¯

4. **Why Larger Language Models Do In-context Learning Differently?** (ICML 2024) [[paper]](http://arxiv.org/abs/2405.19592) æœ¬æ–‡å¯¹äºæ›´å¤§çš„æ¨¡å‹æ›´å®¹æ˜“åœ¨flipped labelä»»åŠ¡ä¸Šå¤±è´¥ç»™äº†ç†è®ºè§£é‡Šï¼šå¤§æ¨¡å‹æ›´å®¹æ˜“å—åˆ°promptä¸­noiseçš„å½±å“ï¼Œè€Œå°æ¨¡å‹åªä¼šå…³æ³¨æ›´é‡è¦çš„featureæ‰€ä»¥ä¸å®¹æ˜“å—åˆ°noiseå½±å“ï¼Œè¿›è€Œä½¿pretrain featureå‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚

5. **Dual Operating Modes of In-Context Learning** (ICML 2024) [[paper]](http://arxiv.org/abs/2402.18819) ç†è®ºsettingï¼šåœ¨æ··åˆé«˜æ–¯çš„çº¿æ€§å›å½’ä¸Šé¢„è®­ç»ƒï¼Œåˆ†æäº†ç»™å®štest contextæ—¶çš„åéªŒæ¦‚ç‡ï¼Œè§£é‡Šäº†task recognitionå’Œtask learningï¼šå‘ç°contextè¾ƒçŸ­æ—¶ä»¥task recognitionï¼ˆè°ƒæ•´åéªŒçš„æ··åˆé«˜æ–¯çš„å„åˆ†é‡çš„æƒé‡ï¼‰ä¸ºä¸»ã€‚contextå˜é•¿ä¹‹åä»¥task learningä¸ºä¸»ã€‚

6. **In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2402.11639) softmaxèƒ½adaptivelyå­¦ä¸€ä¸ªattention windowæ¥å®ç°å°†context $y_i$ è¿›è¡Œæ’å€¼ä½œä¸ºé¢„æµ‹ï¼Œå°†åˆ†ç±»ä»»åŠ¡ä¸­è§åˆ°çš„retrievalæœºåˆ¶æ‹“å±•åˆ°äº†å›å½’ä»»åŠ¡ä¸Šã€‚

### 2023

1. **What learning algorithm is in-context learning? Investigations with linear models** (ICLR 2023) [[paper]](http://arxiv.org/abs/2211.15661) è¿˜æ²¡çœ‹ï¼Œç†è®ºç†è§£ICLæœºåˆ¶çš„æ–‡ç« ï¼Œlinear regressionä»»åŠ¡ï¼Œä½†å®ƒçš„ç†è®ºè®¾å®šæ˜¯æ¨¡å‹è¦åœ¨ICLä»»åŠ¡ä¸Šé¢„è®­ç»ƒï¼Œä¸å®é™…çš„Auto Regressiveé¢„è®­ç»ƒæœ‰è¾ƒå¤§gapã€‚å®ƒçš„è¯æ˜æ€è·¯ä¹Ÿæ˜¯é€šè¿‡ç½‘ç»œå‚æ•°æ„é€ è§£ï¼Œå’ŒA Theoretical Understanding of Self-Correction through In-context Alignmentè¿™ç¯‡ç±»ä¼¼ã€‚
2. **Transformers as Algorithms: Generalization and Stability in In-context Learning** (ICML 2023) [[paper]](http://arxiv.org/abs/2301.07067) è€ƒè™‘äº†contextä¸ºä¸€ç³»åˆ—ç‹¬ç«‹pairå’Œå‰åæ ·æœ¬æœ‰å…³è”ä¸¤ç§æ¨¡å¼ï¼Œåœ¨è¿›è¡ŒICLé¢„è®­ç»ƒçš„æ¡ä»¶ä¸‹ï¼Œç»™äº†ä¸€ä¸ªnon-linear transformerçš„excess riskçš„upper bound
3. **In-Context Convergence of Transformers** (Arxiv Oct 2023) [[paper]](http://arxiv.org/abs/2310.05249) linear regressionä»»åŠ¡ï¼Œéœ€è¦é¢„è®­ç»ƒï¼Œä¸€å±‚éçº¿æ€§attentionï¼Œä½†æ˜¯åšäº†å…¶ä»–ç®€åŒ–ä½¿å¾—transforerå°±æ˜¯åœ¨æ ¹æ®xä¹‹é—´çš„attention weightæ¥åŠ æƒç»„åˆå„ä¸ªcontext yä½œä¸ºæœ€ç»ˆé¢„æµ‹ã€‚
4. **Trained Transformers Learn Linear Models In-Context** (Arxiv Oct 2023) [[paper]](http://arxiv.org/abs/2306.09927) linear regressionä»»åŠ¡ï¼Œéœ€è¦é¢„è®­ç»ƒï¼Œä¸€å±‚çº¿æ€§attentionã€‚è¯æ˜äº†é¢„è®­ç»ƒlossæ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£æ—¶ï¼Œå½“è®­ç»ƒå’Œæµ‹è¯•contextè¶³å¤Ÿé•¿æ—¶ï¼Œèƒ½å­¦åˆ°æµ‹è¯•promptä¸Šçš„æ­£ç¡®è§£Wã€‚
5. **What and How Does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization** (Arxiv Oct 2023) [[paper]](arXiv:2305.19420) ç†è®ºæ–‡ç« ï¼Œè¿˜æ²¡çœ‹



## Others

### Network Architectures/Neural Network Properties

### 2023

1. **Analyzing Transformers in Embedding Space** [[paper]](http://arxiv.org/abs/2209.02535) Section 2.1 çš„ transformer architectureè®²å¾—æ¯”attention is all you needå¥½
1. **The Low-Rank Simplicity Bias in Deep Networks** [[paper]](http://arxiv.org/abs/2103.10427) ç¥ç»ç½‘ç»œçš„è¿‡å‚æ•°åŒ–ï¼ˆæ›´æ·±çš„ç½‘ç»œå±‚æ•°ï¼Œæ¯”å¦‚$W$é‡å‚æ•°åŒ–ä¸º$W=W_1W_2...W_L$ï¼‰èƒ½å¤Ÿä½¿å¾—å…¶å­¦åˆ°effective rankæ›´ä½çš„è¡¨ç¤ºï¼Œä»è€Œæœ‰åˆ©äºç°å®ä¸–ç•Œä»»åŠ¡ï¼ˆæ¯”å¦‚åˆ†ç±»ï¼Œå¤§éƒ¨åˆ†æ˜¯æƒ³å­¦ä¸€ä¸ªä»xåˆ°yçš„ä½ç§©æ˜ å°„ï¼‰ä¸Šçš„æ³›åŒ–ã€‚

#### 2021

1. **Transformer Feed-Forward Layers Are Key-Value Memories** (EMNLP 2021) [[paper]](http://arxiv.org/abs/2012.14913) transformerçš„FFNç¬¬ä¸€å±‚æ˜¯keyï¼Œç¬¬äºŒå±‚æ˜¯value
2. 

### Optimization

1. **On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima** (ICLR 2017) large-batchä¼šå¯¼è‡´æ¨¡å‹æ›´å®¹æ˜“è¿›å…¥å°–é”çš„æå°å€¼ç‚¹ï¼Œè€Œå°–é”çš„æå°å€¼ç‚¹ä¼šä¸åˆ©äºæ³›åŒ– [[paper]](https://arxiv.org/abs/1609.04836)

### Adversarial Robustness

#### 2022

1. **The Dimpled Manifold Model of Adversarial Examples in Machine Learning** (arxiv 2022) ä»manifoldè§’åº¦ç ”ç©¶ç½‘ç»œè®­ç»ƒçš„è¿‡ç¨‹ä»¥åŠATçš„è¡Œä¸ºã€‚å‘ç°è®­ç»ƒDNNå¤§æ¦‚åˆ†ä¸ºä¸¤ä¸ªè¿‡ç¨‹ï¼šâ‘ ä½¿å†³ç­–è¾¹ç•Œç”±éšæœºå¿«é€Ÿåˆ†å¸ƒåˆ°æ•°æ®çš„manifoldé™„è¿‘ â‘¡é€šè¿‡åœ¨å†³ç­–è¾¹ç•Œä¸­äº§ç”Ÿå‡¹å‡¸ï¼Œæ¥ä½¿å†³ç­–è¾¹ç•Œç§»åŠ¨åˆ°æ ·æœ¬çš„æ­£ç¡®ä¸€ä¾§ã€‚ATä¼šä½¿å¾—å†³ç­–è¾¹ç•Œçš„èµ·ä¼æ›´å¤§ï¼Œå³å‘off-manifoldæ–¹å‘ç§»åŠ¨ã€‚[[paper]](https://arxiv.org/abs/2106.10151) [[notes]](./all_notes/The-Dimpled-Manifold.pdf)
2. **Understanding Adversarial Robustness Against On-manifold Adversarial Examples** (arxiv 2022) manifoldè§’åº¦ç ”ç©¶ATè¡Œä¸ºï¼Œå‘ç°äº†on-manifoldå¯¹æŠ—æ ·æœ¬çš„å­˜åœ¨ï¼Œæå‡ºäº†åœ¨GANçš„éšç©ºé—´åšATã€åœ¨è®­ç»ƒæ•°æ®çš„ç‰¹å¾å‘é‡å¼ æˆçš„å­ç©ºé—´ä¸­åšATä¸¤ç§æ€è·¯ [[paper]] (https://arxiv.org/abs/2210.00430) [[notes]](./all_notes/Understanding-Adversarial-Robustness-Against.pdf)

### Neural Collapse

#### 2023

1. **Neural Collapse: A Review on Modelling Principles and Generalization** (TMLR 2023)  [[paper]](http://arxiv.org/abs/2206.04041)
1. **How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability** (ICCV 2023) [[paper]]() è®¾è®¡äº†ä¸€ä¸ªempiricalçš„æŒ‡æ ‡æ¥æµ‹é‡ç›®æ ‡åŸŸæ¨¡å‹ç¦»NCæœ‰å¤šè¿œï¼Œæ¥å®ç°ä»¥è¾ƒä½å¼€é”€ä¼°è®¡æ¨¡å‹åœ¨ä¸‹æ¸¸ç›®æ ‡åŸŸçš„æ€§èƒ½ï¼ˆä¾èµ–çš„ideaæ˜¯åœ¨ç›®æ ‡åŸŸä¸Šè¶ŠNCï¼Œæ€§èƒ½è¶Šå¥½ï¼‰ã€‚ä¸€ä¸ªé‡è¦çš„å‘ç°æ˜¯å¯¹äºæœ‰ç›‘ç£/æ— ç›‘ç£é¢„è®­ç»ƒï¼Œåœ¨æºåŸŸä¸Šwithin-class variabilityè¶Šå¤§ï¼Œä¸‹æ¸¸è¿ç§»èƒ½åŠ›è¶Šå¥½ã€‚

#### 2022

1. **Limitations of Neural Collapse for Understanding Generalization in Deep Learning** (Arxiv 2022) [[paper]](http://arxiv.org/abs/2202.08384) å‘ç°ï¼šâ‘ è®­ç»ƒæ—¶å’Œæµ‹è¯•æ—¶çš„NCå¹¶ä¸ä¸€å®šä¼šåŒæ—¶å‘ç”Ÿï¼Œç”šè‡³æœ‰å¯èƒ½è¶‹åŠ¿ç›¸åã€‚å› æ­¤å»ºè®®å¯¹å®ƒä»¬åˆ†åˆ«åˆ»ç”»ã€‚â‘¡æµ‹è¯•æ—¶å‘ç”ŸNCä¼šå¯¼è‡´è¿ç§»æ€§æ›´å·®çš„ç‰¹å¾ã€‚ï¼ˆæ³¨ï¼šè¿™é‡Œçš„â€æµ‹è¯•æ—¶â€œNCå…¶å®å¹¶ä¸å‡†ç¡®ï¼Œå› ä¸ºä»–åé¢æµ‹çš„å®é™…ä¸Šæ˜¯ä¸æµ‹è¯•æ—¶ä¸åŒçš„ä¸€ä¸ªä»»åŠ¡ï¼šCIFAR10ï¼Œåœ¨â€æµ‹è¯•æ—¶â€œï¼ˆä¹Ÿå°±æ˜¯é¢„è®­ç»ƒï¼‰äºŒåˆ†ç±»ï¼ˆä¸¤ä¸ªç²—ç±»ï¼‰ï¼Œä¸‹æ¸¸ä»»åŠ¡10åˆ†ç±»ã€‚ï¼‰


### Feature Collapse

1. **Feature Collapse** (Arxiv 2023)

### Causality

1. **Causal Confusion in Imitation Learning** (NeurIPS 2019) è§£å†³imitation learningä¸­çš„distribution shifté—®é¢˜ï¼ˆè§£å†³åœºæ™¯æ¯”è¾ƒåƒOOD generalizationä¸­çš„correlation shiftï¼‰ã€‚imitation learningï¼šç»™å‡ºobservation $X^t$å’Œexpert actions $A^t$ï¼Œå¸Œæœ›å­¦å‡º$\pi:X^t\rightarrow A^t$ã€‚æœ¬æ–‡çš„ä¸€ä¸ªæœ‰è¶£çš„å‘ç°æ˜¯ï¼šè§‚æµ‹åˆ°å¤ªå¤šçš„ï¼ˆå†å²ï¼‰ä¿¡æ¯ä¼šå½±å“æ³›åŒ–æ€§èƒ½ã€‚[[paper]](https://proceedings.neurips.cc/paper_files/paper/2019/hash/947018640bf36a2bb609d3557a285329-Abstract.html) [[notes&thinking]](./all_notes/Causality.pdf)
2. **Deep Latent-Variable Models** (NeurIPS 2017) æå‡ºäº†ä¸€ç§VAEæ¥å®ç°å­˜åœ¨unobserved confounderæ—¶çš„interventional probabilityçš„è®¡ç®— [[paper]](https://proceedings.neurips.cc/paper/2017/hash/94b5bde6de888ddf9cde6748ad2523d1-Abstract.html) [[notes&thinking]](./all_notes/Causality.pdf)
3. **Fairness in Decision-Making The Causal Explanation Formula** (AAAI 2018) æå‡ºäº†ä¸€ç§éå‚æ•°ç”»çš„counterfactual probabilityçš„è®¡ç®—æ–¹æ³•ï¼Œæ‰€æå‡ºçš„ä¸‰ç§æŒ‡æ ‡DE IE SEåˆ†åˆ«detectä¸‰ç§discrimination pathï¼ˆä»interventionåˆ°outcomeçš„pathï¼‰æ˜¯å¦å­˜åœ¨ï¼Œå¹¶è¯æ˜äº†total variationèƒ½å¤Ÿè¢«è¿™ä¸‰ä¸ªé‡è¡¨ç¤º [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/11564) [[notes&thinking]](./all_notes/Causality.pdf)

### Graph Homophily/Heterophily

#### 2023

1. **Characterizing Graph Datasets for Node Classification: Homophilyâ€“Heterophily Dichotomy and Beyond** 
   

### Meta-learning

#### 2024

1. **Meta-learning Approaches for Few-Shot Learning: A Survey of Recent Advances** (ACM Computing Surveys 2024) ç»¼è¿°
2. **Advances and Challenges in Meta-Learning: A Technical Review** (TPAMI 2024) ç»¼è¿°

#### 2021

1. **Generalization Bounds For Meta-Learning: An Information-Theoretic Analysis** (NeurIPS 2021) 

#### 2019

1. **Provable Guarantees for Gradient-Based Meta-Learning**

## Slides

ä¸»è¦æ˜¯ä»ç»„å†…çš„è®ºæ–‡åˆ†äº«ä¼šè®²è¿‡çš„é‡Œè¾¹æŒ‘é€‰å‡ºçš„æ¯”è¾ƒæˆä½“ç³»çš„æŠ¥å‘Šï¼Œæ¯ä¸ªæŠ¥å‘Šä¸€èˆ¬ä¼šåŒ…å«å¤šç¯‡åœ¨æŸä¸€ä¸ªå°é—®é¢˜ä¸Šçš„å·¥ä½œ

* 2024-06-12 Recent Advances on ICL Theories [[slides]](./slides/2024.6.11-ICL_Theories.pptx)

* 2024-05-21 Causality Perspective on Domain Generalization [[slides]](./slides/2024.05.21-Causality-OOD.pptx)

* 2024-04-23 Some Empirical Explorations of ICL's Working Mechanism [[slides]](./slides/2024.4.23-ICL_mechanism.pptx) 

* 2024-03-12 In-Context Learning Can Helps the Generalization of Vision Models [[slides]](./slides/2024.3.12-ICL-OOD.pptx)

* 2024-01-10 How Large Vision-Language Models Help OOD Generalization [[slides]](./slides/2024.01.10-OOD-VLM.pptx)

  
