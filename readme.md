## Preface

æœ¬ä»“åº“è®°å½•å…³äºOOD Generalization/Domain Generlization/Domain Adaptation/Causalityç­‰topicçš„æ–‡ç« ã€‚çœ‹è¿‡çš„æ–‡ç« ä¼šè‡³å°‘ç”¨ä¸€å¥è¯æ¦‚æ‹¬å†…å®¹ï¼Œæœ‰äº›è¿˜ä¼šæœ‰notesã€‚åªæœ‰æ ‡é¢˜çš„å°±æ˜¯è¿˜æ²¡çœ‹è¿‡çš„ï¼Œåªæ˜¯å…ˆå­˜æ¡£åˆ°è¿™é‡Œã€‚

###  ğŸ”¥ Updates

- 2024-02-21 æ¥ä¸‹æ¥å°†ä¼šä¸»è¦å…³æ³¨LLM/Multi-modal LLMsçš„generalizationã€‚

## Directory

* [OOD Generalization of (Multimodal) LLMs](ood-generalization-of-multimodal-LLMs)



## OOD Generalization of (Multimodal) LLMs 

### 2024

1. **On the Out-Of-Distribution Generalization of Multimodal Large Language Models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.06599) å¯¹MLLMè¿›è¡Œäº†ä¸€ç³»åˆ—OODæ€§èƒ½çš„éªŒè¯ï¼Œå‘ç°ï¼šâ‘ åœ¨ç»å…¸OODæ•°æ®é›†ä¸ŠMLLMèƒ½è¶…è¿‡ä»¥å‰çš„sota â‘¡åœ¨åŒ»å­¦ã€åˆ†å­æ•°æ®é›†ä¸Šæ•ˆæœä¸å¥½ â‘¢ICLèƒ½æå‡MLLM OODæ€§èƒ½ï¼Œå³ä½¿ICL exampleçš„åˆ†å¸ƒå’Œæµ‹è¯•åŸŸä¸åŒã€‚

### 2023

1. **In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax** (Arxiv Nov 2023) [[paper] ](In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax) æœ¬æ–‡é€šè¿‡æ„å»ºä¸€äº›è¯­æ³•ä»»åŠ¡æ¥æµ‹è¯•æ¨¡å‹å¯¹äºå¥å­ç»“æ„çš„ç†è§£èƒ½åŠ›ï¼Œä»¥åŠOODæ³›åŒ–æ€§èƒ½ã€‚æ€»çš„è¯´æ¥ï¼ŒLLMè¿˜æ˜¯ä¼šç”¨åˆ°ä¸€äº›spurious correlationã€‚
2. **On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective** (ICLR 2024 workshop highlighted) [[paper]](http://arxiv.org/abs/2302.12095)
3. **Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes** (ongoing work) [[paper]](Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes) å‘ç°æ¨¡å‹å¯¹äºdemonstrationçš„permutation invarianceæˆ–è®¸æ˜¯ICL OODçš„å…³é”®ã€‚æå‡ºä½¿ç”¨ç›¸åŒçš„positional encodingæ¥æå‡ICL OODæ€§èƒ½ã€‚
4. **A Closer Look at In-Context Learning under Distribution Shifts** (Arxiv May 2023) [[paper]](http://arxiv.org/abs/2305.16704) 



## OOD Generalization using Large Vision-language models

### 2023

1. **PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization** (ICCV 2023) å…ˆè®­ç»ƒKä¸ªç”¨äºç”Ÿæˆstyle word embeddingï¼ˆå¤šæ ·æ€§+ä¿æŒè¯­ä¹‰ï¼‰ï¼Œç„¶åæŠŠå®ƒä»¬å’ŒNä¸ªç±»åˆ«è¯embeddingç»“åˆï¼Œå–‚ç»™CLIPtextç”ŸæˆK*Nä¸ªstyle-contentçš„text featureã€‚ä¹‹åæ‹¿è¿™äº›featureè®­ç»ƒä¸€ä¸ªlinear classifierã€‚æ¨æ–­æ—¶æŠŠè¿™ä¸ªclassifieræ¥åˆ°CLIP image encoderä¸Šï¼ˆ**å› ä¸ºåœ¨joint image-textç©ºé—´ä¸­ï¼Œå­˜åœ¨cross-modal transferability phenomenon**ï¼‰ã€‚ [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Cho_PromptStyler_Prompt-driven_Style_Generation_for_Source-free_Domain_Generalization_ICCV_2023_paper.html)
3. **A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance** (ICCV 2023) å°†CLIPçš„language encoderè¾“å‡ºçš„embeddingä½œä¸º"generic text representation"ï¼Œç„¶åè®©studentï¼ˆvisual modelï¼‰çš„è¡¨ç¤ºå»å¯¹é½teacherï¼ˆCLIPï¼‰çš„text representationã€‚åŒæ—¶å¯¹é½studentå’Œteacherçš„é¢„æµ‹logitã€‚ã€insight 1ã€‘recent workå‘ç°åŸºäºç¯å¢ƒåˆ’åˆ†çš„æ–¹æ³•ä¸é‚£ä¹ˆworkï¼Œå› ä¸ºçœŸå®ä¸–ç•Œçš„ç¯å¢ƒåˆ’åˆ†ä¸æ˜ç¡® ã€insight 2ã€‘ä»ä¼˜åŒ–éš¾åº¦è§’åº¦è¯´æ˜anchor sampleåœ¨å¯¹é½æ—¶çš„ä½œç”¨ [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Huang_A_Sentence_Speaks_a_Thousand_Images_Domain_Generalization_through_Distilling_ICCV_2023_paper.html) [[slides]](/all_notes/2023.12.29-OOD-LM.pptx)
4. **Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models** (NIPS 2023) åœ¨ç”¨In domainæ•°æ®ç”Ÿæˆçš„Discrete adversarial eaxmpleä¸Šæ‹¿ä¸€ä¸ªCLIPåšè’¸é¦æå‡å°±èƒ½è¶…è¿‡æ™®é€šçš„Knowledge Distillationå’ŒDAT [[paper]](https://arxiv.org/pdf/2311.01441.pdf) [[slides]](/all_notes/2023.12.29-OOD-LM.pptx)
5. **Distilling from Vision-Language Models for Improved OOD Generalization in Vision Tasks** (Arxiv Oct 2023) å¯¹é½student modelç»è¿‡ä¸€ä¸ªprojectoråçš„è¡¨ç¤ºå’ŒCLIPçš„text/image encoderçš„è¾“å‡º [[paper]](https://arxiv.org/abs/2310.08255) [[slides]](/all_notes/2023.12.29-OOD-LM.pptx)
6. **Context-Aware Robust Fine-Tuning** (IJCV Dec 2023) å‘ç°å¾®è°ƒä¼šç ´åCLIPåŸæœ‰çš„å¯¹äºcontextï¼ˆè™šå‡ç‰¹å¾ï¼‰zero-shotçš„åˆ†ç±»èƒ½åŠ›ã€‚æå‡ºåœ¨å¾®è°ƒçš„æ—¶å€™å¯¹é½é¢„è®­ç»ƒçš„CLIPé¢„æµ‹çš„contextçš„æ¦‚ç‡åˆ†å¸ƒå’Œæ­£åœ¨è¢«å¾®è°ƒçš„æ¨¡å‹é¢„æµ‹çš„contextçš„æ¦‚ç‡åˆ†å¸ƒ [[paper]](https://link.springer.com/article/10.1007/s11263-023-01951-2) 
7. **ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models **(Arxiv Nov 2023) [[paper]](http://arxiv.org/abs/2311.16494) æœ¬æ–‡æå‡ºäº†neg promptï¼Œå°±æ˜¯åœ¨prompté‡ŒåŠ å…¥neg attributeçš„embeddingã€‚åœ¨prompt tuningæ—¶ï¼Œå¸Œæœ›åœ¨ä½¿ç”¨neg promptæ—¶ï¼Œé¢„æµ‹æˆä»»ä½•ç±»åˆ«çš„æ¦‚ç‡ç›¸åŒï¼Œä»¥æ­¤æ¥å¼ºè¡Œæ¶ˆé™¤è™šå‡å…³è”ï¼ˆæ–‡ä¸­losså¼(9)ï¼‰ã€‚ä½¿ç”¨neg promptæ—¶é¢„æµ‹ä¸ºæŸä¸€ç±»çš„æ¦‚ç‡å°±æ˜¯ç”¨neg embedding+ä»»ä½•classçš„embeddingå’Œå›¾åƒè¡¨ç¤ºç®—ç›¸ä¼¼åº¦ã€‚



## New tasks of OOD Generalization

1. **SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization** (NeurIPS 2023) å¤šæ¨¡æ€DGï¼Œå°†å„æ¨¡æ€çš„ç‰¹å¾åˆ†ä¸ºä¸åŒæ¨¡æ€shareçš„éƒ¨åˆ†ä»¥åŠå„ä¸ªæ¨¡æ€specificçš„éƒ¨åˆ†ï¼Œæ‹‰è¿‘åŒä¸€classçš„ä¸åŒæ¨¡æ€sharedéƒ¨åˆ†çš„ç‰¹å¾ï¼Œæ¨è¿œshareéƒ¨åˆ†çš„ç‰¹å¾å’Œspecificéƒ¨åˆ†çš„ç‰¹å¾ã€‚[[paper]](https://openreview.net/forum?id=RiSMijlsLT)



## OOD Generalization (classic)
### 2024
1. **Spurious Feature Diversification Improves Out-of-distribution Generalization** (ICLR 2024 under review) é€šè¿‡ensembleå­¦æ›´å¤šçš„spurious featureèƒ½â€œå†²æ·¡â€å®ƒä»¬å„è‡ªçš„å½±å“ [[paper]](https://openreview.net/forum?id=d6H4RBi7RH)
1. **Out-Of-Domain Unlabeled Data Improves Generalization** (ICLR 2024 spotlight) [[paper]](https://openreview.net/forum?id=Bo6GpQ3B9a)

### 2023

1. **FREE LUNCH FOR DOMAIN ADVERSARIAL TRAINING: ENVIRONMENT LABEL SMOOTHING** (ICLR2023) ä½¿ç”¨ç®€å•çš„domain label smoothingæ–¹æ³•æ¥è§£å†³DANNè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œç»™å‡ºäº†ä¸°å¯Œçš„ç†è®ºæ”¯æŒ [[paper]](https://arxiv.org/abs/2302.00194)[[è®ºæ–‡åˆ†äº«ä¼šslides]](./all_notes/2023.2.17domain_adversarial_training_with_ELS.pdf)

2. **Generalizability of Adversarial Robustness Under Distribution Shifts** (AAAI2023) ATæœ‰åŠ©äºDomain Adaptationï¼Œä½†æ˜¯åœ¨Domain Generalization settingä¸‹ä¸å¦‚æ­£å¸¸æ¨¡å‹ [[paper]](https://arxiv.org/abs/2209.15042)

3. **WHAT IS MISSING IN IRM TRAINING AND EVALUATION? CHALLENGES AND SOLUTIONS** (ICLR2023) æŒ‡å‡ºlarge-batchä¼šä½¿å¾—å„IRMæ–¹æ³•çš„ç»“æœä¸å‡†ï¼Œsmall-batchçš„æ•ˆæœè¶³ä»¥åŒ¹æ•ŒåŸºäºlarge-batchçš„ä¼˜åŒ–ç®—æ³•ã€‚æŒ‡å‡ºäº†CMNISTåªæµ‹-90%æ˜¯ä¸å®Œå–„çš„ã€‚åŸºäºIRM-GAMEæå‡ºäº†BLOC-IRMï¼Œå‘ç°æ˜¾ç¤ºåœ°å¼•å…¥per-environment stationaryæ­£åˆ™é¡¹èƒ½ç¼“è§£å„domainåˆ†ç±»å™¨ä¸€è‡´çš„è¿™ä¸ªçº¦æŸå¸¦æ¥çš„å„ç¯å¢ƒåˆ†ç±»å™¨æ¬¡ä¼˜çš„é—®é¢˜ã€‚[[paper]](https://openreview.net/forum?id=MjsDeTcDEy)

4. **Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization** (arxiv2023) ä½¿ç”¨contrastive learningæ¥æ‹‰è¿‘ç»Ÿä¸€domainæ•°æ®çš„è¡¨ç¤ºã€æ¨è¿œä¸åŒdomainæ•°æ®çš„è¡¨ç¤ºï¼ˆæ–‡ç« claimè‡ªå·±æ˜¯é¦–ä¸ªåœ¨contrastive learning for DGä¸­è€ƒè™‘ä¸åŒdomainçš„ï¼‰ï¼ŒåŒæ—¶æå‡ºæ‰€è°“çš„domain-specific featureä¹Ÿæœ‰åŠ©äºæ³›åŒ–ï¼ˆä½†æ˜¯æ–‡ç« çš„å®ç°ç®—æ³•è²Œä¼¼æ²¡æœ‰æ˜¾å¼çš„è§£è€¦invariant/variant featureçš„è¿‡ç¨‹ï¼Œä¹Ÿæ²¡æœ‰æ˜¾ç¤ºåœ°æ„å»ºdomain-specific featureï¼‰[[paper]](https://arxiv.org/abs/2302.02350)

5. **Domain Generalization Emerges from Dreaming** (arxiv2023) å…ˆé€šè¿‡ç±»ä¼¼AdaINçš„æ–¹æ³•ç”Ÿæˆé£æ ¼è¿ç§»çš„å›¾åƒ $x'$ï¼Œå†minimizeåŸå§‹å›¾åƒ $x$ å’Œ $x'$çš„predictionçš„å·®è·ã€‚æ€§èƒ½ç”šè‡³å¯ä»¥è¶…è¿‡SWADã€‚[[paper]](https://arxiv.org/pdf/2302.00980.pdf)

6. **BREAKING CORRELATION SHIFT VIA CONDITIONAL INVARIANT REGULARIZER** (ICLR2023) è¯æ˜äº†å¯¹äºcorrelation shiftï¼Œåœ¨ç»™å®šyæ—¶ï¼Œè‹¥ç½‘ç»œè¾“å‡º $f(X)$ ç‹¬ç«‹äºspurious feature $Z$ ï¼Œåˆ™å¯ä¿è¯OOD [[paper]](https://arxiv.org/pdf/2207.06687.pdf)

7. **ASSESSING MODEL OUT-OF-DISTRIBUTION GENERALIZATION WITH SOFTMAX PREDICTION PROBABILITY BASELINES AND A CORRELATION METHOD** (ICLR 2023 rejected) æå‡ºäº†æ–°çš„OOD metricï¼Œé¼“åŠ±ï¼šâ‘ é¢„æµ‹çš„confidenté«˜ â‘¡é¢„æµ‹å…·æœ‰å¤šæ ·æ€§ï¼ˆé¿å…æ‰€æœ‰æ ·æœ¬è¢«é¢„æµ‹ä¸ºåŒä¸€ä¸ªç±»ï¼‰ã€‚å› ä¸ºè¢«å®¡ç¨¿äººå–·ç¼ºä¹ç†è®ºä¾æ®è€Œè¢«æ‹’ã€‚[[openreview] ](https://openreview.net/forum?id=1maXoEyeqx)

8. **MODELING THE DATA-GENERATING PROCESS IS NECESSARY FOR OUT-OF-DISTRIBUTION GENERALIZATION** (ICLR 2023 top 25%) ç”¨ç»Ÿä¸€çš„causal graphæ¥æè¿°correlation shiftå’Œdiversity shiftï¼Œæå‡ºäº†multi-shiftçš„é—®é¢˜ï¼Œå¹¶è¯æ˜äº†å­¦åˆ°çš„è¡¨ç¤ºå¿…é¡»æ»¡è¶³ç”±causal graphæ‰€å¯¼å‡ºçš„æ¡ä»¶ç‹¬ç«‹æ€§æ¡ä»¶æ˜¯ä¿è¯OODæ³›åŒ–çš„å¿…è¦æ¡ä»¶ã€‚ [[paper]](https://openreview.net/forum?id=uyqks-LILZX) [[è®ºæ–‡åˆ†äº«ä¼šslides]](./all_notes/2023.3.15[ICLR2023]CACM.pdf)

9. **Domain Generalization via Nuclear Norm Regularization** (Arxiv 2023) è®¤ä¸ºå¼•å…¥å¯¹ $\phi(x)$ çš„ä½ç§©constraintæœ‰åŠ©äºè§£è€¦ä¸å˜ç‰¹å¾å’Œç¯å¢ƒç‰¹å¾ [[paper]](https://arxiv.org/pdf/2303.07527.pdf)

10. **REVISITING ADAPTERS WITH ADVERSARIAL TRAINING** å‘ç°AdvPropä¸­ä¸ºcleanå’Œadvæ ·æœ¬åˆ†åˆ«å‡†å¤‡BNå±‚ä¸æ˜¯å¿…é¡»çš„ï¼Œå…³é”®åœ¨äºå…±äº«å‚æ•°+domain adaptator (ICLR 2023) [[paper]](https://openreview.net/forum?id=HPdxC1THU8T)

11. **UNDERSTANDING OUT-OF-DISTRIBUTION ACCURACIES THROUGH QUANTIFYING DIFFICULTY OF TEST SAMPLES**  (Arxiv 2023) ä½¿ç”¨entropy based confusion scoreæ¥è¡¡é‡æ ·æœ¬çš„difficultyï¼Œå‘ç°OODæ•°æ®confusion scoreæ›´é«˜ï¼ˆæ¨¡å‹é¢„æµ‹çš„ç†µæ›´é«˜ï¼‰ [[paper]](https://arxiv.org/pdf/2203.15100.pdf)

12. **Effective Robustness against Natural Distribution Shifts for Models with Different Training Data** (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2302.01381.pdf)

13. **Sharpness-Aware Gradient Matching for Domain Generalization** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Sharpness-Aware_Gradient_Matching_for_Domain_Generalization_CVPR_2023_paper.html)

14. **Improved Test-Time Adaptation for Domain Generalization** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Improved_Test-Time_Adaptation_for_Domain_Generalization_CVPR_2023_paper.html)

15. **Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Michalkiewicz_Domain_Generalization_Guided_by_Gradient_Signal_to_Noise_Ratio_of_ICCV_2023_paper.html)

16. **Cross Contrasting Feature Perturbation for Domain Generalization** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Li_Cross_Contrasting_Feature_Perturbation_for_Domain_Generalization_ICCV_2023_paper.html)

17. **A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance** (ICCV 2023) å°†CLIPçš„language encoderè¾“å‡ºçš„embeddingä½œä¸º"generic text representation"ï¼Œç„¶åè®©studentï¼ˆvisual modelï¼‰çš„è¡¨ç¤ºå»å¯¹é½teacherï¼ˆCLIPï¼‰çš„text representationã€‚åŒæ—¶å¯¹é½studentå’Œteacherçš„é¢„æµ‹logitã€‚ã€insight 1ã€‘recent workå‘ç°åŸºäºç¯å¢ƒåˆ’åˆ†çš„æ–¹æ³•ä¸é‚£ä¹ˆworkï¼Œå› ä¸ºçœŸå®ä¸–ç•Œçš„ç¯å¢ƒåˆ’åˆ†ä¸æ˜ç¡® ã€insight 2ã€‘ä»ä¼˜åŒ–éš¾åº¦è§’åº¦è¯´æ˜anchor sampleåœ¨å¯¹é½æ—¶çš„ä½œç”¨ [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Huang_A_Sentence_Speaks_a_Thousand_Images_Domain_Generalization_through_Distilling_ICCV_2023_paper.html)

18. **Understanding Hessian Alignment for Domain Generalization** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Hemati_Understanding_Hessian_Alignment_for_Domain_Generalization_ICCV_2023_paper.pdf)

19. **Flatness-Aware Minimization for Domain Generalization** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Flatness-Aware_Minimization_for_Domain_Generalization_ICCV_2023_paper.html)

20. **MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_MAP_Towards_Balanced_Generalization_of_IID_and_OOD_through_Model-Agnostic_ICCV_2023_paper.html)

21. **Learning Diverse Features in Vision Transformers for Improved Generalization** (ICML 2023 Spurious Correlations Workshop) æœ€å°åŒ–ViTä¸åŒheadå¯¹äºåŒä¸€tokenæ¢¯åº¦çš„ç›¸ä¼¼æ€§æ¥é¼“åŠ±æ¨¡å‹çš„diversityã€‚å®éªŒå‘ç°ViTçš„ä¸åŒtokenä¹‹é—´æœ‰äº›æ˜¯spuriousï¼Œæœ‰äº›æ˜¯shift-robustçš„ã€‚æå‡ºçš„regularizationæœ‰åŠ©äºæ‰©å¤§ä¸åŒheadçš„gapã€‚ [[paper]](https://arxiv.org/pdf/2210.04206.pdf)

22. **Towards Understanding Feature Learning in Out-of-Distribution Generalization** (NeulPS 2023) ERMå·²ç»èƒ½å¤Ÿå­¦åˆ°invå’Œspç‰¹å¾ã€‚ä¸å¥½çš„ERM pretrainä¼šå¯¼è‡´IRMv1å­¦ä¸åˆ°invç‰¹å¾ã€‚æå‡ºäº†ä¸€ç§è¿­ä»£å­¦ä¹ å‰ä¸€è½®æ²¡å­¦åˆ°çš„ç‰¹å¾çš„ç®—æ³•FATã€‚ [[paper]](https://arxiv.org/pdf/2304.11327.pdf)

23. **Diversify and Disambiguate: Learning From Underspecified Data** (ICLR 2023) å¾ˆåƒCVPR2022 Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions With Superior OOD Generalizationçš„åšæ³•ï¼šåŒä¸€ä¸ªbackboneï¼ˆfeature extractorï¼‰è®­ç»ƒå¾ˆå¤šheadï¼ŒåŠ æ­£åˆ™é¼“åŠ±è¿™äº›headçš„ä¸åŒï¼Œç„¶åå€ŸåŠ©oracleä¿¡æ¯æ¥é€‰å‡ºä¸€ä¸ªå¤´ï¼ˆæœ¬æ–‡æ˜¯ç”¨ä¸€ä¸ªæ‰€è°“çš„æœ€informativeçš„æµ‹è¯•å­é›†æ¥é€‰ï¼ŒCVPRé‚£ç¯‡æ˜¯ç”¨ä¸€ä¸ªood validation setæ¥é€‰ï¼‰ã€‚[[paper]](http://arxiv.org/abs/2202.03418)

24. **SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization** (NeurIPS 2023) å¤šæ¨¡æ€DGï¼Œå°†å„æ¨¡æ€çš„ç‰¹å¾åˆ†ä¸ºä¸åŒæ¨¡æ€shareçš„éƒ¨åˆ†ä»¥åŠå„ä¸ªæ¨¡æ€specificçš„éƒ¨åˆ†ï¼Œæ‹‰è¿‘åŒä¸€classçš„ä¸åŒæ¨¡æ€sharedéƒ¨åˆ†çš„ç‰¹å¾ï¼Œæ¨è¿œshareéƒ¨åˆ†çš„ç‰¹å¾å’Œspecificéƒ¨åˆ†çš„ç‰¹å¾ã€‚[[paper]](https://openreview.net/forum?id=RiSMijlsLT)

25. **PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization** (ICCV 2023) å…ˆè®­ç»ƒKä¸ªç”¨äºç”Ÿæˆstyle word embeddingï¼ˆå¤šæ ·æ€§+ä¿æŒè¯­ä¹‰ï¼‰ï¼Œç„¶åæŠŠå®ƒä»¬å’ŒNä¸ªç±»åˆ«è¯embeddingç»“åˆï¼Œå–‚ç»™CLIPtextç”ŸæˆK*Nä¸ªstyle-contentçš„text featureã€‚ä¹‹åæ‹¿è¿™äº›featureè®­ç»ƒä¸€ä¸ªlinear classifierã€‚æ¨æ–­æ—¶æŠŠè¿™ä¸ªclassifieræ¥åˆ°CLIP image encoderä¸Šï¼ˆ**å› ä¸ºåœ¨joint image-textç©ºé—´ä¸­ï¼Œå­˜åœ¨cross-modal transferability phenomenon**ï¼‰ã€‚ [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Cho_PromptStyler_Prompt-driven_Style_Generation_for_Source-free_Domain_Generalization_ICCV_2023_paper.html)

26. **Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization** (ICML 2023) åˆ©ç”¨å¾ˆå¤šåœ¨ä¸åŒä»»åŠ¡ä¸Šfine-tuneçš„åŒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ¥æå‡OODæ€§èƒ½ [[paper]](https://proceedings.mlr.press/v202/rame23a/rame23a.pdf) 

27. **Explore and Exploit the Diverse Knowledge in Model Zoo for Domain Generalization** (ICML 2023) å……åˆ†åˆ©ç”¨å¤šæ ·çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¸ä»…ä»…æ˜¯åˆ©ç”¨æœ€å¼ºçš„æ¨¡å‹ï¼‰æ¥æå‡OODæ€§èƒ½ [[paper]](https://proceedings.mlr.press/v202/chen23m/chen23m.pdf)

    


### 2022

1. **Out-of-distribution Generalization with Causal Invariant Transformations** (CVPR2022) ç»™å‡ºäº†ä¸€ç§ä¸éœ€è¦discover causal featureçš„æ–¹æ³•ï¼Œè€Œæ˜¯ç›´æ¥æ‰¾Causal Invariant Transformationï¼Œè¯æ˜äº†å¦‚æœç½‘ç»œè¾“å‡ºå¯¹äºCITä¸å˜ï¼Œåˆ™å¯å®ç°OODæ³›åŒ– [[paper]](https://arxiv.org/abs/2203.11528)[[note]](./all_notes/Out-of-distribution_Generalization_with_Causal_Invariant_Transformations.md)
2. **Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization** (CVPR2022) å¯¹AdaINçš„æ”¹è¿›ï¼Œé€šè¿‡å¯¹é½å†…å®¹å›¾ç‰‡å’Œé£æ ¼å›¾ç‰‡çš„é«˜é˜¶ç»Ÿè®¡é‡ï¼Œæ¥å®ç°æ›´å¥½çš„é£æ ¼è¿ç§»ï¼Œå¹¶ç”¨è¿ç§»çš„å›¾ç‰‡æ¥å¢å¼ºæ³›åŒ–èƒ½åŠ› [[paper]](https://arxiv.org/abs/2203.07740) [[è®ºæ–‡åˆ†äº«ä¼šslides]](./all_notes/2022.11.17-EHM(final).pdf)
3. **Enhance the Visual Representation via Discrete Adversarial Training** (NeurIPS 2022) åˆ©ç”¨VQGANï¼Œåœ¨å…¶äº§ç”Ÿçš„ç¦»æ•£è¡¨ç¤ºä¸ŠåšATï¼Œç”Ÿæˆå…·æœ‰æ›´å°‘é«˜é¢‘å™ªå£°ã€æ›´å¤§èŒƒæ•°ã€æ›´ç¬¦åˆç°å®ä¸–ç•ŒOODåç§»çš„augmented dataï¼Œæ¥å¢å¼ºOODæ³›åŒ–ã€‚åˆ·çˆ†è®¸å¤šImageNet OOD datasetçš„SOTAã€‚ [[paper]](https://arxiv.org/abs/2209.07735) [[è®ºæ–‡åˆ†äº«ä¼šslides]](./all_notes/2022.12.15DAT.pdf)
4. **Pyramid Adversarial Training Improves ViT Performance** (CVPR2022) é€šè¿‡å…·æœ‰ä¸åŒå°ºåº¦ç»“æ„æ‰°åŠ¨çš„ATæ¥å¢å¼ºViTæ€§èƒ½ï¼Œå­¦åˆ°çš„æ‰°åŠ¨èƒ½ä¿å­˜shapeä¿¡æ¯ [[paper]](https://arxiv.org/abs/2111.15121)
5. **CROSSMATCH: CROSS-CLASSIFIER CONSISTENCY REGULARIZATION FOR OPEN-SET SINGLE DOMAIN GENERALIZATION** (ICLR2022) å¾ˆéš¾çš„æ–°settingï¼Œopen-set single domain generalization [[paper]](https://openreview.net/forum?id=48RBsJwGkJf) [[notes]](./all_notes/cross-matching-ICLR22.pdf)
6. **UNCERTAINTY MODELING FOR OUT-OF-DISTRIBUTION GENERALIZATION** (ICLR2022) å‡è®¾æ•°æ®çš„å‡å€¼å’Œæ–¹å·®æœä»æ½œåœ¨çš„é«˜æ–¯åˆ†å¸ƒ $\mathcal{N}(\mu,\Sigma_\mu)$ å’Œ $\mathcal{N}(\sigma,\Sigma_\sigma)$ ï¼Œæå‡ºäº†ä¼°è®¡ $\Sigma_\mu$ å’Œ $\Sigma_\sigma$ çš„æ–¹æ³•ï¼Œå¹¶ç”±æ­¤æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼º [[paper]](https://arxiv.org/abs/2202.03958) [[notes]](./all_notes/uncertain.pdf)
7. **Gradient Matching for Domain Generalization** ï¼ˆICLR2022ï¼‰é€šè¿‡æœ€å¤§åŒ–ä¸åŒenvironmentçš„lossæ¢¯åº¦çš„å†…ç§¯ï¼Œæ¥å¯»æ‰¾domain invariant featuresã€‚[[paper]](https://openreview.net/forum?id=vDwBW49HmO) [[notes]](./all_notes/grad_fishr.pdf)
8. **Fishr: Invariant Gradient Variances for Out-of-distribution Generalization** (ICML2022) matchä¸åŒdomainçš„lossæ¢¯åº¦çš„åæ–¹å·®çŸ©é˜µ [[paper]](https://arxiv.org/abs/2109.02934) [[notes]](./all_notes/grad_fishr.pdf)
9. **Towards Principled Disentanglement for Domain Generalization** (CVPR2022 Oral) åˆ†åˆ«ä½¿ç”¨ä¸¤ä¸ªç½‘ç»œå»ºæ¨¡semanticç‰¹å¾å’Œvariationç‰¹å¾ï¼ˆnon-causalï¼‰ï¼Œå¹¶é€šè¿‡è¿™ä¸¤ä¸ªç‰¹å¾é‡å»ºxï¼Œè¦æ±‚é‡å»ºè¿‡ç¨‹å…³äºvariationç‰¹å¾ä¸å˜ï¼Œæå‡ºäº†æ‰€è°“çš„*Invariance based on disentanglement*ã€‚ [[paper]](https://arxiv.org/abs/2111.13839)
10. **On the Strong Correlation Between Model Invariance and Generalization** (NeurIPS2022) æå‡ºäº†æ–°çš„ä¸å˜æ€§è¡¡é‡æ ‡å‡†EIï¼ˆè¡¡é‡çš„æ˜¯ç½‘ç»œå¯¹äºxå’Œç»OODå˜æ¢åçš„x'çš„é¢„æµ‹çš„å·®è·ï¼‰ï¼Œå‘ç°å…¶å¯¹äºä¸å˜æ€§çš„åˆ»ç”»è¿œä¼˜äºJS divergenceã€‚åŒæ—¶éªŒè¯äº†EIä¸OODæ³›åŒ–æ€§èƒ½çš„æ­£æ¯”å…³ç³»ã€‚[[paper]](https://arxiv.org/pdf/2207.07065.pdf)
11. **OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization** (CVPR2022 Oral) diversity shift/Correlation shift [[paper]](https://arxiv.org/abs/2106.03721) [[notes(åŒ…å«å…³äºä¸¤ç§OODæ•°æ®é›†æ‰€å¯¼å‡ºçš„ P(X) ä»¥åŠ P(Y|X)) æ€§è´¨çš„æ€»ç»“]](./all_noets/oodbench.pdf)
12. **Assaying Out-Of-Distribution Generalization in Transfer Learning** (Arxiv 2022) å„ç§metricå¯¹OOD accå½±å“çš„å®éªŒæ€§æ€»ç»“ [[paper]](https://arxiv.org/abs/2207.09239)
13. **On Certifying and Improving Generalization to Unseen Domains** (Arxiv 2022) å‘ç°äº†æ¨¡å‹å¯¹äºåœ¨åŒä¸€åˆ†å¸ƒè·ç¦»ä¸‹ä¸åŒç±»å‹OOD shiftçš„æ€§èƒ½æœ‰è¾ƒå¤§å·®å¼‚ï¼Œæå‡ºäº†minimizeæŸä¸€è·ç¦»ä¸‹worst-case lossã€‚ [[paper]](https://arxiv.org/abs/2206.12364) [[notes]](./all_notes/certify.md)
14. **ZooD: Exploiting Model Zoo for Out-of-Distribution Generalization** æå‡ºäº†ä¸€ç§OOD accuracyçš„metricï¼šä»å¤šä¸ªé¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨ä¸­ä¸­æ ¹æ®MLEé€‰å‡ºä½¿ä¼¼ç„¶$p(y'|\phi',y,\phi)$å’Œ$p(\phi'|\phi)$æœ€å¤§çš„æ¨¡å‹ã€‚åœ¨domainbedæœ‰å¤§å¹…æå‡ã€‚[[paper]](https://arxiv.org/pdf/2210.09236.pdf)
15. **ID AND OOD PERFORMANCE ARE SOMETIMES INVERSELY CORRELATED ON REAL-WORLD DATASETS** (Arxiv 2022) å‘ç°äº†ä¹‹å‰å·¥ä½œä¸­å¿½ç•¥çš„OODå’ŒID accurayå‘ˆåç›¸å…³çš„æƒ…å†µï¼ˆæ–‡ç« claimè¿™æ˜¯ç”±äºdiversity-inducing methodä¸å¤Ÿå¤šã€ä¸åŒæ¨¡å‹çš„è¡¨ç°åœ¨åŒä¸€training epochæ˜¯ä¸åŒçš„è€Œä¸åº”è¯¥æ··åœ¨ä¸€èµ·ã€ä¸æ˜¯æ‰€æœ‰çš„pretrain seedéƒ½ä¼šå‡ºç°è€Œå¼•èµ·ï¼‰ [[https://arxiv.org/pdf/2209.00613.pdf]]
16. **Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift** (NeurIPS 2022) åœ¨å¯ä»¥è·å–OOD unlabelled dataçš„æƒ…å†µä¸‹ï¼Œå‘ç°IDå’ŒOOD accuracyæœ‰å¼ºæ­£ç›¸å…³ $\leftrightarrow$ IDå’ŒOODçš„agreementå¼ºæ­£ç›¸å…³ [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/7a8d388b7a17df480856dff1cc079b08-Paper-Conference.pdf)
17. **When Does Group Invariant Learning Survive Spurious Correlations?** (NeurIPS 2022) æŒ‡å‡ºäº†group invariant learningå¿…é¡»æ»¡è¶³çš„ä¸¤ä¸ªå‡†åˆ™
18. **Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions with Superior OOD Generalization** (CVPR 2022) ä¸»å¼ ä½¿ç”¨å¤šä¸ªåˆ†ç±»å™¨ï¼Œå¹¶é¼“åŠ±è¿™äº›åˆ†ç±»å™¨è¾“å‡ºå¯¹è¾“å…¥ç‰¹å¾çš„æ¢¯åº¦å·®å¼‚å°½é‡å¤§ï¼Œä»¥å¾—åˆ°diverseçš„æ¨¡å‹ã€‚ä¹‹åä¾èµ–äºOOD validation é€‰å‡ºä¸€ä¸ªæœ€å¥½çš„æ¨¡å‹ä½œä¸ºæœ€ç»ˆæµ‹è¯•çš„æ¨¡å‹ã€‚ [[paper]](https://ieeexplore.ieee.org/document/9878716/) 
19. **Rich Feature Construction for the Optimization-Generalization Dilemma** (ICML 2022) ERM pre-trainå¯¹äºæå‡OOD objectivveçš„æ€§èƒ½å¾ˆå…³é”® [[paper]](https://proceedings.mlr.press/v162/zhang22u/zhang22u.pdf)
20. **Diverse Weight Averaging for Out-of-Distribution Generalization** (DiWA) (NeurIPS 2022) ensemblingï¼Œå¹³å‡ä¸åŒè¶…å‚çš„æ¨¡å‹ [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/46108d807b50ad4144eb353b5d0e8851-Paper-Conference.pdf)
21. **Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization** (EoA) (NeurIPS 2022) å…ˆå¯¹æ¯ä¸ªæ¨¡å‹åšæ²¿è®­ç»ƒè½¨è¿¹çš„moving averageï¼ˆä¸åŒepochï¼‰ï¼Œå†å¯¹è¿™äº›moving averageå¾—åˆ°çš„æ¨¡å‹åšensemblingï¼ˆä¸åŒè¶…å‚ï¼‰ã€‚å…·ä½“åšæ³•ï¼šsimply combines all checkpoints uniformly starting from batch 100 until the end of training [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/372cb7805eaccb2b7eed641271a30eec-Paper-Conference.pdf)
22. **Model Agnostic Sample Reweighting for Out-of-Distribution Learning** (MAPLE) (ICML 2022) æå‡ºäº†ä¸€ä¸ªè¿­ä»£è¿›è¡Œçš„bilevel optimizationè¿‡ç¨‹ï¼š(1)ä¼˜åŒ–ä»»æ„çš„ä¸€ä¸ªOOD lossæ›´æ–°æ¨¡å‹å‚æ•°$\theta^*$ (2)åœ¨æ›´æ–°å¾—åˆ°çš„$\theta^*$ä¸‹ä¼˜åŒ–ERM lossï¼Œæ›´æ–°weight $w$
23. 

### 2021

1. **The Many Faces of Robustness: A Critical Analysis of OOD Generalization** (ICCV 2021) OOD robustnessä¸åº”è¯¥æ˜¯ä¸€ä¸ªç®€å•çš„æŒ‡æ ‡ï¼Œå®ƒåœ¨ä¸åŒdistribution shiftä¸‹åº”è¯¥æ˜¯ä¸åŒçš„æŒ‡æ ‡ã€‚éš¾ä»¥æ¨æ–­ç°æœ‰æ–¹æ³•ä¸­å“ªä¸ªèƒ½æ›´å¹¿æ³›åœ°work[[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Hendrycks_The_Many_Faces_of_Robustness_A_Critical_Analysis_of_Out-of-Distribution_ICCV_2021_paper.pdf)[[note]](./all_notes/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-OOD-Generalization.pdf)
2. **Improved OOD Generalization via Adversarial Training and Pre-training** (ICML 2021) ä»ç†è®ºä¸Šè¯æ˜äº†åœ¨ç”¨Wassersteinè·ç¦»åˆ»ç”»åˆ†å¸ƒåç§»çš„æƒ…å†µä¸‹ï¼ŒATå¯¹ä¸€å®šè·ç¦»å†…çš„OODæ•°æ®æœ‰æ³›åŒ–èƒ½åŠ› [[paper]](https://arxiv.org/abs/2105.11144) 
3. **Learning Representations that Support Robust Transfer of Predictors** (Arxiv 2021) æå‡ºäº†TRM(Transfer Risk Minimization)ï¼ŒæŠŠæ¨¡å‹åœ¨ä¸åŒdomainçš„æ³›åŒ–èƒ½åŠ›ç›´æ¥ä½œä¸ºobjectiveä¼˜åŒ– [[paper]](https://arxiv.org/abs/2110.09940) [[notes]](./all_notes/TRM.pdf)
4. **Learning Causal Semantic Representation for OoD prediction** (NeurIPS 2021) å¾ˆç¡¬æ ¸ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„Causal Semantic Modelï¼Œæ–‡ç« å…³é”®å‡è®¾æ˜¯ $p(x|s,v)$ å’Œ $p(y|s)$ åœ¨domainé—´ä¿æŒä¸å˜ï¼Œ $p(s,v)$ æ˜¯domain changeçš„å”¯ä¸€æ¥æºã€‚è¯æ˜äº†causalæœºåˆ¶çš„å¯è¾¨è¯†æ€§  [[paper]](https://proceedings.neurips.cc/paper/2021/file/310614fca8fb8e5491295336298c340f-Paper.pdf) [[notes]](./all_notes/causal1.pdf)
5. **Exploiting Domain-Specifific Features to Enhance Domain Generalization** (NeurIPS 2021) ä½¿ç”¨äº†å¾ˆå¤štrickï¼šä»information bottleneckçš„è§’åº¦è¯æ˜äº†domain-specific featureå¯¹æ³›åŒ–ä¹Ÿæ˜¯æœ‰å¸®åŠ©çš„ï¼›æå‡ºäº†ä¸€ç§åŸºäºæœ€å°åŒ–domain-invariant featureå’Œdomain-specific featureä¹‹é—´åæ–¹å·®çŸ©é˜µçš„è§£è€¦ï¼›æå‡ºé€šè¿‡meta learningæ¥åˆ©ç”¨domain-specific featureçš„æ³›åŒ–èƒ½åŠ›ã€‚[[paper]](https://arxiv.org/abs/2110.09410) [[notes]](./all_notes/disen1.md)
6. **Towards a Theoretical Framework of Out-of-Distribution Generalization** (NeurIPS 2021) é€šè¿‡å®šä¹‰äº†conditioned on classçš„è¡¨ç¤ºä¸å˜æ€§ï¼Œä»¥åŠè¡¨ç¤ºçš„informativenessï¼Œå¹¶æ®æ­¤å®šä¹‰äº†ä»€ä¹ˆæ˜¯å¯å­¦ä¹ çš„OODé—®é¢˜ï¼Œæ¨å¯¼å‡ºäº†OODæ³›åŒ–è¯¯å·®ä¸Šç•Œå¯ä»¥è¢«è¿™ä¸¤ä¸ªé‡è¡¨ç¤º [[paper]](https://proceedings.neurips.cc/paper/2021/hash/c5c1cb0bebd56ae38817b251ad72bedb-Abstract.html) 
7. **Quantifying and Improving Transferability in Domain Generalization** (NeurIPS 2021)  æå‡ºäº†åŸºäºç»™å®šå‡è®¾æ—çš„åŸŸé—´å¯è¿ç§»æ€§çš„æŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå®¹æ˜“è¿ç§»/æ³›åŒ–ï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªmin-maxä¼˜åŒ–è¿‡ç¨‹æ¥æå‡è¿ç§»æ€§ï¼ˆæœ‰ç†è®ºä¿è¯ï¼‰ [[paper]](https://arxiv.org/abs/2106.03632)
8. **SWAD: Domain Generalization by Seeking Flat Minima** (NeurIPS 2021) æå‡ºäº†SWADï¼ˆå¹³å‡åŒä¸€æ¨¡å‹çš„ä¸åŒepochçš„å‚æ•°ï¼‰ï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†flatçš„loss landscapeå¯¹OODæ³›åŒ–æœ‰å¸®åŠ© [[paper]](https://arxiv.org/pdf/2102.08604.pdf)
9. **Domain Generalization Using Causal Matching** (ICML 2021) å¯¹é½ä¸åŒç¯å¢ƒçš„åŒä¸€ç±»åˆ«çš„è¡¨ç¤ºæ¥å­¦ä¹ ä¸å˜ç‰¹å¾ã€‚[[paper]](http://arxiv.org/abs/2006.07500)
10. **Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization** (Arxiv 2021) æå‡ºäº†IB-IRMï¼Œå³åŠ äº†ä¸€ä¸ªæœ€å°åŒ–æ¨¡å‹è¡¨ç¤ºçš„ç†µçš„ç›®æ ‡ã€‚æœ¬æ–‡çš„ä¸€ä¸ªæ ¸å¿ƒè§‚å¯Ÿæ˜¯ï¼šå½“Yå’Œspurious featureæ²¡æœ‰å…³è”ï¼ˆæœ¬æ–‡ç§°ä½œFIIFï¼Œfully informative invariant featuresï¼Œå¯¹åº”äºcovariate shiftçš„æƒ…å†µï¼‰æ—¶ï¼Œæ™®é€šIRMä¼šå¤±æ•ˆï¼›åœ¨æ–‡ä¸­æ„å»ºçš„FIIF exampleä¸‹ï¼Œåªä½¿ç”¨inv featureæˆ–åªä½¿ç”¨sp featureéƒ½ä¼šå¯¼è‡´è¡¨ç¤ºçš„ç†µæ¯”åŒæ—¶ä½¿ç”¨invå’Œsp featureçš„è¡¨ç¤ºçš„ç†µæ›´ä½ï¼Œå› æ­¤IBçš„ç›®æ ‡ä¼šå¯¼è‡´åªä½¿ç”¨invæˆ–åªä½¿ç”¨spçš„è§£ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šæœ€å°åŒ–åˆ†ç±»lossï¼Œå°±èƒ½å­¦åˆ°ä»…ä¾èµ–äºinvçš„è§£ã€‚ [[paper]](http://arxiv.org/abs/2106.06607) [[notes]](./all_notes/IB-IRM.md)
11. 

### 2020

1. **Out-of-Distribution Generalization via Risk Extrapolation (V-REx)** (2020) ç›®æ ‡ï¼š

   $\min _{\omega, \Phi} \sum^e \mathcal{R}^e(\omega, \Phi)+\lambda \text{Var}\left(\mathcal{R}^e(\omega, \Phi)\right)$ï¼ŒåŠ¨æœºï¼šå¦‚æœ$\Phi$æ˜¯ä¸å˜è¡¨ç¤ºï¼Œé‚£ä¹ˆä¸åŒç¯å¢ƒeåº”è¯¥æ»¡è¶³ $P^e(Y \mid \Phi(X))=P(Y \mid \Phi(X))$ ï¼Œæ­¤æ—¶lossä¹Ÿåº”è¯¥ä¸€æ ·ï¼Œæ‰€ä»¥å¯¹lossçš„æ–¹å·®åšæ­£åˆ™ã€‚ï¼ˆä¸ªäººè§‰å¾—å­˜åœ¨é—®é¢˜ï¼Œå› ä¸ºå½“ä¸åŒç¯å¢ƒlossçš„å€¼ä¸€æ ·æ—¶ï¼Œæœ€ä¼˜åˆ†ç±»å™¨ä¹Ÿä¸ä¸€å®šä¸€æ ·ï¼‰[[paper]](https://arxiv.org/pdf/2003.00688.pdf)

2. **DISTRIBUTIONALLY ROBUST NEURAL NETWORKS FOR GROUP SHIFTS: ON THE IMPORTANCE OF REGULARIZATION FOR WORST-CASE GENERALIZATION (Group DRO)** (ICLR 2020) å°†æ•°æ®é›†åˆ†æˆ(spurious feature, label)çš„groupï¼Œæœ€å°åŒ–æ¯ä¸ªgroupä¸Šçš„lossçš„æœ€å¤§çš„å‡¸ç»„åˆï¼ˆæƒé‡å¯ä»¥ä¼˜åŒ–ï¼Œlossæ›´å¤§çš„groupçš„losså€¾å‘äºæœ‰æ›´å¤§çš„æƒé‡ï¼‰ [[paper]](https://arxiv.org/abs/1911.08731)

3. **AUGMIX: A SIMPLE DATA PROCESSING METHOD TO IMPROVE ROBUSTNESS AND UNCERTAINTY** (ICLR 2020) é¼“åŠ±æ¨¡å‹å¯¹äºç»è¿‡ä¸åŒdata augmentationå¢å¼ºåçš„æ ·æœ¬çš„é¢„æµ‹ä¸€è‡´æ€§ [[paper]](https://arxiv.org/pdf/1912.02781.pdf)

4. **Adversarial Examples Improve Image Recognition** (NeurIPS 2020)  Advprop [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Xie_Adversarial_Examples_Improve_Image_Recognition_CVPR_2020_paper.html)

5. **Improving out-of-distribution generalization via multi-task self-supervised pretraining** (Arxiv 2020) SSL pretraining tasks for OOD [[paper]](http://arxiv.org/abs/2003.13525)

### ç»å…¸æ–‡ç« 

1. **Domain-Adversarial Training of Neural Networks** (2016) æå‡ºDANNï¼Œè®­ç»ƒdomain labelåˆ¤åˆ«å™¨æ¥ä¿ƒä½¿ç‰¹å¾æå–å™¨å­¦ä¹ åˆ°domain-invariantçš„ç‰¹å¾ [[paper]](https://www.jmlr.org/papers/volume17/15-239/15-239.pdf)
2. **In Search of Lost Domain Generalization** (ICLR 2021) Domainbed benchmark [[paper]](https://arxiv.org/abs/2007.01434)
3. **Invariant Risk Minimization** (2019) IRM æ³¨æ„ï¼šIRMçš„æ³›åŒ–è¯¯å·®boundä¸­ï¼ŒXæ˜¯é€šè¿‡è™šå‡ç‰¹å¾å’Œä¸å˜ç‰¹å¾concatenateå†ç»è¿‡çº¿æ€§å˜æ¢å¾—æ¥çš„ï¼Œå¾ˆstrict [[paper]](https://arxiv.org/abs/1907.02893)
4. **Self-Challenging Improves Cross-Domain Generalization** (Arxiv 2020) RSC 
5. **UNDERSTANDING THE FAILURE MODES OF OUT-OFDISTRIBUTION GENERALIZATION** (ICLR 2021) åˆ»ç”»äº†ERM OODæ³›åŒ–å¤±è´¥çš„ä¸¤ç§æƒ…å†µï¼šgeometric skewå’Œstatistical skewã€‚ç›´è§‚ç†è§£ï¼šgeometric skewæ˜¯ç”±äºmax-margin classifierç”±äºmajor partçš„å­˜åœ¨å¯¼è‡´ä¸ºäº†ç»´æŒæœ€å¤§marginï¼Œä¸å¾—ä¸ä½¿ç”¨spurious featureï¼›statistical skewçº¯ç²¹æ˜¯ç”±äºåˆ†æ$\frac{w_{\text{sp}(t)}}{w_{\text{inv}(t)}}$çš„æ”¶æ•›ï¼Œå‘ç°è¯¥æ¯”å€¼çš„lower boundä¼šéšç€spurious correlation pçš„å¢åŠ è€Œå•è°ƒä¸Šå‡ã€‚ [[paper]](http://arxiv.org/abs/2010.15775)







## Test-time adaptation for Generalization

### 2023

1. **Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization** [[paper]](https://arxiv.org/pdf/2311.01459.pdf)
2. 




## Domain Generalization/Adaptation in Down-stream CV/NLP Tasks

### 2023

1. **CLIP the Gap: A Domain Generalization Approach for Object Detection** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Vidit_CLIP_the_Gap_A_Single_Domain_Generalization_Approach_for_Object_CVPR_2023_paper.html)
2. **Improving Zero-Shot Generalization and Robustness of Multi-Modal Models** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.html)
3. **Towards Domain Generalization for Multi-View 3D Object Detection in Bird-Eye-View** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Towards_Domain_Generalization_for_Multi-View_3D_Object_Detection_in_Bird-Eye-View_CVPR_2023_paper.html)
4. **Single Domain Generalization for LiDAR Semantic Segmentation** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Single_Domain_Generalization_for_LiDAR_Semantic_Segmentation_CVPR_2023_paper.html)
5. **Modality-Agnostic Debiasing for Single Domain Generalization** (CVPR 2023) [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Qu_Modality-Agnostic_Debiasing_for_Single_Domain_Generalization_CVPR_2023_paper.html)
6. **Improving Zero-Shot Generalization for CLIP with Synthesized Prompts** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Improving_Zero-Shot_Generalization_for_CLIP_with_Synthesized_Prompts_ICCV_2023_paper.html)
7. **Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation** (ICCV 2023) [[paper]](https://arxiv.org/abs/2303.16456)
8. **Bayesian Prompt Learning for Image-Language Model Generalization** (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.html) 



## Graph OOD Generalization (graph-level & node-level)

### 2023

1.  **Towards Understanding Generalization of Graph Neural Networks** (ICLR 2023) åˆ†æäº†ä¸åŒGNNæ¶æ„å¯¹æ³›åŒ–è¯¯å·®çš„å½±å“ã€‚ç‰¹åˆ«åœ°ï¼Œå¯¹äºGNNæœ€å¤§åº¦æ•°è¶Šå°ï¼Œæ³›åŒ–è¯¯å·®è¶Šå°ã€‚ [[paper]](https://openreview.net/forum?id=BhMyLk0YNy)
2.  **Multi-task Self-supervised Graph Neural Networks Enable Stronger Task Generalization** (ICLR 2023) åˆ©ç”¨Multiple Gradient Descentæ¥åŒæ—¶ä¼˜åŒ–å¤šä¸ªSSLä»»åŠ¡çš„æŸå¤±ï¼Œä»¥å®ç°task generalization [[paper]](https://openreview.net/forum?id=1tHAZRqftM)
3.  **Mind the Label Shift of Augmentation-based Graph OOD Generalization (LiSA)** (CVPR 2023) æœ€å°åŒ–å¢å¼ºå›¾çš„åˆ†ç±»è¯¯å·®ä»¥åŠå¢å¼ºå›¾ä¸åŸå›¾ï¼ˆå­å›¾å’Œå…¨å›¾ï¼‰çš„äº’ä¿¡æ¯ï¼ŒåŒæ—¶æœ€å¤§åŒ–ä¸åŒå­å›¾ä¹‹é—´åŸºäºenergy functionè¡¡é‡çš„åˆ†å¸ƒè·ç¦»ï¼Œæ¥è·å¾—ä¸€ç³»åˆ—label-invariantçš„å­å›¾ï¼ˆä¸€ä¸ªå­å›¾ç”Ÿæˆå™¨æ¨¡æ‹Ÿäº§ç”Ÿä¸€ä¸ªç¯å¢ƒçš„æ•°æ®ï¼‰ã€‚å¾—åˆ°ä¸åŒç¯å¢ƒæ•°æ®åå†min loss+varï¼ˆå¸¸è§„åšæ³•ï¼‰ [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Mind_the_Label_Shift_of_Augmentation-Based_Graph_OOD_Generalization_CVPR_2023_paper.html)
4.  **Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization (LECI)** (Arxiv 2023) **[FIIF&PIIF]** éœ€è¦ç¯å¢ƒæ ‡ç­¾ã€‚ä¼˜åŒ–ä¸¤ä¸ªç›®æ ‡ï¼š(1)è®©å­å›¾ç”Ÿæˆå™¨äº§ç”Ÿåˆ†ç±»è¯¯å·®å°½é‡å¤§çš„spurious subgraphï¼Œå¹¶å¯¹æŠ—åœ°è®­ç»ƒä¸€ä¸ªç”¨spurious graphåšé¢„æµ‹çš„åˆ†ç±»å™¨ (2)è®©å­å›¾ç”Ÿæˆå™¨äº§ç”Ÿæ— æ³•åˆ¤åˆ«ç¯å¢ƒçš„causal graphï¼Œå¹¶å¯¹æŠ—åœ°è®­ç»ƒä¸€ä¸ªç¯å¢ƒåˆ¤åˆ«å™¨ [[paper]](https://arxiv.org/abs/2306.01103)
5.  **RETHINKING INVARIANT GRAPH REPRESENTATION LEARNING WITHOUT ENVIRONMENT PARTITIONS (GALA)** (ICLR 2023 Domain Generalization Workshop) æ”¹è¿›ç‰ˆCIGAã€‚è®²CIGAçš„causal contrastive lossæ”¹ä¸ºå¯¹é½spurious correlationä¸»å¯¼å’Œinvariant correlationä¸»å¯¼çš„ä¸¤éƒ¨åˆ†å­é›†ï¼Œä¸åŒå…³è”ä¸»å¯¼çš„æ•°æ®é€šè¿‡ä¸€ä¸ªé¢å¤–çš„ERMæ¨¡å‹è·å¾—ï¼ˆERMé¢„æµ‹çš„labelä¸åŒï¼Œå°±è®¤ä¸ºæ˜¯å®ƒä»¬å±äºç”±ä¸åŒçš„å…³è”ä¸»å¯¼çš„partï¼‰ [[paper]](https://openreview.net/forum?id=bjw5jqGtDy)
6.  **FLOOD: A Flexible Invariant Learning Framework for Out-of-Distribution Generalization on Graphs** (KDD 2023) node-level OODä»»åŠ¡ã€‚é€šè¿‡å·²æœ‰çš„å›¾augmentationæ–¹æ³•äº§ç”Ÿä¸€ç³»åˆ—å¢å¼ºç¯å¢ƒï¼Œå†ä½¿ç”¨å¦‚VRExçš„ä¸å˜å­¦ä¹ ç›®æ ‡ï¼›åŒæ—¶åŠ ä¸€ä¸ªBootstrapped Representation Learningç›®æ ‡ã€‚ [[paper]](https://dl.acm.org/doi/10.1145/3580305.3599355)
7.  **Causality and Independence Enhancement for Biased Node Classification** (CIKM 2023) æŠŠcausal featureçœ‹æˆdo(c)ï¼Œç„¶åå‡è®¾sæ˜¯cçš„backdoorï¼ŒåŸºäºæ­¤æ¨¡å‹æ¥ç”¨ä¸€ä¸ªç»éªŒæ€§æ–¹æ³•å»ºæ¨¡p(Y|C,S)ä»è€Œå­¦å‡ºcausal feature/spurious featureï¼ˆé€šè¿‡åœ¨node featureä¸ŠåŠ 2ä¸ªMLPå®ç°ï¼‰ã€‚æ„Ÿè§‰causal graphçš„å‡è®¾å¤ªå¼ºã€‚å¹¶ä¸”åªèƒ½è§£å†³concept shiftã€‚ [[paper]](https://dl.acm.org/doi/10.1145/3583780.3614804)

###  2022

1. **Learning Substructure Invariance for Out-of-Distribution Molecular Representations** (NeurIPS 2022 Spotlight) æ— ç¯å¢ƒæ ‡ç­¾çš„å›¾åˆ†ç±»ï¼Œèƒ½æ‹¿åˆ°å¤šä¸ªå›¾ï¼ˆåˆ†å­ï¼‰ã€‚å…ˆæ¨å¯¼å‡ºäº†ä¸€ä¸ªELBOä»¥è®­ç»ƒå‡ºä¸€ä¸ªç¯å¢ƒåˆ¤åˆ«å™¨ï¼Œä¼˜åŒ–ç›®æ ‡ä¸ºæœ€å¤§åŒ–å›¾è¡¨ç¤º $z$ ä¸æ ‡ç­¾ $y$ä¹‹é—´çš„äº’ä¿¡æ¯ã€æœ€å°åŒ–$I(y;e|z)$ã€‚[[paper]](https://openreview.net/forum?id=2nWUNTnFijm) [[notes]](./all_notes/molecular.pdf)
2. **Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs (CIGA)**  (NeurIPS 2022) **[FIIF&PIIF]** é¦–å…ˆé€šè¿‡gæ¥é¢„æµ‹causalå­å›¾ï¼Œå…¶è¡¥å›¾å°±æ˜¯spurious subgraphã€‚ç„¶åé€šè¿‡æ¨å¯¼è¯´æ˜äº†causal featureä¸ç¯å¢ƒæ— å…³çš„æ¡ä»¶å¯ä»¥è½¬åŒ–ä¸ºæœ€å¤§åŒ–åŒä¸€ç±»åˆ«çš„featureçš„äº’ä¿¡æ¯ï¼ˆé€šè¿‡ä¸€ä¸ªsupervised contrastive losså®ç°ï¼‰ã€‚åœ¨è¿™ä¸ªçº¦æŸä¸‹ï¼Œå†æœ€å¤§åŒ– $G_s$  $G_c$  åˆ†åˆ«ä¸yçš„äº’ä¿¡æ¯ï¼ˆ$G_s$ æ˜¯ $G_c$  çš„è¡¥å›¾ï¼Œæœ€å¤§åŒ–Gså’Œyçš„äº’ä¿¡æ¯æ˜¯ä¸ºäº†â€œå°†Gsä»Gcä¸­åˆ†ç¦»å‡ºå»â€ï¼‰ï¼ŒåŒæ—¶è¦ä¿è¯Gsä¸yçš„äº’ä¿¡æ¯æ¯”Gcå’Œyçš„äº’ä¿¡æ¯è¦å°ï¼ˆäº’ä¿¡æ¯çš„ä¼˜åŒ–é€šè¿‡ä¼˜åŒ–losså®ç°ï¼Œlossè¶Šå°äº’ä¿¡æ¯è¶Šå¤§ï¼‰[[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/8b21a7ea42cbcd1c29a7a88c444cce45-Supplemental-Conference.pdf) [[notes]](./all_notes/nips22_causal.md)
3. **Discovering Invariant Rationales for Graph Neural Networks (DIR)**  (ICLR 2022)  **[FIIF]** å°†ç‚¹ä¹‹é—´ç‰¹å¾å†…ç§¯æœ€å¤§çš„å­å›¾ä½œä¸ºcausalå­å›¾ï¼ŒåŒæ—¶æ‰‹åŠ¨åœ°æ”¹å˜causalå­å›¾å’Œspuriouså­å›¾çš„ç»„åˆï¼Œæ¨¡æ‹Ÿå¤šä¸ªsçš„ç¯å¢ƒã€‚ä¼˜åŒ–ç›®æ ‡ä¸ºä¼˜åŒ–æ‰€æœ‰sä¸‹çš„lossä»¥åŠå…¶æ–¹å·®ã€‚å…¶ä»–è¡¥å……ï¼šâ‘ åŠ äº†ä¸€ä¸ªpredictionå†…ç§¯é¡¹ä»¥å¯¹spurious correlationè¿›è¡Œæ­£åˆ™ â‘¡ä¸“é—¨å­¦ä¸€ä¸ªç”¨sæ¥é¢„æµ‹yçš„åˆ†æ”¯ï¼Œæœ‰ç‚¹ç±»ä¼¼Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs (NIPS 22)çš„åšæ³•ï¼Œæ˜¯ä¸ºäº†æ›´å¥½åœ°è§£è€¦cå’Œsã€‚æ€è€ƒï¼šcausalå­å›¾çš„å¯»æ‰¾æ–¹æ³•ç¼ºä¹ä¾æ® [[paper]](https://arxiv.org/abs/2201.12872) 
4. **Handling Distribution Shifts on Graphs: An Invariance Perspective (EERM)** (ICLR 2022) node-levelåˆ†ç±»ä»»åŠ¡ã€‚å…ˆé€šè¿‡ä¸€ä¸ªAT method ï¼Œè¿›è¡Œåˆ /å¢è¾¹å’Œæœ€å¤§åŒ–æ¯ä¸ªå­å›¾çš„æ–¹å·®æ¥è·å¾—kä¸ªå­å›¾ï¼ˆæ¥æ¨¡æ‹Ÿkä¸ªç¯å¢ƒï¼‰ï¼Œç„¶åå†minæ¯ä¸ªå­å›¾ï¼ˆç¯å¢ƒï¼‰ä¸Šlossçš„varianceå’Œåˆ†ç±»lossã€‚[[paper]](https://arxiv.org/abs/2202.02466) [[notes]](./all_notes/EERM.md)
5. **Shift-Robust Node Classification via Graph Adversarial Clustering** (Arxiv 2022) node-levelåˆ†ç±»ä»»åŠ¡ï¼Œopen-set DAåœºæ™¯ï¼Œä¸€ä¸ªå›¾é‡Œæœ‰labeled source dataå’Œunlabeled target dataã€‚è¯¥æ¡†æ¶å¯è§£å†³new classå’Œnew distributionçš„é—®é¢˜ã€‚å…ˆåœ¨unlabel target domainèšç±»ï¼Œç„¶åæŠŠèšç±»æ¨¡å‹æ‹¿åˆ°label sourceä¸Šå¯¹é½labelã€‚åŒæ—¶ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–èšç±»èƒ½åŠ›ï¼Œè¿˜æœ€å°åŒ–äº†sourceèšç±»ç»“æœå’ŒçœŸå®labelä¹‹é—´çš„KLæ•£åº¦ã€‚[[paper]](https://arxiv.org/pdf/2203.15802.pdf)
6. **GOOD: A Graph Out-of-Distribution Benchmark** (NeurIPS 2022) graph OOD benchmark [[paper]](http://arxiv.org/abs/2206.08452)
7. **Learning Invariant Graph Representations for Out-of-Distribution Generalization** (GIL) graph-level OODä»»åŠ¡ã€‚å…ˆåˆ’åˆ†sprious/causalå­å›¾ï¼Œç„¶åæ‹¿ç€spuriouså­å›¾èšç±»è·å¾—ç¯å¢ƒã€‚ä¹‹åç”¨ä¸€ä¸ªVRExç›®æ ‡ã€‚ [[paper]](https://openreview.net/forum?id=acKK8MQe2xc)

### 2021

1. **DISTRIBUTIONALLY ROBUST SEMI-SUPERVISED LEARNING OVER GRAPHS** (NeurIPS 2021) å¯¹æŠ—åœ°ä¼˜åŒ–DROçš„surrogate lossï¼Œæ¥è·å¾—å¯¹wassersteinè·ç¦»æ„ä¹‰ä¸‹distributionally robustçš„æ¨¡å‹ã€‚[[paper]](http://arxiv.org/abs/2110.10582)
1. **Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training Data** (NeurIPS 2021) ä¸€ç§è§£å†³node-level OOD åˆ†ç±»çš„ç½‘ç»œã€‚[[paper]](https://proceedings.neurips.cc/paper/2021/file/eb55e369affa90f77dd7dc9e2cd33b16-Paper.pdf)
1. **Mixup for Node and Graph Classification** (WWW 2021) å°†è¡¨ç¤ºmixupï¼Œå¹¶å°†å¯¹åº”çš„label mixupèµ·æ¥ï¼Œå°±èƒ½æå‡æ³›åŒ–ã€‚æ–‡ç« æå‡ºäº†node-levelå’Œgraph-levelåˆ†åˆ«çš„mixupæ–¹æ¡ˆï¼Œåç»­æ–‡ç« éªŒè¯äº†node-levelçš„æ–¹æ¡ˆè¦æ¯”graphçš„è¡¨ç°å¥½ [[paper]](https://dl.acm.org/doi/10.1145/3442381.3449796)

### 2020

1. **DROPEDGE: TOWARDS DEEP GRAPH CONVOLUTIONAL NETWORKS ON NODE CLASSIFICATION** (ICLR)



## Domain Adaptation

### 2023

1. **Diffusion-based Target Sampler for Unsupervised Domain Adaptation** ç”¨diffusion modelç”Ÿæˆå’Œç›®æ ‡åŸŸç‰¹å¾ä¸€è‡´çš„æ ·æœ¬æ¥å®ç°DA (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2303.12724.pdf)

### 2019

1. **On Learning Invariant Representations for Domain Adaptation** (ICML2019) ç»å…¸ä¹‹ä½œï¼ŒæŒ‡å‡ºäº†å­¦ä¹ ä¸å˜è¡¨ç¤ºçš„åšæ³•çš„é—®é¢˜ï¼šåœ¨æºåŸŸå’Œç›®æ ‡åŸŸçš„labelçš„è¾¹ç¼˜åˆ†å¸ƒ $p(Y)$ çš„å·®è·è¾ƒå¤§æ—¶ï¼Œå¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸä¸å˜è¡¨ç¤º+æœ€å°åŒ–æºåŸŸè¯¯å·®çš„åšæ³•ä¼šå¯¼è‡´ç›®æ ‡åŸŸè¯¯å·®å˜å¤§ [[paper]](http://proceedings.mlr.press/v97/zhao19a/zhao19a.pdf) [[notes]](./all_notes/On-Learning-Invariant-Representations-forDA.md)
2. **Support and Invertibility in Domain-Invariant Representations** (PMLR2019) ä¹Ÿclaimåªå¯¹é½è¡¨ç¤ºçš„è¾¹ç¼˜åˆ†å¸ƒ $p_s(Z) \approx p_t(Z)$ å¯¹äºDAæ˜¯ä¸å¤Ÿçš„ [[paper]](http://proceedings.mlr.press/v89/johansson19a.html)



## Adversarial Robustness

### 2022

1. **The Dimpled Manifold Model of Adversarial Examples in Machine Learning** (arxiv 2022) ä»manifoldè§’åº¦ç ”ç©¶ç½‘ç»œè®­ç»ƒçš„è¿‡ç¨‹ä»¥åŠATçš„è¡Œä¸ºã€‚å‘ç°è®­ç»ƒDNNå¤§æ¦‚åˆ†ä¸ºä¸¤ä¸ªè¿‡ç¨‹ï¼šâ‘ ä½¿å†³ç­–è¾¹ç•Œç”±éšæœºå¿«é€Ÿåˆ†å¸ƒåˆ°æ•°æ®çš„manifoldé™„è¿‘ â‘¡é€šè¿‡åœ¨å†³ç­–è¾¹ç•Œä¸­äº§ç”Ÿå‡¹å‡¸ï¼Œæ¥ä½¿å†³ç­–è¾¹ç•Œç§»åŠ¨åˆ°æ ·æœ¬çš„æ­£ç¡®ä¸€ä¾§ã€‚ATä¼šä½¿å¾—å†³ç­–è¾¹ç•Œçš„èµ·ä¼æ›´å¤§ï¼Œå³å‘off-manifoldæ–¹å‘ç§»åŠ¨ã€‚[[paper]](https://arxiv.org/abs/2106.10151) [[notes]](./all_notes/The-Dimpled-Manifold.pdf)
2. **Understanding Adversarial Robustness Against On-manifold Adversarial Examples** (arxiv 2022) manifoldè§’åº¦ç ”ç©¶ATè¡Œä¸ºï¼Œå‘ç°äº†on-manifoldå¯¹æŠ—æ ·æœ¬çš„å­˜åœ¨ï¼Œæå‡ºäº†åœ¨GANçš„éšç©ºé—´åšATã€åœ¨è®­ç»ƒæ•°æ®çš„ç‰¹å¾å‘é‡å¼ æˆçš„å­ç©ºé—´ä¸­åšATä¸¤ç§æ€è·¯ [[paper]] (https://arxiv.org/abs/2210.00430) [[notes]](./all_notes/Understanding-Adversarial-Robustness-Against.pdf)



## LLMs/Large Multi-modal models

### 2024

1. **Model Editing with Canonical Examples** [[paper]](http://arxiv.org/abs/2402.06155) æå‡ºäº†ä¸€ä¸ªæ–°ä»»åŠ¡ï¼šè®©æ¨¡å‹å­¦ä¹ å‡ ä¸ªç‰¹å®šçš„æ–‡æœ¬ä¾‹å­ï¼Œä»¥å®ç°æŸäº›çº æ­£ï¼ŒåŒæ—¶è¿˜ä¸èƒ½è®©æ¨¡å‹æ”¹å˜å¾ˆå¤šã€‚

### 2022

1. **Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models** [[paper]](https://arxiv.org/pdf/2210.14199.pdf)
2. **Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)** (Arxiv 2022) [[paper]](https://arxiv.org/abs/2205.01397)



### 2021

**LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS** å°†å¯¹æ¨¡å‹æƒé‡çŸ©é˜µçš„æ›´æ–°é™åˆ¶ä¸ºä½ç§©çŸ©é˜µä¹˜ç§¯$BA$çš„å½¢å¼ï¼Œæå¤§å‡å°‘äº†pre-trained modelè¿ç§»åˆ°æ–°ä»»åŠ¡çš„ä»£ä»·ï¼ˆä¸ç”¨fine-tuneæ‰€æœ‰å‚æ•°ï¼‰ [[paper]](https://arxiv.org/abs/2106.09685)



## Neural Collapse

### 2023

1. **Neural Collapse: A Review on Modelling Principles and Generalization** (TMLR 2023)  [[paper]](http://arxiv.org/abs/2206.04041)

### 2022

1. **Limitations of Neural Collapse for Understanding Generalization in Deep Learning** (Arxiv 2022) [[paper]](http://arxiv.org/abs/2202.08384)



## Feature Collapse

1. **Feature Collapse** (Arxiv 2023)



## Causality

1. **Causal Confusion in Imitation Learning** (NeurIPS 2019) è§£å†³imitation learningä¸­çš„distribution shifté—®é¢˜ï¼ˆè§£å†³åœºæ™¯æ¯”è¾ƒåƒOOD generalizationä¸­çš„correlation shiftï¼‰ã€‚imitation learningï¼šç»™å‡ºobservation $X^t$å’Œexpert actions $A^t$ï¼Œå¸Œæœ›å­¦å‡º$\pi:X^t\rightarrow A^t$ã€‚æœ¬æ–‡çš„ä¸€ä¸ªæœ‰è¶£çš„å‘ç°æ˜¯ï¼šè§‚æµ‹åˆ°å¤ªå¤šçš„ï¼ˆå†å²ï¼‰ä¿¡æ¯ä¼šå½±å“æ³›åŒ–æ€§èƒ½ã€‚[[paper]](https://proceedings.neurips.cc/paper_files/paper/2019/hash/947018640bf36a2bb609d3557a285329-Abstract.html) [[notes&thinking]](./all_notes/Causality.pdf)
2. **Deep Latent-Variable Models** (NeurIPS 2017) æå‡ºäº†ä¸€ç§VAEæ¥å®ç°å­˜åœ¨unobserved confounderæ—¶çš„interventional probabilityçš„è®¡ç®— [[paper]](https://proceedings.neurips.cc/paper/2017/hash/94b5bde6de888ddf9cde6748ad2523d1-Abstract.html) [[notes&thinking]](./all_notes/Causality.pdf)
3. **Fairness in Decision-Making The Causal Explanation Formula** (AAAI 2018) æå‡ºäº†ä¸€ç§éå‚æ•°ç”»çš„counterfactual probabilityçš„è®¡ç®—æ–¹æ³•ï¼Œæ‰€æå‡ºçš„ä¸‰ç§æŒ‡æ ‡DE IE SEåˆ†åˆ«detectä¸‰ç§discrimination pathï¼ˆä»interventionåˆ°outcomeçš„pathï¼‰æ˜¯å¦å­˜åœ¨ï¼Œå¹¶è¯æ˜äº†total variationèƒ½å¤Ÿè¢«è¿™ä¸‰ä¸ªé‡è¡¨ç¤º [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/11564) [[notes&thinking]](./all_notes/Causality.pdf)

## Graph Homophily/Heterophily

### 2023

1. **Characterizing Graph Datasets for Node Classification: Homophilyâ€“Heterophily Dichotomy and Beyond** 
2. 



## å…¶ä»–æ‚æ–‡

1. **On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima** (ICLR2017) large-batchä¼šå¯¼è‡´æ¨¡å‹æ›´å®¹æ˜“è¿›å…¥å°–é”çš„æå°å€¼ç‚¹ï¼Œè€Œå°–é”çš„æå°å€¼ç‚¹ä¼šä¸åˆ©äºæ³›åŒ– [[paper]](https://arxiv.org/abs/1609.04836)




## Prompt Learning

### Prompt learningï¼š
1. **Conditional Prompt Learning for Vision-Language Models** (CoCoOp, CVPR2022) å°†å›¾ç‰‡ç‰¹å¾ç›´æ¥åŠ åˆ°context tokenä¸Šï¼Œè·å¾—sample-wiseçš„promptï¼Œä»¥å®ç°instanceçš„generalizationã€‚å…¶å®å°±æ˜¯å¸Œæœ›é€šè¿‡å¼•å…¥å›¾åƒä¿¡æ¯æ¥ä½¿å¾—promptæè¿°å¾—æ›´è´´åˆ‡ã€‚ä¸è¿‡æ„Ÿè§‰è¿˜æ˜¯æœ‰ç‚¹æ€ªï¼Œå› ä¸ºæ‰€æœ‰classéƒ½åŠ ä¸Šäº†åŒæ ·çš„å¯å­¦ä¹ prefixï¼Œä¸ºä»€ä¹ˆèƒ½æé«˜é¢„æµ‹ä¸ºæ­£ç¡®ç±»çš„æ¦‚ç‡ï¼Ÿ
2. MaPLe: Multi-modal Prompt Learning, CVPR2023 
3. Prompt-aligned Gradient for Prompt Tuning, ICCV2023
4. Compound Text-Guided Prompt Tuning via Image-Adaptive Cues, AAAI2024
5. MmAP : Multi-modal Alignment Prompt for Cross-domain Multi-task Learning, AAAI2024
6. **Improving Zero-Shot Generalization for CLIP with Synthesized Prompts** (ICCV 2023)

### For DA:

1. Domain Adaptation via Prompt Learning, arxiv 2022
2. AD-CLIP: Adapting Domains in Prompt Space Using CLIP, ICCV2023
3. Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation, NIPS2023
4. Prompt-based Distribution Alignment for Unsupervised Domain Adaptation, AAAI2024

### For DG:

1. StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization, arxiv2023





## In-Context Learning

### 2024

1. **Explore Spurious Correlations at the Concept Level in Language Models for Text Classification** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2311.08648) å‘ç°äº†LLMåœ¨æ–‡æœ¬åˆ†ç±»ä¸­ä¼šä¾èµ–çš„concept-label spurious correlationï¼Œæå‡ºä½¿ç”¨ChatGPTæ¥æ‰©å……æ•°æ®æ¥æ¶ˆé™¤è™šå‡å…³è”ã€‚
2. **Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes** (ongoing work) [[paper]](Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes) å‘ç°æ¨¡å‹å¯¹äºdemonstrationçš„permutation invarianceæˆ–è®¸æ˜¯ICL OODçš„å…³é”®ã€‚æå‡ºä½¿ç”¨ç›¸åŒçš„positional encodingæ¥æå‡ICL OODæ€§èƒ½ã€‚
3. **Simple synthetic data reduces sycophancy in large language models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2308.03958) LLMsä¼šè¿åˆæé—®è€…çš„è§‚ç‚¹è€Œç½”é¡¾äº‹å®ã€‚æå‡ºåˆæˆä¸€äº›ç”¨æˆ·çš„è§‚ç‚¹å’Œæ­£ç¡®æ€§æ— å…³çš„æ–°promptï¼Œç„¶ååœ¨è¿™äº›æ•°æ®ä¸Šfine-tuneæ¥è§£å†³sycophancyé—®é¢˜ã€‚
4. **Batch ICL** å‘ç°ä½¿ç”¨batch ICLï¼Œå°†Nä¸ªexampleè®¾ç½®ä¸ºNä¸ªone-shot inferenceï¼Œå†æŠŠæ¯ä¸ªinferenceå¾—åˆ°çš„tokenåšå¹³å‡ï¼Œæ›¿æ¢åˆ°query sampleåšaggregationæœ€ç»ˆå†é¢„æµ‹èƒ½å¸¦æ¥æå‡ã€‚ä¸€ä¸ªå¥‡ç‰¹çš„å‘ç°æ˜¯åšaggregationæ—¶ä»æŸä¸€å±‚å¾€ååšæ€§èƒ½ä¼šçªå¢ï¼Œåœ¨é‚£ä¹‹å‰æ€§èƒ½æ¥è¿‘é›¶ã€‚å¯¹æ­¤è§£é‡Šæ˜¯transformerçš„ä½å±‚æ˜¯åœ¨å­¦è¯­ä¹‰ä¿¡æ¯ã€‚



### 2023

1. **Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?** [[paper]](https://arxiv.org/abs/2202.12837) åšäº†ä¸€ç³»åˆ—æ¶ˆèå®éªŒæ¥å¯¹ICLè¿›è¡Œè§£é‡Šã€‚ä¸»è¦ç»“è®ºï¼šå³ä½¿inputå’Œlabelä¸æ˜¯ä¸€ä¸€å¯¹åº”ï¼Œåªè¦labelçš„åˆ†å¸ƒåˆç†ï¼Œé‚£ä¹ˆICLåŒæ ·èƒ½ç»™å‡ºè¾ƒä¸ºæ­£ç¡®çš„ç­”æ¡ˆ.
2. **Symbol tuning improves in-context learning in language models** (EMNLP 2023) [[paper]](http://arxiv.org/abs/2305.08298) å°†demonstrationçš„labelæ¢ä¸ºæ— æ„ä¹‰çš„symbolï¼Œç„¶åå¾®è°ƒï¼Œä»¥æ­¤å¼ºè¿«æ¨¡å‹å­¦ä¹ input-label mappingã€‚
3. **In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax** (Arxiv Nov 2023) [[paper] ](In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax) æœ¬æ–‡é€šè¿‡æ„å»ºä¸€äº›è¯­æ³•ä»»åŠ¡æ¥æµ‹è¯•æ¨¡å‹å¯¹äºå¥å­ç»“æ„çš„ç†è§£èƒ½åŠ›ï¼Œä»¥åŠOODæ³›åŒ–æ€§èƒ½ã€‚æ€»çš„è¯´æ¥ï¼ŒLLMè¿˜æ˜¯ä¼šç”¨åˆ°ä¸€äº›spurious correlationã€‚
4. 
